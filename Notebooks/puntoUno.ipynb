{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c738b72d",
   "metadata": {},
   "source": [
    "# Implementaci√≥n Naive Bayes y Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e097a09",
   "metadata": {},
   "source": [
    "## Configuraciones iniciales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfeb7844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Todos los imports cargados correctamente\n"
     ]
    }
   ],
   "source": [
    "# Imports b√°sicos\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Dict, Any\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.model_selection import train_test_split, cross_validate, StratifiedKFold, GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (classification_report, precision_recall_fscore_support, \n",
    "                            confusion_matrix, accuracy_score)\n",
    "\n",
    "print(\"Todos los imports cargados correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f57fe22",
   "metadata": {},
   "source": [
    "Para el split de los datos se definieron los porcentajes de acorde a lo establecido por el enunciado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3333a109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuraci√≥n establecida\n",
      "   - Test size: 30.0%\n",
      "   - Validation size: 10.0%\n",
      "   - Training size: 60.0%\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = Path(r\"Datasets/20news-18828/20news-18828/\")  # AJUSTAR SEG√öN TU SISTEMA\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.30           # 30% para test\n",
    "VAL_RATIO_WITHIN_TRAINVAL = 1.0 / 7.0  # 10% absoluto para validaci√≥n (de 70% restante)\n",
    "USE_ENGLISH_STOPWORDS = True\n",
    "\n",
    "print(\"Configuraci√≥n establecida\")\n",
    "print(f\"   - Test size: {TEST_SIZE*100}%\")\n",
    "print(f\"   - Validation size: {VAL_RATIO_WITHIN_TRAINVAL * (1-TEST_SIZE) * 100:.1f}%\")\n",
    "print(f\"   - Training size: {(1-TEST_SIZE) * (1-VAL_RATIO_WITHIN_TRAINVAL) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6c603f",
   "metadata": {},
   "source": [
    "## Carga de datos y train_val_test script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6428e4b",
   "metadata": {},
   "source": [
    "### 20News"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23b1269",
   "metadata": {},
   "source": [
    "Se cargan los datos utilizando el encoding latin-1 para no tener problemas de codificaci√≥n para algunos de los caracteres presentes en el dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "710e2b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargar_dataset(dataroot: Path):\n",
    "    \"\"\"\n",
    "    Carga el dataset 20newsgroups desde archivos organizados en subcarpetas.\n",
    "    \"\"\"\n",
    "    dataset = load_files(\n",
    "        container_path=str(dataroot),\n",
    "        encoding=\"latin-1\",\n",
    "        decode_error=\"ignore\",\n",
    "        shuffle=True,\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a10a02",
   "metadata": {},
   "source": [
    "Se realiza la partici√≥n de los datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71b61037",
   "metadata": {},
   "outputs": [],
   "source": [
    "def partir_train_val_test(X, y, test_size: float, val_ratio_within_trainval: float, random_state: int):\n",
    "    \"\"\"\n",
    "    Crea partici√≥n 60/10/30 estratificada.\n",
    "    \"\"\"\n",
    "    # Primero: (train+val)=70% y test=30%\n",
    "    X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Luego: dentro del 70%, separa val=10% absoluto y train=60% absoluto\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_trainval, y_trainval,\n",
    "        test_size=val_ratio_within_trainval,\n",
    "        random_state=random_state,\n",
    "        stratify=y_trainval\n",
    "    )\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45812e20",
   "metadata": {},
   "source": [
    "Se construy√≥ una funci√≥n que permitiera construir un vectorizador que permitiera devolver el vectorizador a utilizar basado en un par√°metro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "450b57ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construir_vectorizador(kind: str, use_english_stopwords: bool, optimized=True):\n",
    "    \"\"\"\n",
    "    Construye vectorizador con par√°metros optimizados o b√°sicos.\n",
    "    \"\"\"\n",
    "    stop_words = \"english\" if use_english_stopwords else None\n",
    "    \n",
    "    if optimized:\n",
    "        # Par√°metros menos restrictivos para mejor rendimiento\n",
    "        min_df = 2\n",
    "        max_df = 0.95\n",
    "        max_features = 50000\n",
    "    else:\n",
    "        # Par√°metros m√°s restrictivos para velocidad\n",
    "        min_df = 5\n",
    "        max_df = 0.90\n",
    "        max_features = 20000\n",
    "    \n",
    "    if kind == \"tf\":\n",
    "        return CountVectorizer(\n",
    "            stop_words=stop_words,\n",
    "            min_df=min_df,\n",
    "            max_df=max_df,\n",
    "            max_features=max_features\n",
    "        )\n",
    "    elif kind == \"tfidf\":\n",
    "        return TfidfVectorizer(\n",
    "            stop_words=stop_words,\n",
    "            min_df=min_df,\n",
    "            max_df=max_df,\n",
    "            max_features=max_features\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"kind debe ser 'tf' o 'tfidf'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854b890b",
   "metadata": {},
   "source": [
    "Para la evaluaci√≥n se realiz√≥ una funci√≥n que realiza las siguientes acciones:\n",
    "\n",
    "1. Entrena utilizando √∫nicamente el dataset de train y se eval√∫a con el conjunto de validaci√≥n.\n",
    "2. Luego de haber calculado estas m√©tricas se reentrena el modelo pero esta vez utilizando ambos conjuntos de datos (entrenamiento y validaci√≥n).\n",
    "3. Este nuevo modelo se eval√∫a contra el dataset de test y se vuelve a calcular m√©tricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e851c1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluar_modelo_simple(pipeline, X_train, y_train, X_val, y_val, X_test, y_test, \n",
    "                         nombre_modelo, target_names, mostrar_detalles=True):\n",
    "    \"\"\"\n",
    "    Entrena y eval√∫a un modelo en validaci√≥n y test.\n",
    "    \"\"\"\n",
    "    # Entrenar solo en train\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluar en validaci√≥n\n",
    "    y_pred_val = pipeline.predict(X_val)\n",
    "    acc_val = accuracy_score(y_val, y_pred_val)\n",
    "    p_mac_val, r_mac_val, f1_mac_val, _ = precision_recall_fscore_support(y_val, y_pred_val, average=\"macro\")\n",
    "    p_mic_val, r_mic_val, f1_mic_val, _ = precision_recall_fscore_support(y_val, y_pred_val, average=\"micro\")\n",
    "    \n",
    "    # Re-entrenar con train+val para test\n",
    "    X_train_full = np.concatenate([X_train, X_val])\n",
    "    y_train_full = np.concatenate([y_train, y_val])\n",
    "    pipeline.fit(X_train_full, y_train_full)\n",
    "    \n",
    "    # Evaluar en test\n",
    "    y_pred_test = pipeline.predict(X_test)\n",
    "    acc_test = accuracy_score(y_test, y_pred_test)\n",
    "    p_mac_test, r_mac_test, f1_mac_test, _ = precision_recall_fscore_support(y_test, y_pred_test, average=\"macro\")\n",
    "    p_mic_test, r_mic_test, f1_mic_test, _ = precision_recall_fscore_support(y_test, y_pred_test, average=\"micro\")\n",
    "    \n",
    "    if mostrar_detalles:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"MODELO: {nombre_modelo}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        print(\"\\n VALIDACI√ìN:\")\n",
    "        print(f\"   Accuracy: {acc_val:.4f}\")\n",
    "        print(f\"   Macro  -> P: {p_mac_val:.4f}  R: {r_mac_val:.4f}  F1: {f1_mac_val:.4f}\")\n",
    "        print(f\"   Micro  -> P: {p_mic_val:.4f}  R: {r_mic_val:.4f}  F1: {f1_mic_val:.4f}\")\n",
    "        \n",
    "        print(\"\\n TEST:\")\n",
    "        print(f\"   Accuracy: {acc_test:.4f}\")\n",
    "        print(f\"   Macro  -> P: {p_mac_test:.4f}  R: {r_mac_test:.4f}  F1: {f1_mac_test:.4f}\")\n",
    "        print(f\"   Micro  -> P: {p_mic_test:.4f}  R: {r_mic_test:.4f}  F1: {f1_mic_test:.4f}\")\n",
    "        \n",
    "        print(\"\\n Classification Report (Test):\")\n",
    "        print(classification_report(y_test, y_pred_test, target_names=target_names, digits=3))\n",
    "    \n",
    "    return {\n",
    "        'val_accuracy': acc_val, 'val_f1_macro': f1_mac_val, 'val_f1_micro': f1_mic_val,\n",
    "        'test_accuracy': acc_test, 'test_f1_macro': f1_mac_test, 'test_f1_micro': f1_mic_test\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da77a96f",
   "metadata": {},
   "source": [
    "## Ejecuci√≥n de la carga, ejecuci√≥n y evaluaci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44eed1fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Cargando dataset 20newsgroups...\n",
      " Dataset cargado:\n",
      "   - Total documentos: 18,828\n",
      "   - N√∫mero de clases: 20\n",
      "   - Primeras 5 clases: ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware']\n"
     ]
    }
   ],
   "source": [
    "print(\" Cargando dataset 20newsgroups...\")\n",
    "dataset = cargar_dataset(DATA_DIR)\n",
    "\n",
    "X = dataset.data\n",
    "y = dataset.target\n",
    "target_names = dataset.target_names\n",
    "\n",
    "print(f\" Dataset cargado:\")\n",
    "print(f\"   - Total documentos: {len(X):,}\")\n",
    "print(f\"   - N√∫mero de clases: {len(target_names)}\")\n",
    "print(f\"   - Primeras 5 clases: {target_names[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5bce57e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Creando particiones de datos...\n",
      " Particiones creadas:\n",
      "   - Train: 11,296 documentos (60.0%)\n",
      "   - Val:   1,883 documentos (10.0%)\n",
      "   - Test:  5,649 documentos (30.0%)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n Creando particiones de datos...\")\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = partir_train_val_test(\n",
    "    X, y, TEST_SIZE, VAL_RATIO_WITHIN_TRAINVAL, RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(f\" Particiones creadas:\")\n",
    "print(f\"   - Train: {len(X_train):,} documentos ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "print(f\"   - Val:   {len(X_val):,} documentos ({len(X_val)/len(X)*100:.1f}%)\")\n",
    "print(f\"   - Test:  {len(X_test):,} documentos ({len(X_test)/len(X)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30487581",
   "metadata": {},
   "source": [
    "### Construcci√≥n de los pipelines de entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486eccfa",
   "metadata": {},
   "source": [
    "Una vez se han definido todas lasfunciones requeridas para este entrenamiento el siguiente paso es definir los pipelines para realizar los entrenamientos para todas las combinaciones necesarias entre los modelos de clasificaci√≥n y el vectorizador a utilizar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "304a31a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "I. COMPARACI√ìN DE CLASIFICADORES NB Y LR\n",
      "================================================================================\n",
      "\n",
      " Entrenando modelos con representaciones TF y TF-IDF...\n",
      "\n",
      "  Entrenando NB + TF...\n",
      "\n",
      "================================================================================\n",
      "MODELO: NB + TF\n",
      "================================================================================\n",
      "\n",
      " VALIDACI√ìN:\n",
      "   Accuracy: 0.8640\n",
      "   Macro  -> P: 0.8813  R: 0.8617  F1: 0.8485\n",
      "   Micro  -> P: 0.8640  R: 0.8640  F1: 0.8640\n",
      "\n",
      " TEST:\n",
      "   Accuracy: 0.8727\n",
      "   Macro  -> P: 0.8833  R: 0.8694  F1: 0.8610\n",
      "   Micro  -> P: 0.8727  R: 0.8727  F1: 0.8727\n",
      "\n",
      " Classification Report (Test):\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism      0.875     0.938     0.905       240\n",
      "           comp.graphics      0.700     0.856     0.770       292\n",
      " comp.os.ms-windows.misc      0.902     0.186     0.308       296\n",
      "comp.sys.ibm.pc.hardware      0.638     0.861     0.733       295\n",
      "   comp.sys.mac.hardware      0.831     0.906     0.867       288\n",
      "          comp.windows.x      0.743     0.915     0.820       294\n",
      "            misc.forsale      0.881     0.812     0.845       292\n",
      "               rec.autos      0.922     0.916     0.919       297\n",
      "         rec.motorcycles      0.960     0.960     0.960       298\n",
      "      rec.sport.baseball      0.973     0.973     0.973       298\n",
      "        rec.sport.hockey      0.973     0.957     0.965       300\n",
      "               sci.crypt      0.946     0.943     0.944       297\n",
      "         sci.electronics      0.866     0.878     0.872       294\n",
      "                 sci.med      0.966     0.956     0.961       297\n",
      "               sci.space      0.952     0.946     0.949       296\n",
      "  soc.religion.christian      0.884     0.943     0.913       299\n",
      "      talk.politics.guns      0.875     0.945     0.908       273\n",
      "   talk.politics.mideast      0.968     0.975     0.972       282\n",
      "      talk.politics.misc      0.875     0.901     0.888       233\n",
      "      talk.religion.misc      0.936     0.622     0.748       188\n",
      "\n",
      "                accuracy                          0.873      5649\n",
      "               macro avg      0.883     0.869     0.861      5649\n",
      "            weighted avg      0.883     0.873     0.862      5649\n",
      "\n",
      "\n",
      "  Entrenando NB + TF-IDF...\n",
      "\n",
      "================================================================================\n",
      "MODELO: NB + TF-IDF\n",
      "================================================================================\n",
      "\n",
      " VALIDACI√ìN:\n",
      "   Accuracy: 0.8901\n",
      "   Macro  -> P: 0.8979  R: 0.8784  F1: 0.8786\n",
      "   Micro  -> P: 0.8901  R: 0.8901  F1: 0.8901\n",
      "\n",
      " TEST:\n",
      "   Accuracy: 0.8911\n",
      "   Macro  -> P: 0.9010  R: 0.8799  F1: 0.8801\n",
      "   Micro  -> P: 0.8911  R: 0.8911  F1: 0.8911\n",
      "\n",
      " Classification Report (Test):\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism      0.909     0.875     0.892       240\n",
      "           comp.graphics      0.828     0.825     0.827       292\n",
      " comp.os.ms-windows.misc      0.824     0.851     0.837       296\n",
      "comp.sys.ibm.pc.hardware      0.757     0.844     0.798       295\n",
      "   comp.sys.mac.hardware      0.912     0.896     0.904       288\n",
      "          comp.windows.x      0.934     0.908     0.921       294\n",
      "            misc.forsale      0.891     0.808     0.847       292\n",
      "               rec.autos      0.922     0.916     0.919       297\n",
      "         rec.motorcycles      0.957     0.970     0.963       298\n",
      "      rec.sport.baseball      0.977     0.983     0.980       298\n",
      "        rec.sport.hockey      0.967     0.977     0.972       300\n",
      "               sci.crypt      0.905     0.966     0.935       297\n",
      "         sci.electronics      0.909     0.847     0.877       294\n",
      "                 sci.med      0.965     0.939     0.952       297\n",
      "               sci.space      0.938     0.963     0.950       296\n",
      "  soc.religion.christian      0.729     0.963     0.830       299\n",
      "      talk.politics.guns      0.801     0.971     0.877       273\n",
      "   talk.politics.mideast      0.930     0.986     0.957       282\n",
      "      talk.politics.misc      0.968     0.768     0.856       233\n",
      "      talk.religion.misc      1.000     0.340     0.508       188\n",
      "\n",
      "                accuracy                          0.891      5649\n",
      "               macro avg      0.901     0.880     0.880      5649\n",
      "            weighted avg      0.899     0.891     0.887      5649\n",
      "\n",
      "\n",
      "  Entrenando LR + TF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\ir-gensim\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1281: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\anaconda3\\envs\\ir-gensim\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\anaconda3\\envs\\ir-gensim\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1281: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\anaconda3\\envs\\ir-gensim\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MODELO: LR + TF\n",
      "================================================================================\n",
      "\n",
      " VALIDACI√ìN:\n",
      "   Accuracy: 0.9028\n",
      "   Macro  -> P: 0.9055  R: 0.9015  F1: 0.9028\n",
      "   Micro  -> P: 0.9028  R: 0.9028  F1: 0.9028\n",
      "\n",
      " TEST:\n",
      "   Accuracy: 0.8986\n",
      "   Macro  -> P: 0.8990  R: 0.8960  F1: 0.8971\n",
      "   Micro  -> P: 0.8986  R: 0.8986  F1: 0.8986\n",
      "\n",
      " Classification Report (Test):\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism      0.903     0.896     0.900       240\n",
      "           comp.graphics      0.818     0.849     0.834       292\n",
      " comp.os.ms-windows.misc      0.838     0.841     0.840       296\n",
      "comp.sys.ibm.pc.hardware      0.787     0.776     0.782       295\n",
      "   comp.sys.mac.hardware      0.846     0.896     0.870       288\n",
      "          comp.windows.x      0.866     0.857     0.862       294\n",
      "            misc.forsale      0.821     0.880     0.850       292\n",
      "               rec.autos      0.913     0.919     0.916       297\n",
      "         rec.motorcycles      0.950     0.956     0.953       298\n",
      "      rec.sport.baseball      0.950     0.950     0.950       298\n",
      "        rec.sport.hockey      0.964     0.973     0.968       300\n",
      "               sci.crypt      0.965     0.936     0.950       297\n",
      "         sci.electronics      0.859     0.847     0.853       294\n",
      "                 sci.med      0.940     0.946     0.943       297\n",
      "               sci.space      0.962     0.932     0.947       296\n",
      "  soc.religion.christian      0.911     0.923     0.917       299\n",
      "      talk.politics.guns      0.920     0.927     0.923       273\n",
      "   talk.politics.mideast      0.965     0.965     0.965       282\n",
      "      talk.politics.misc      0.924     0.884     0.904       233\n",
      "      talk.religion.misc      0.878     0.766     0.818       188\n",
      "\n",
      "                accuracy                          0.899      5649\n",
      "               macro avg      0.899     0.896     0.897      5649\n",
      "            weighted avg      0.899     0.899     0.899      5649\n",
      "\n",
      "\n",
      "  Entrenando LR + TF-IDF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\ir-gensim\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1281: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\anaconda3\\envs\\ir-gensim\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\anaconda3\\envs\\ir-gensim\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1281: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\anaconda3\\envs\\ir-gensim\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MODELO: LR + TF-IDF\n",
      "================================================================================\n",
      "\n",
      " VALIDACI√ìN:\n",
      "   Accuracy: 0.8996\n",
      "   Macro  -> P: 0.9006  R: 0.8944  F1: 0.8958\n",
      "   Micro  -> P: 0.8996  R: 0.8996  F1: 0.8996\n",
      "\n",
      " TEST:\n",
      "   Accuracy: 0.8940\n",
      "   Macro  -> P: 0.8963  R: 0.8878  F1: 0.8897\n",
      "   Micro  -> P: 0.8940  R: 0.8940  F1: 0.8940\n",
      "\n",
      " Classification Report (Test):\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism      0.887     0.879     0.883       240\n",
      "           comp.graphics      0.790     0.839     0.814       292\n",
      " comp.os.ms-windows.misc      0.821     0.865     0.842       296\n",
      "comp.sys.ibm.pc.hardware      0.799     0.769     0.784       295\n",
      "   comp.sys.mac.hardware      0.896     0.899     0.898       288\n",
      "          comp.windows.x      0.891     0.888     0.889       294\n",
      "            misc.forsale      0.823     0.873     0.847       292\n",
      "               rec.autos      0.895     0.919     0.907       297\n",
      "         rec.motorcycles      0.962     0.940     0.951       298\n",
      "      rec.sport.baseball      0.960     0.973     0.967       298\n",
      "        rec.sport.hockey      0.967     0.963     0.965       300\n",
      "               sci.crypt      0.982     0.923     0.951       297\n",
      "         sci.electronics      0.849     0.861     0.855       294\n",
      "                 sci.med      0.910     0.953     0.931       297\n",
      "               sci.space      0.943     0.946     0.944       296\n",
      "  soc.religion.christian      0.838     0.933     0.883       299\n",
      "      talk.politics.guns      0.904     0.927     0.915       273\n",
      "   talk.politics.mideast      0.961     0.965     0.963       282\n",
      "      talk.politics.misc      0.926     0.863     0.893       233\n",
      "      talk.religion.misc      0.924     0.580     0.712       188\n",
      "\n",
      "                accuracy                          0.894      5649\n",
      "               macro avg      0.896     0.888     0.890      5649\n",
      "            weighted avg      0.896     0.894     0.893      5649\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(\"I. COMPARACI√ìN DE CLASIFICADORES NB Y LR\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "print(\"\\n Entrenando modelos con representaciones TF y TF-IDF...\")\n",
    "\n",
    "# Crear pipelines\n",
    "pipelines = {\n",
    "    \"NB + TF\": Pipeline([\n",
    "        (\"vec\", construir_vectorizador(\"tf\", USE_ENGLISH_STOPWORDS, optimized=True)),\n",
    "        (\"clf\", MultinomialNB())\n",
    "    ]),\n",
    "    \"NB + TF-IDF\": Pipeline([\n",
    "        (\"vec\", construir_vectorizador(\"tfidf\", USE_ENGLISH_STOPWORDS, optimized=True)),\n",
    "        (\"clf\", MultinomialNB())\n",
    "    ]),\n",
    "    \"LR + TF\": Pipeline([\n",
    "        (\"vec\", construir_vectorizador(\"tf\", USE_ENGLISH_STOPWORDS, optimized=True)),\n",
    "        (\"clf\", LogisticRegression(solver=\"liblinear\", multi_class=\"ovr\", random_state=RANDOM_STATE, max_iter=2000))\n",
    "    ]),\n",
    "    \"LR + TF-IDF\": Pipeline([\n",
    "        (\"vec\", construir_vectorizador(\"tfidf\", USE_ENGLISH_STOPWORDS, optimized=True)),\n",
    "        (\"clf\", LogisticRegression(solver=\"liblinear\", multi_class=\"ovr\", random_state=RANDOM_STATE, max_iter=2000))\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Evaluar cada pipeline\n",
    "resultados_parte1 = {}\n",
    "for nombre, pipeline in pipelines.items():\n",
    "    print(f\"\\n  Entrenando {nombre}...\")\n",
    "    resultado = evaluar_modelo_simple(\n",
    "        pipeline, X_train, y_train, X_val, y_val, X_test, y_test,\n",
    "        nombre, target_names, mostrar_detalles=True\n",
    "    )\n",
    "    resultados_parte1[nombre] = resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70d2ec91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      " RESUMEN PARTE I - COMPARACI√ìN INICIAL\n",
      "================================================================================\n",
      "\n",
      " M√©tricas en TEST:\n",
      "             test_accuracy  test_f1_macro  test_f1_micro\n",
      "NB + TF             0.8727         0.8610         0.8727\n",
      "NB + TF-IDF         0.8911         0.8801         0.8911\n",
      "LR + TF             0.8986         0.8971         0.8986\n",
      "LR + TF-IDF         0.8940         0.8897         0.8940\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(\" RESUMEN PARTE I - COMPARACI√ìN INICIAL\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "df_parte1 = pd.DataFrame(resultados_parte1).T\n",
    "print(\"\\n M√©tricas en TEST:\")\n",
    "print(df_parte1[['test_accuracy', 'test_f1_macro', 'test_f1_micro']].round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7812119",
   "metadata": {},
   "source": [
    "De la tabla anterior se observa que LR + TF obtiene el mejor desempe√±o global en todas las m√©tricas. Sin embargo, LR + TF-IDF tambi√©n presenta unas m√©tricas bastante buenas y cercanas al mejor modelo.\n",
    "\n",
    "Tambi√©n es posible notar que parece ser que el uso de TF-IDF presenta un impacto positivo especialmente para Naive Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da22bc9",
   "metadata": {},
   "source": [
    "### Validaci√≥n cruzada"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd29079",
   "metadata": {},
   "source": [
    "#### Cross validation Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1e4333",
   "metadata": {},
   "source": [
    "En este experimento se implementa un proceso de validaci√≥n cruzada de diez pliegues. El procedimiento consiste en construir dos configuraciones de *pipeline* que difieren √∫nicamente en el tipo de representaci√≥n vectorial utilizada para los documentos (frecuencias absolutas de t√©rminos y frecuencias ponderadas mediante TF-IDF) y medir su rendimiento de forma consistente a trav√©s de m√∫ltiples m√©tricas de evaluaci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6379fca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "II. INVESTIGACI√ìN DE VALIDACI√ìN CRUZADA\n",
      "================================================================================\n",
      "\n",
      " Preparando validaci√≥n cruzada con train+validation...\n",
      "   - Datos para CV: 13,179 documentos\n",
      "\n",
      "============================================================\n",
      " VALIDACI√ìN CRUZADA - NAIVE BAYES\n",
      "============================================================\n",
      "\n",
      "  Evaluando NB + TF...\n",
      " Resultados NB + TF (10-fold CV):\n",
      "   accuracy       : 0.8582 ¬± 0.0095\n",
      "   precision_macro: 0.8738 ¬± 0.0090\n",
      "   recall_macro   : 0.8554 ¬± 0.0091\n",
      "   f1_macro       : 0.8434 ¬± 0.0098\n",
      "   precision_micro: 0.8582 ¬± 0.0095\n",
      "   recall_micro   : 0.8582 ¬± 0.0095\n",
      "   f1_micro       : 0.8582 ¬± 0.0095\n",
      "\n",
      "  Evaluando NB + TF-IDF...\n",
      " Resultados NB + TF-IDF (10-fold CV):\n",
      "   accuracy       : 0.8799 ¬± 0.0066\n",
      "   precision_macro: 0.8898 ¬± 0.0071\n",
      "   recall_macro   : 0.8684 ¬± 0.0068\n",
      "   f1_macro       : 0.8680 ¬± 0.0074\n",
      "   precision_micro: 0.8799 ¬± 0.0066\n",
      "   recall_micro   : 0.8799 ¬± 0.0066\n",
      "   f1_micro       : 0.8799 ¬± 0.0066\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(\"II. INVESTIGACI√ìN DE VALIDACI√ìN CRUZADA\")\n",
    "print(f\"{'='*80}\")\n",
    "print(\"\\n Preparando validaci√≥n cruzada con train+validation...\")\n",
    "X_trainval = list(X_train) + list(X_val)\n",
    "y_trainval = list(y_train) + list(y_val)\n",
    "\n",
    "print(f\"   - Datos para CV: {len(X_trainval):,} documentos\")\n",
    "\n",
    "# Configurar CV\n",
    "cv_strategy = StratifiedKFold(n_splits=10, shuffle=True, random_state=RANDOM_STATE)\n",
    "scoring_metrics = [\"accuracy\", \"precision_macro\", \"recall_macro\", \"f1_macro\", \n",
    "                  \"precision_micro\", \"recall_micro\", \"f1_micro\"]\n",
    "\n",
    "# CV para Naive Bayes\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\" VALIDACI√ìN CRUZADA - NAIVE BAYES\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "nb_results = {}\n",
    "for vectorizer_name, vectorizer_type in [(\"TF\", \"tf\"), (\"TF-IDF\", \"tfidf\")]:\n",
    "    print(f\"\\n  Evaluando NB + {vectorizer_name}...\")\n",
    "    \n",
    "    pipeline_nb = Pipeline([\n",
    "        (\"vec\", construir_vectorizador(vectorizer_type, USE_ENGLISH_STOPWORDS, optimized=False)),\n",
    "        (\"clf\", MultinomialNB())\n",
    "    ])\n",
    "    \n",
    "    cv_results = cross_validate(\n",
    "        pipeline_nb, X_trainval, y_trainval,\n",
    "        cv=cv_strategy, scoring=scoring_metrics, n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    nb_results[f\"NB + {vectorizer_name}\"] = cv_results\n",
    "    \n",
    "    print(f\" Resultados NB + {vectorizer_name} (10-fold CV):\")\n",
    "    for metric in scoring_metrics:\n",
    "        scores = cv_results[f\"test_{metric}\"]\n",
    "        print(f\"   {metric:15s}: {np.mean(scores):.4f} ¬± {np.std(scores):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fcaf32",
   "metadata": {},
   "source": [
    "Los resultados de la validaci√≥n cruzada muestran un patr√≥n claro: el uso de representaciones TF-IDF ofrece una mejora consistente frente a las representaciones basadas √∫nicamente en frecuencias absolutas (TF). En particular, Naive Bayes con TF-IDF alcanza un *accuracy* promedio cercano al 88% con una desviaci√≥n est√°ndar baja, lo que refleja tanto un mejor rendimiento como una mayor estabilidad entre los pliegues. Adem√°s, las m√©tricas macro (precisi√≥n, recall y F1) son superiores en este esquema, lo que indica un tratamiento m√°s equilibrado de las diferentes clases. En contraste, el modelo con TF presenta un rendimiento aceptable pero con un sesgo m√°s marcado, evidenciado en un F1 macro m√°s bajo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2448cd8",
   "metadata": {},
   "source": [
    "#### Cross validation Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91013d5d",
   "metadata": {},
   "source": [
    "En este caso se lleva a cabo una b√∫squeda sistem√°tica de hiperpar√°metros para modelos de regresi√≥n log√≠stica aplicados a clasificaci√≥n de texto. El procedimiento consiste en integrar un *pipeline* con dos variantes de vectorizaci√≥n (TF y TF-IDF) y un clasificador de regresi√≥n log√≠stica, sobre el cual se exploran combinaciones de hiperpar√°metros clave: el coeficiente de regularizaci√≥n (*C*), el tipo de penalizaci√≥n (*l1* o *l2*) y el solucionador (*liblinear*, compatible con ambas penalizaciones). Para cada configuraci√≥n se ejecuta una b√∫squeda en malla (*Grid Search*) con validaci√≥n cruzada  de diez pliegues, utilizando como m√©trica principal el F1 macro.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36754b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "B√öSQUEDA DE HIPERPAR√ÅMETROS - LOGISTIC REGRESSION\n",
      "============================================================\n",
      "\n",
      " B√∫squeda de hiperpar√°metros LR + TF...\n",
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\ir-gensim\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1281: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\anaconda3\\envs\\ir-gensim\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Mejores par√°metros LR + TF:\n",
      "   clf__C: 0.5\n",
      "   clf__penalty: l2\n",
      "   clf__solver: liblinear\n",
      "   Mejor F1-macro CV: 0.8927\n",
      "üìä Resultados LR + TF (mejor modelo, 10-fold CV):\n",
      "   accuracy       : 0.8941 ¬± 0.0082\n",
      "   precision_macro: 0.8963 ¬± 0.0083\n",
      "   recall_macro   : 0.8915 ¬± 0.0084\n",
      "   f1_macro       : 0.8927 ¬± 0.0084\n",
      "   precision_micro: 0.8941 ¬± 0.0082\n",
      "   recall_micro   : 0.8941 ¬± 0.0082\n",
      "   f1_micro       : 0.8941 ¬± 0.0082\n",
      "\n",
      " B√∫squeda de hiperpar√°metros LR + TF-IDF...\n",
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\ir-gensim\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1281: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\anaconda3\\envs\\ir-gensim\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Mejores par√°metros LR + TF-IDF:\n",
      "   clf__C: 5.0\n",
      "   clf__penalty: l2\n",
      "   clf__solver: liblinear\n",
      "   Mejor F1-macro CV: 0.9084\n",
      "üìä Resultados LR + TF-IDF (mejor modelo, 10-fold CV):\n",
      "   accuracy       : 0.9105 ¬± 0.0062\n",
      "   precision_macro: 0.9118 ¬± 0.0063\n",
      "   recall_macro   : 0.9071 ¬± 0.0065\n",
      "   f1_macro       : 0.9084 ¬± 0.0064\n",
      "   precision_micro: 0.9105 ¬± 0.0062\n",
      "   recall_micro   : 0.9105 ¬± 0.0062\n",
      "   f1_micro       : 0.9105 ¬± 0.0062\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*60}\")\n",
    "print(\"B√öSQUEDA DE HIPERPAR√ÅMETROS - LOGISTIC REGRESSION\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Par√°metros m√°s amplios pero computacionalmente factibles\n",
    "param_grid_lr = {\n",
    "    'clf__C': [0.1, 0.5, 1.0, 2.0, 5.0],  # Regularizaci√≥n\n",
    "    'clf__penalty': ['l1', 'l2'],          # Tipo de regularizaci√≥n\n",
    "    'clf__solver': ['liblinear']            # Solver compatible con l1 y l2\n",
    "}\n",
    "\n",
    "lr_results = {}\n",
    "for vectorizer_name, vectorizer_type in [(\"TF\", \"tf\"), (\"TF-IDF\", \"tfidf\")]:\n",
    "    print(f\"\\n B√∫squeda de hiperpar√°metros LR + {vectorizer_name}...\")\n",
    "    \n",
    "    pipeline_lr = Pipeline([\n",
    "        (\"vec\", construir_vectorizador(vectorizer_type, USE_ENGLISH_STOPWORDS, optimized=False)),\n",
    "        (\"clf\", LogisticRegression(multi_class=\"ovr\", random_state=RANDOM_STATE, max_iter=3000))\n",
    "    ])\n",
    "    \n",
    "    # GridSearch con CV\n",
    "    grid_search = GridSearchCV(\n",
    "        pipeline_lr, param_grid_lr,\n",
    "        cv=cv_strategy, scoring='f1_macro',\n",
    "        n_jobs=-1, verbose=1, refit=True\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_trainval, y_trainval)\n",
    "    \n",
    "    # Mejores par√°metros\n",
    "    print(f\" Mejores par√°metros LR + {vectorizer_name}:\")\n",
    "    for param, value in grid_search.best_params_.items():\n",
    "        print(f\"   {param}: {value}\")\n",
    "    print(f\"   Mejor F1-macro CV: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    # Evaluaci√≥n completa del mejor modelo\n",
    "    best_pipeline = grid_search.best_estimator_\n",
    "    cv_results_best = cross_validate(\n",
    "        best_pipeline, X_trainval, y_trainval,\n",
    "        cv=cv_strategy, scoring=scoring_metrics, n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    lr_results[f\"LR + {vectorizer_name}\"] = {\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'best_score': grid_search.best_score_,\n",
    "        'cv_results': cv_results_best,\n",
    "        'best_pipeline': best_pipeline\n",
    "    }\n",
    "    \n",
    "    print(f\"üìä Resultados LR + {vectorizer_name} (mejor modelo, 10-fold CV):\")\n",
    "    for metric in scoring_metrics:\n",
    "        scores = cv_results_best[f\"test_{metric}\"]\n",
    "        print(f\"   {metric:15s}: {np.mean(scores):.4f} ¬± {np.std(scores):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41949373",
   "metadata": {},
   "source": [
    "Los resultados obtenidos muestran que la combinaci√≥n de regresi√≥n log√≠stica con representaci√≥n TF-IDF ofrece un desempe√±o muy s√≥lido y balanceado en la tarea de clasificaci√≥n. El mejor modelo, configurado con __regularizaci√≥n L2__ y un coeficiente de regularizaci√≥n de __5.0__, alcanza un F1-macro promedio de __0.9084__ en validaci√≥n cruzada, lo que indica un buen equilibrio entre precisi√≥n y exhaustividad en todas las clases. Adem√°s, la consistencia de las m√©tricas ‚Äîcon desviaciones est√°ndar muy bajas en torno a 0.006‚Äî refleja una alta estabilidad del modelo frente a las distintas particiones de los datos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57696444",
   "metadata": {},
   "source": [
    "#### Resumen de la validaci√≥n cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aaef28bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      " RESUMEN VALIDACI√ìN CRUZADA (10-fold)\n",
      "================================================================================\n",
      "                    F1 Macro         F1 Micro         Accuracy\n",
      "NB + TF      0.8434 ¬± 0.0098  0.8582 ¬± 0.0095  0.8582 ¬± 0.0095\n",
      "NB + TF-IDF  0.8680 ¬± 0.0074  0.8799 ¬± 0.0066  0.8799 ¬± 0.0066\n",
      "LR + TF      0.8927 ¬± 0.0084  0.8941 ¬± 0.0082  0.8941 ¬± 0.0082\n",
      "LR + TF-IDF  0.9084 ¬± 0.0064  0.9105 ¬± 0.0062  0.9105 ¬± 0.0062\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(\" RESUMEN VALIDACI√ìN CRUZADA (10-fold)\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "cv_summary = {}\n",
    "for model_name, results in nb_results.items():\n",
    "    cv_summary[model_name] = {\n",
    "        'F1 Macro': f\"{np.mean(results['test_f1_macro']):.4f} ¬± {np.std(results['test_f1_macro']):.4f}\",\n",
    "        'F1 Micro': f\"{np.mean(results['test_f1_micro']):.4f} ¬± {np.std(results['test_f1_micro']):.4f}\",\n",
    "        'Accuracy': f\"{np.mean(results['test_accuracy']):.4f} ¬± {np.std(results['test_accuracy']):.4f}\"\n",
    "    }\n",
    "\n",
    "for model_name, results in lr_results.items():\n",
    "    cv_summary[model_name] = {\n",
    "        'F1 Macro': f\"{np.mean(results['cv_results']['test_f1_macro']):.4f} ¬± {np.std(results['cv_results']['test_f1_macro']):.4f}\",\n",
    "        'F1 Micro': f\"{np.mean(results['cv_results']['test_f1_micro']):.4f} ¬± {np.std(results['cv_results']['test_f1_micro']):.4f}\",\n",
    "        'Accuracy': f\"{np.mean(results['cv_results']['test_accuracy']):.4f} ¬± {np.std(results['cv_results']['test_accuracy']):.4f}\"\n",
    "    }\n",
    "\n",
    "df_cv = pd.DataFrame(cv_summary).T\n",
    "print(df_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61988d6",
   "metadata": {},
   "source": [
    "#### Evaluaci√≥n de los modelos Naive Bayes enm test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0c716b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "III. EVALUACI√ìN FINAL EN TEST SET\n",
      "================================================================================\n",
      " Evaluando todos los modelos en el conjunto de test...\n",
      "\n",
      "  Evaluando NB + TF en test...\n",
      "   Accuracy: 0.8727 | F1-Macro: 0.8610 | F1-Micro: 0.8727\n",
      "\n",
      "  Evaluando NB + TF-IDF en test...\n",
      "   Accuracy: 0.8911 | F1-Macro: 0.8801 | F1-Micro: 0.8911\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(\"III. EVALUACI√ìN FINAL EN TEST SET\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "print(\" Evaluando todos los modelos en el conjunto de test...\")\n",
    "\n",
    "# Re-entrenar modelos con train+val y evaluar en test\n",
    "final_results = {}\n",
    "\n",
    "# Naive Bayes models\n",
    "for vectorizer_name, vectorizer_type in [(\"TF\", \"tf\"), (\"TF-IDF\", \"tfidf\")]:\n",
    "    model_name = f\"NB + {vectorizer_name}\"\n",
    "    print(f\"\\n  Evaluando {model_name} en test...\")\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        (\"vec\", construir_vectorizador(vectorizer_type, USE_ENGLISH_STOPWORDS, optimized=True)),\n",
    "        (\"clf\", MultinomialNB())\n",
    "    ])\n",
    "    \n",
    "    pipeline.fit(X_trainval, y_trainval)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    \n",
    "    # Calcular m√©tricas\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    p_macro, r_macro, f1_macro, _ = precision_recall_fscore_support(y_test, y_pred, average=\"macro\")\n",
    "    p_micro, r_micro, f1_micro, _ = precision_recall_fscore_support(y_test, y_pred, average=\"micro\")\n",
    "    \n",
    "    final_results[model_name] = {\n",
    "        'Accuracy': acc, 'Precision Macro': p_macro, 'Recall Macro': r_macro, 'F1 Macro': f1_macro,\n",
    "        'Precision Micro': p_micro, 'Recall Micro': r_micro, 'F1 Micro': f1_micro\n",
    "    }\n",
    "    \n",
    "    print(f\"   Accuracy: {acc:.4f} | F1-Macro: {f1_macro:.4f} | F1-Micro: {f1_micro:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e31cdb2",
   "metadata": {},
   "source": [
    "#### Evaluaci√≥n de los modelos de regresi√≥n log√≠stica con mejores hiperpar√°metros en test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e4c9ad8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Evaluando LR + TF en test (mejores hiperpar√°metros)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\ir-gensim\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1281: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\anaconda3\\envs\\ir-gensim\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Accuracy: 0.8915 | F1-Macro: 0.8898 | F1-Micro: 0.8915\n",
      "   Mejores par√°metros: {'clf__C': 0.5, 'clf__penalty': 'l2', 'clf__solver': 'liblinear'}\n",
      "\n",
      "  Evaluando LR + TF-IDF en test (mejores hiperpar√°metros)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\ir-gensim\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1281: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\anaconda3\\envs\\ir-gensim\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Accuracy: 0.9106 | F1-Macro: 0.9087 | F1-Micro: 0.9106\n",
      "   Mejores par√°metros: {'clf__C': 5.0, 'clf__penalty': 'l2', 'clf__solver': 'liblinear'}\n"
     ]
    }
   ],
   "source": [
    "for vectorizer_name in [\"TF\", \"TF-IDF\"]:\n",
    "    model_name = f\"LR + {vectorizer_name}\"\n",
    "    print(f\"\\n  Evaluando {model_name} en test (mejores hiperpar√°metros)...\")\n",
    "    \n",
    "    best_pipeline = lr_results[model_name]['best_pipeline']\n",
    "    best_pipeline.fit(X_trainval, y_trainval)\n",
    "    y_pred = best_pipeline.predict(X_test)\n",
    "    \n",
    "    # Calcular m√©tricas\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    p_macro, r_macro, f1_macro, _ = precision_recall_fscore_support(y_test, y_pred, average=\"macro\")\n",
    "    p_micro, r_micro, f1_micro, _ = precision_recall_fscore_support(y_test, y_pred, average=\"micro\")\n",
    "    \n",
    "    final_results[model_name] = {\n",
    "        'Accuracy': acc, 'Precision Macro': p_macro, 'Recall Macro': r_macro, 'F1 Macro': f1_macro,\n",
    "        'Precision Micro': p_micro, 'Recall Micro': r_micro, 'F1 Micro': f1_micro\n",
    "    }\n",
    "    \n",
    "    print(f\"   Accuracy: {acc:.4f} | F1-Macro: {f1_macro:.4f} | F1-Micro: {f1_micro:.4f}\")\n",
    "    print(f\"   Mejores par√°metros: {lr_results[model_name]['best_params']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dc59fb69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      " RESULTADOS FINALES EN TEST SET\n",
      "================================================================================\n",
      "\n",
      " M√©tricas de rendimiento:\n",
      "             Accuracy  Precision Macro  Recall Macro  F1 Macro  \\\n",
      "NB + TF        0.8727           0.8833        0.8694    0.8610   \n",
      "NB + TF-IDF    0.8911           0.9010        0.8799    0.8801   \n",
      "LR + TF        0.8915           0.8916        0.8887    0.8898   \n",
      "LR + TF-IDF    0.9106           0.9112        0.9074    0.9087   \n",
      "\n",
      "             Precision Micro  Recall Micro  F1 Micro  \n",
      "NB + TF               0.8727        0.8727    0.8727  \n",
      "NB + TF-IDF           0.8911        0.8911    0.8911  \n",
      "LR + TF               0.8915        0.8915    0.8915  \n",
      "LR + TF-IDF           0.9106        0.9106    0.9106  \n",
      "\n",
      " MEJORES MODELOS:\n",
      "    Mejor F1-Macro:  LR + TF-IDF (0.9087)\n",
      "    Mejor F1-Micro:  LR + TF-IDF (0.9106)\n",
      "    Mejor Accuracy:  LR + TF-IDF (0.9106)\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(\" RESULTADOS FINALES EN TEST SET\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "df_final = pd.DataFrame(final_results).T\n",
    "print(\"\\n M√©tricas de rendimiento:\")\n",
    "print(df_final.round(4))\n",
    "\n",
    "# Identificar el mejor modelo\n",
    "best_model_f1_macro = df_final['F1 Macro'].idxmax()\n",
    "best_model_f1_micro = df_final['F1 Micro'].idxmax()\n",
    "best_model_accuracy = df_final['Accuracy'].idxmax()\n",
    "\n",
    "print(f\"\\n MEJORES MODELOS:\")\n",
    "print(f\"    Mejor F1-Macro:  {best_model_f1_macro} ({df_final.loc[best_model_f1_macro, 'F1 Macro']:.4f})\")\n",
    "print(f\"    Mejor F1-Micro:  {best_model_f1_micro} ({df_final.loc[best_model_f1_micro, 'F1 Micro']:.4f})\")\n",
    "print(f\"    Mejor Accuracy:  {best_model_accuracy} ({df_final.loc[best_model_accuracy, 'Accuracy']:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "87d107cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      " AN√ÅLISIS DEL MEJOR MODELO\n",
      "================================================================================\n",
      "Modelo seleccionado: LR + TF-IDF\n",
      "Representaci√≥n: TF-IDF\n",
      "Hiperpar√°metros optimizados:\n",
      "   clf__C: 5.0\n",
      "   clf__penalty: l2\n",
      "   clf__solver: liblinear\n",
      "\n",
      "Reporte detallado del mejor modelo:\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism      0.893     0.900     0.896       240\n",
      "           comp.graphics      0.816     0.863     0.839       292\n",
      " comp.os.ms-windows.misc      0.828     0.861     0.844       296\n",
      "comp.sys.ibm.pc.hardware      0.830     0.797     0.813       295\n",
      "   comp.sys.mac.hardware      0.902     0.899     0.901       288\n",
      "          comp.windows.x      0.891     0.891     0.891       294\n",
      "            misc.forsale      0.845     0.894     0.869       292\n",
      "               rec.autos      0.929     0.923     0.926       297\n",
      "         rec.motorcycles      0.973     0.956     0.964       298\n",
      "      rec.sport.baseball      0.970     0.980     0.975       298\n",
      "        rec.sport.hockey      0.973     0.970     0.972       300\n",
      "               sci.crypt      0.982     0.933     0.957       297\n",
      "         sci.electronics      0.870     0.888     0.879       294\n",
      "                 sci.med      0.931     0.953     0.942       297\n",
      "               sci.space      0.955     0.939     0.947       296\n",
      "  soc.religion.christian      0.891     0.933     0.912       299\n",
      "      talk.politics.guns      0.941     0.941     0.941       273\n",
      "   talk.politics.mideast      0.972     0.972     0.972       282\n",
      "      talk.politics.misc      0.931     0.927     0.929       233\n",
      "      talk.religion.misc      0.901     0.729     0.806       188\n",
      "\n",
      "                accuracy                          0.911      5649\n",
      "               macro avg      0.911     0.907     0.909      5649\n",
      "            weighted avg      0.911     0.911     0.911      5649\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(\" AN√ÅLISIS DEL MEJOR MODELO\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "best_overall = best_model_f1_macro  # Usar F1-macro como criterio principal\n",
    "print(f\"Modelo seleccionado: {best_overall}\")\n",
    "\n",
    "if best_overall.startswith(\"LR\"):\n",
    "    vectorizer_type = \"TF\" if \"TF-IDF\" not in best_overall else \"TF-IDF\"\n",
    "    best_params = lr_results[best_overall]['best_params']\n",
    "    print(f\"Representaci√≥n: {vectorizer_type}\")\n",
    "    print(f\"Hiperpar√°metros optimizados:\")\n",
    "    for param, value in best_params.items():\n",
    "        print(f\"   {param}: {value}\")\n",
    "\n",
    "print(f\"\\nReporte detallado del mejor modelo:\")\n",
    "if best_overall.startswith(\"LR\"):\n",
    "    best_pipeline = lr_results[best_overall]['best_pipeline']\n",
    "else:\n",
    "    vectorizer_type = \"tf\" if \"TF-IDF\" not in best_overall else \"tfidf\"\n",
    "    best_pipeline = Pipeline([\n",
    "        (\"vec\", construir_vectorizador(vectorizer_type, USE_ENGLISH_STOPWORDS, optimized=True)),\n",
    "        (\"clf\", MultinomialNB())\n",
    "    ])\n",
    "    best_pipeline.fit(X_trainval, y_trainval)\n",
    "\n",
    "y_pred_best = best_pipeline.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_best, target_names=target_names, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39eb6fb6",
   "metadata": {},
   "source": [
    "Al final de toda la experimentaci√≥n, el mejor modelo identificado fue la regresi√≥n log√≠stica con representaci√≥n TF-IDF y regularizaci√≥n L2, que alcanz√≥ un desempe√±o sobresaliente con una exactitud global del 91.1 %. El an√°lisis detallado por clase muestra resultados consistentes y elevados en precisi√≥n, recall y F1, con un promedio macro de 0.911, 0.907 y 0.909, respectivamente. Se destacan categor√≠as como *rec.sport.baseball*, *rec.motorcycles* y *talk.politics.mideast*, donde los puntajes F1 superan el 0.95, evidenciando la capacidad del modelo para distinguir con gran efectividad estos temas. Aunque algunas clases como *talk.religion.misc* presentan un rendimiento relativamente menor, el modelo mantiene en general un balance adecuado en todos los grupos.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
