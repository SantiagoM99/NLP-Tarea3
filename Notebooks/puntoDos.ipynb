{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea53f3a2",
   "metadata": {},
   "source": [
    "# Implementación del Análisis de sentimiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a0fdba",
   "metadata": {},
   "source": [
    "## Importación de librerías y configuraciones iniciales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db0b12f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Imports cargados correctamente\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, classification_report\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "print(\"✅ Imports cargados correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5880b5c8",
   "metadata": {},
   "source": [
    "## Definición de funciones para carga de datos y transformación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea284781",
   "metadata": {},
   "source": [
    "El flujo de procesamiento comienza con la transformación de cada línea de reseña en una representación estructurada. A partir de cadenas de texto que contienen tokens acompañados de sus frecuencias, se construye un diccionario de características acumuladas y, cuando está disponible, se extrae la etiqueta de sentimiento en formato binario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b4d1c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_review_counts_line(line: str) -> Tuple[Dict[str, float], Optional[int]]:\n",
    "    \"\"\"\n",
    "    Devuelve (features_dict, label) donde label es 1=positive, 0=negative o None si no viene.\n",
    "    Ignora pares malformados.\n",
    "    \"\"\"\n",
    "    feats = {}\n",
    "    y = None\n",
    "    parts = line.strip().split()\n",
    "    for item in parts:\n",
    "        if item.startswith(\"#label#:\"):\n",
    "            raw = item.split(\":\", 1)[1].strip().lower()\n",
    "            if raw in (\"positive\", \"pos\", \"1\", \"p\"):\n",
    "                y = 1\n",
    "            elif raw in (\"negative\", \"neg\", \"0\", \"n\"):\n",
    "                y = 0\n",
    "            continue\n",
    "        if \":\" not in item:\n",
    "            continue\n",
    "        tok, cnt = item.split(\":\", 1)\n",
    "        try:\n",
    "            val = int(cnt)\n",
    "        except ValueError:\n",
    "            try:\n",
    "                val = float(cnt)\n",
    "            except ValueError:\n",
    "                continue\n",
    "        if tok in feats:\n",
    "            feats[tok] = feats[tok] + val\n",
    "        else:\n",
    "            feats[tok] = val\n",
    "    return feats, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb920240",
   "metadata": {},
   "source": [
    "Con este mecanismo establecido, se habilita la lectura sistemática de archivos de reseñas. Cada archivo es procesado línea por línea, generando colecciones de ejemplos donde las características textuales quedan vinculadas a etiquetas de clase, o en su defecto se asigna una etiqueta predeterminada. Esto permite integrar en un mismo esquema tanto datos etiquetados como no etiquetados, facilitando posteriores etapas de análisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad0224bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_review_file(path: Path, default_label: Optional[int] = None):\n",
    "    \"\"\"\n",
    "    Lee un .review línea por línea y devuelve (X_dicts, y_labels).\n",
    "    - Si la línea trae #label#, se usa.\n",
    "    - Si no trae y default_label no es None, se asigna ese.\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    with path.open(\"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line == \"\":\n",
    "                continue\n",
    "            feats, lbl = parse_review_counts_line(line)\n",
    "            if lbl is None:\n",
    "                lbl = default_label\n",
    "            if feats:\n",
    "                X.append(feats)\n",
    "                y.append(lbl)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98ab47c",
   "metadata": {},
   "source": [
    "A nivel organizacional, los datos se estructuran de dos formas complementarias. Por un lado, es posible cargar la información categoría por categoría, diferenciando reseñas positivas, negativas y no etiquetadas, lo cual permite realizar un estudio detallado de cada dominio de interés. Por otro, también se ofrece la combinación de todas las categorías en un único conjunto global, en el que se reúnen reseñas etiquetadas y no etiquetadas de manera conjunta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c188ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_domains_by_category(root: Path, categories: List[str]) -> Dict[str, Dict[str, List]]:\n",
    "    \"\"\"\n",
    "    Carga datos organizados por categoría para análisis individual.\n",
    "    Retorna: {categoria: {X_pos, y_pos, X_neg, y_neg, X_unl}}\n",
    "    \"\"\"\n",
    "    data = {}\n",
    "    for cat in categories:\n",
    "        d = root / cat\n",
    "        Xp, yp = load_review_file(d / \"positive.review\", default_label=1)\n",
    "        Xn, yn = load_review_file(d / \"negative.review\", default_label=0)\n",
    "        Xu, yu = load_review_file(d / \"unlabeled.review\", default_label=None)\n",
    "        data[cat] = {\n",
    "            \"X_pos\": Xp, \"y_pos\": yp,\n",
    "            \"X_neg\": Xn, \"y_neg\": yn,\n",
    "            \"X_unl\": Xu\n",
    "        }\n",
    "        print(f\"[{cat}] pos={len(Xp)} neg={len(Xn)} unl={len(Xu)}\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e41fe2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_domains_combined(root: Path, categories: List[str]) -> Dict[str, List]:\n",
    "    \"\"\"\n",
    "    Carga datos combinados de todas las categorías para análisis global.\n",
    "    Retorna: {X_labeled, y_labeled, X_unlabeled}\n",
    "    \"\"\"\n",
    "    X_lab, y_lab = [], []\n",
    "    X_unl = []\n",
    "\n",
    "    for cat in categories:\n",
    "        d = root / cat\n",
    "        Xp, yp = load_review_file(d / \"positive.review\", default_label=1)\n",
    "        Xn, yn = load_review_file(d / \"negative.review\", default_label=0)\n",
    "        Xu, yu = load_review_file(d / \"unlabeled.review\", default_label=None)\n",
    "\n",
    "        X_lab.extend(Xp)\n",
    "        y_lab.extend(yp)\n",
    "        X_lab.extend(Xn)\n",
    "        y_lab.extend(yn)\n",
    "        X_unl.extend(Xu)\n",
    "\n",
    "    return {\n",
    "        \"X_labeled\": X_lab,\n",
    "        \"y_labeled\": y_lab,\n",
    "        \"X_unlabeled\": X_unl\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187d5f46",
   "metadata": {},
   "source": [
    "Finalmente, el flujo incorpora un recurso léxico externo a través de la integración de SentiWordNet. A partir de este repositorio, se construye un diccionario donde cada palabra queda asociada a su puntaje medio de positividad y negatividad, calculado sobre sus distintas apariciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b04bd5f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Funciones de carga de datos definidas\n"
     ]
    }
   ],
   "source": [
    "def load_sentiwordnet(path: Path) -> Dict[str, Tuple[float, float]]:\n",
    "    \"\"\"\n",
    "    Retorna dict: palabra -> (pos_score_promedio, neg_score_promedio)\n",
    "    Ignora líneas de comentario que comienzan con '#'.\n",
    "    \"\"\"\n",
    "    pos_acc = defaultdict(float)\n",
    "    neg_acc = defaultdict(float)\n",
    "    cnt_acc = defaultdict(int)\n",
    "\n",
    "    with path.open(\"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for line in f:\n",
    "            if not line or line.startswith(\"#\"):\n",
    "                continue\n",
    "            cols = line.strip().split(\"\\t\")\n",
    "            if len(cols) < 5:\n",
    "                continue\n",
    "            try:\n",
    "                pos_score = float(cols[2])\n",
    "                neg_score = float(cols[3])\n",
    "            except ValueError:\n",
    "                continue\n",
    "            terms = cols[4].split()\n",
    "            for t in terms:\n",
    "                lemma = t.split(\"#\", 1)[0].lower()\n",
    "                pos_acc[lemma] += pos_score\n",
    "                neg_acc[lemma] += neg_score\n",
    "                cnt_acc[lemma] += 1\n",
    "\n",
    "    lex = {}\n",
    "    for w in cnt_acc:\n",
    "        lex[w] = (pos_acc[w] / cnt_acc[w], neg_acc[w] / cnt_acc[w])\n",
    "    return lex\n",
    "\n",
    "print(\"✅ Funciones de carga de datos definidas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48188f2d",
   "metadata": {},
   "source": [
    "Sobre la base de los datos cargados y el léxico construido, se incorporan ahora mecanismos para transformar la información en representaciones numéricas más ricas. Se diseñan atributos derivados de SentiWordNet que resumen distintos aspectos del tono emocional de los textos: desde la suma ponderada de puntajes positivos y negativos hasta medidas de frecuencia relativa, máximos observados y promedios entre los términos reconocidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b348edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentiWordNetFeatures(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, lexicon: Dict[str, Tuple[float, float]]):\n",
    "        self.lexicon = lexicon\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        rows = []\n",
    "        for feats in X:  # feats es un dict token -> count\n",
    "            total_tokens = len(feats)\n",
    "            sum_pos = 0.0\n",
    "            sum_neg = 0.0\n",
    "            hits_pos = 0.0\n",
    "            hits_neg = 0.0\n",
    "            max_pos = 0.0\n",
    "            max_neg = 0.0\n",
    "            acc_pos = 0.0\n",
    "            acc_neg = 0.0\n",
    "            seen = 0.0\n",
    "\n",
    "            for tok, cnt in feats.items():\n",
    "                key = tok.lower()\n",
    "                if key in self.lexicon:\n",
    "                    p, n = self.lexicon[key]\n",
    "                    sum_pos = sum_pos + p * cnt\n",
    "                    sum_neg = sum_neg + n * cnt\n",
    "                    acc_pos = acc_pos + p\n",
    "                    acc_neg = acc_neg + n\n",
    "                    seen = seen + 1.0\n",
    "                    if p > 0:\n",
    "                        hits_pos = hits_pos + 1.0\n",
    "                        if p > max_pos:\n",
    "                            max_pos = p\n",
    "                    if n > 0:\n",
    "                        hits_neg = hits_neg + 1.0\n",
    "                        if n > max_neg:\n",
    "                            max_neg = n\n",
    "\n",
    "            if total_tokens > 0:\n",
    "                ratio_pos = hits_pos / float(total_tokens)\n",
    "                ratio_neg = hits_neg / float(total_tokens)\n",
    "            else:\n",
    "                ratio_pos = 0.0\n",
    "                ratio_neg = 0.0\n",
    "\n",
    "            if seen > 0:\n",
    "                avg_pos = acc_pos / seen\n",
    "                avg_neg = acc_neg / seen\n",
    "            else:\n",
    "                avg_pos = 0.0\n",
    "                avg_neg = 0.0\n",
    "\n",
    "            rows.append([sum_pos, sum_neg, hits_pos, hits_neg,\n",
    "                         ratio_pos, ratio_neg, max_pos, max_neg,\n",
    "                         avg_pos, avg_neg])\n",
    "\n",
    "        arr = np.array(rows, dtype=float)\n",
    "        return arr "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1843942f",
   "metadata": {},
   "source": [
    "Con este insumo, se preparan diferentes configuraciones de modelos de referencia que permiten comparar alternativas de representación y clasificación. Entre ellas se consideran esquemas basados en Naive Bayes y Regresión Logística, aplicados tanto sobre recuentos simples de términos como sobre transformaciones TF-IDF que ponderan la relevancia de las palabras en el corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1cdf551",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_nb_tf():\n",
    "    return Pipeline([\n",
    "        (\"dict\", DictVectorizer(sparse=True)),\n",
    "        (\"nb\", MultinomialNB())\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "721592fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_nb_tfidf():\n",
    "    return Pipeline([\n",
    "        (\"dict\", DictVectorizer(sparse=True)),\n",
    "        (\"tfidf\", TfidfTransformer()),\n",
    "        (\"nb\", MultinomialNB())\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ff9a388",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lr_tf():\n",
    "    clf = LogisticRegression(\n",
    "        solver=\"liblinear\", multi_class=\"ovr\", penalty=\"l2\",\n",
    "        C=1.0, max_iter=2000, tol=1e-3, n_jobs=1, random_state=42\n",
    "    )\n",
    "    return Pipeline([\n",
    "        (\"dict\", DictVectorizer(sparse=True)),\n",
    "        (\"lr\", clf)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff91da1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lr_tfidf():\n",
    "    clf = LogisticRegression(\n",
    "        solver=\"liblinear\", multi_class=\"ovr\", penalty=\"l2\",\n",
    "        C=1.0, max_iter=2000, tol=1e-3, n_jobs=1, random_state=42\n",
    "    )\n",
    "    return Pipeline([\n",
    "        (\"dict\", DictVectorizer(sparse=True)),\n",
    "        (\"tfidf\", TfidfTransformer()),\n",
    "        (\"lr\", clf)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b62d5f",
   "metadata": {},
   "source": [
    "En paralelo, se contemplan configuraciones que aprovechan directamente las características derivadas del léxico. En este caso, los vectores sentimentales generados se normalizan y se introducen en clasificadores como Regresión Logística o Gaussian Naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2dca159b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lr_lexicon(lexicon: Dict[str, Tuple[float, float]]):\n",
    "    clf = LogisticRegression(\n",
    "        solver=\"liblinear\", multi_class=\"ovr\", penalty=\"l2\",\n",
    "        C=1.0, max_iter=2000, tol=1e-3, n_jobs=1, random_state=42\n",
    "    )\n",
    "    return Pipeline([\n",
    "        (\"lex\", SentiWordNetFeatures(lexicon)),\n",
    "        (\"sc\", StandardScaler(with_mean=False)),\n",
    "        (\"lr\", clf)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2cc8d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_nb_lexicon(lexicon: Dict[str, Tuple[float, float]]):\n",
    "    # Usar GaussianNB para características continuas del léxico\n",
    "    clf = GaussianNB()\n",
    "    return Pipeline([\n",
    "        (\"lex\", SentiWordNetFeatures(lexicon)),\n",
    "        (\"sc\", StandardScaler(with_mean=False)),\n",
    "        (\"nb\", clf)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d43bbc0",
   "metadata": {},
   "source": [
    "## Ejecución del Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e06e0a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rutas (AJUSTAR SEGÚN TU SISTEMA)\n",
    "ROOT = Path(r\"Datasets/Multi Domain Sentiment/processed_acl/processed_acl\")\n",
    "CATEGORIES = [\"Books\", \"DVD\", \"Electronics\", \"Kitchen\"]\n",
    "SWN_PATH = Path(r\"Datasets/EN_Lexicons/SentiWordNet_3.0.0.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7081248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando datos por categoría...\n",
      "[Books] pos=1000 neg=1000 unl=4465\n",
      "[DVD] pos=1000 neg=1000 unl=3586\n",
      "[Electronics] pos=1000 neg=1000 unl=5681\n",
      "[Kitchen] pos=1000 neg=1000 unl=5945\n"
     ]
    }
   ],
   "source": [
    "print(\"Cargando datos por categoría...\")\n",
    "data_by_category = load_all_domains_by_category(ROOT, CATEGORIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "588608e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cargando léxico SentiWordNet...\n",
      "Léxico cargado: 147306 palabras\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nCargando léxico SentiWordNet...\")\n",
    "lexicon = load_sentiwordnet(SWN_PATH)\n",
    "print(f\"Léxico cargado: {len(lexicon)} palabras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b6fee1",
   "metadata": {},
   "source": [
    "## Evaluación de los modelos individuales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d51b7bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluar_modelo(nombre, pipe, X_tr, y_tr, X_te, y_te, mostrar_reporte=False):\n",
    "    \"\"\"Evalúa un modelo y retorna métricas\"\"\"\n",
    "    pipe.fit(X_tr, y_tr)\n",
    "    y_pred = pipe.predict(X_te)\n",
    "    \n",
    "    p_mac, r_mac, f1_mac, _ = precision_recall_fscore_support(y_te, y_pred, average=\"macro\", zero_division=0)\n",
    "    p_mic, r_mic, f1_mic, _ = precision_recall_fscore_support(y_te, y_pred, average=\"micro\", zero_division=0)\n",
    "    acc = accuracy_score(y_te, y_pred)\n",
    "    \n",
    "    if mostrar_reporte:\n",
    "        print(f\"\\n{nombre}\")\n",
    "        print(\"Accuracy: {:.4f}\".format(acc))\n",
    "        print(\"Macro  -> P: {:.4f}  R: {:.4f}  F1: {:.4f}\".format(p_mac, r_mac, f1_mac))\n",
    "        print(\"Micro  -> P: {:.4f}  R: {:.4f}  F1: {:.4f}\".format(p_mic, r_mic, f1_mic))\n",
    "        print(\"\\nReporte por clase:\")\n",
    "        print(classification_report(y_te, y_pred, target_names=[\"negative\", \"positive\"], digits=3, zero_division=0))\n",
    "    \n",
    "    return {\n",
    "        \"Accuracy\": acc,\n",
    "        \"F1 Macro\": f1_mac,\n",
    "        \"F1 Micro\": f1_mic,\n",
    "        \"Precision Macro\": p_mac,\n",
    "        \"Recall Macro\": r_mac\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43524c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analizar_categoria(categoria, data_cat, lexicon):\n",
    "    \"\"\"Analiza una categoría específica con los 6 modelos\"\"\"\n",
    "    print(f\"\\n{'='*100}\")\n",
    "    print(f\"ANÁLISIS CATEGORÍA: {categoria.upper()}\")\n",
    "    print(f\"{'='*100}\")\n",
    "    \n",
    "    # Preparar datos\n",
    "    X_lab = data_cat[\"X_pos\"] + data_cat[\"X_neg\"]\n",
    "    y_lab = data_cat[\"y_pos\"] + data_cat[\"y_neg\"]\n",
    "    \n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "        X_lab, y_lab, test_size=0.30, random_state=42, stratify=y_lab\n",
    "    )\n",
    "    \n",
    "    # Definir los 6 modelos\n",
    "    modelos = {\n",
    "        \"NB + TF\": make_nb_tf(),\n",
    "        \"NB + TF-IDF\": make_nb_tfidf(),\n",
    "        \"LR + TF\": make_lr_tf(),\n",
    "        \"LR + TF-IDF\": make_lr_tfidf(),\n",
    "        \"LR + Lexicon(SWN)\": make_lr_lexicon(lexicon),\n",
    "        \"NB + Lexicon(SWN)\": make_nb_lexicon(lexicon)\n",
    "    }\n",
    "    \n",
    "    # Evaluar cada modelo\n",
    "    resultados = {}\n",
    "    modelos_entrenados = {}\n",
    "    \n",
    "    for nombre, pipe in modelos.items():\n",
    "        print(f\"\\nEntrenando {nombre}...\")\n",
    "        try:\n",
    "            resultado = evaluar_modelo(f\"[{categoria}] {nombre}\", pipe, X_tr, y_tr, X_te, y_te, mostrar_reporte=True)\n",
    "            resultados[nombre] = resultado\n",
    "            modelos_entrenados[nombre] = pipe\n",
    "        except Exception as e:\n",
    "            print(f\"Error con {nombre}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Crear tabla resumen\n",
    "    df_resultados = pd.DataFrame(resultados).T\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"RESUMEN {categoria}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(df_resultados[[\"Accuracy\", \"F1 Macro\", \"F1 Micro\"]].round(4))\n",
    "    \n",
    "    # Encontrar mejor modelo y predecir unlabeled\n",
    "    if resultados:\n",
    "        mejor_nombre = max(resultados.keys(), key=lambda x: resultados[x][\"F1 Macro\"])\n",
    "        X_unl = data_cat[\"X_unl\"]\n",
    "        if X_unl:\n",
    "            y_pred_unl = modelos_entrenados[mejor_nombre].predict(X_unl)\n",
    "            print(f\"\\nMejor modelo: {mejor_nombre}\")\n",
    "            print(f\"Predicciones unlabeled (primeros 10): {list(y_pred_unl[:10])}\")\n",
    "            print(f\"Distribución unlabeled: Pos={np.sum(y_pred_unl)} | Neg={len(y_pred_unl)-np.sum(y_pred_unl)}\")\n",
    "    \n",
    "    return df_resultados, modelos_entrenados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b91bfa",
   "metadata": {},
   "source": [
    "### Evaluación de resultados en una categoría específica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "62102611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "ANÁLISIS CATEGORÍA: BOOKS\n",
      "====================================================================================================\n",
      "\n",
      "Entrenando NB + TF...\n",
      "\n",
      "[Books] NB + TF\n",
      "Accuracy: 0.7650\n",
      "Macro  -> P: 0.7837  R: 0.7650  F1: 0.7611\n",
      "Micro  -> P: 0.7650  R: 0.7650  F1: 0.7650\n",
      "\n",
      "Reporte por clase:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.711     0.893     0.792       300\n",
      "    positive      0.857     0.637     0.730       300\n",
      "\n",
      "    accuracy                          0.765       600\n",
      "   macro avg      0.784     0.765     0.761       600\n",
      "weighted avg      0.784     0.765     0.761       600\n",
      "\n",
      "\n",
      "Entrenando NB + TF-IDF...\n",
      "\n",
      "[Books] NB + TF-IDF\n",
      "Accuracy: 0.7883\n",
      "Macro  -> P: 0.8162  R: 0.7883  F1: 0.7836\n",
      "Micro  -> P: 0.7883  R: 0.7883  F1: 0.7883\n",
      "\n",
      "Reporte por clase:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.722     0.937     0.816       300\n",
      "    positive      0.910     0.640     0.751       300\n",
      "\n",
      "    accuracy                          0.788       600\n",
      "   macro avg      0.816     0.788     0.784       600\n",
      "weighted avg      0.816     0.788     0.784       600\n",
      "\n",
      "\n",
      "Entrenando LR + TF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\ir-gensim\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1281: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Books] LR + TF\n",
      "Accuracy: 0.8067\n",
      "Macro  -> P: 0.8078  R: 0.8067  F1: 0.8065\n",
      "Micro  -> P: 0.8067  R: 0.8067  F1: 0.8067\n",
      "\n",
      "Reporte por clase:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.789     0.837     0.812       300\n",
      "    positive      0.826     0.777     0.801       300\n",
      "\n",
      "    accuracy                          0.807       600\n",
      "   macro avg      0.808     0.807     0.806       600\n",
      "weighted avg      0.808     0.807     0.806       600\n",
      "\n",
      "\n",
      "Entrenando LR + TF-IDF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\ir-gensim\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1281: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Books] LR + TF-IDF\n",
      "Accuracy: 0.8033\n",
      "Macro  -> P: 0.8106  R: 0.8033  F1: 0.8022\n",
      "Micro  -> P: 0.8033  R: 0.8033  F1: 0.8033\n",
      "\n",
      "Reporte por clase:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.763     0.880     0.817       300\n",
      "    positive      0.858     0.727     0.787       300\n",
      "\n",
      "    accuracy                          0.803       600\n",
      "   macro avg      0.811     0.803     0.802       600\n",
      "weighted avg      0.811     0.803     0.802       600\n",
      "\n",
      "\n",
      "Entrenando LR + Lexicon(SWN)...\n",
      "\n",
      "[Books] LR + Lexicon(SWN)\n",
      "Accuracy: 0.7100\n",
      "Macro  -> P: 0.7105  R: 0.7100  F1: 0.7098\n",
      "Micro  -> P: 0.7100  R: 0.7100  F1: 0.7100\n",
      "\n",
      "Reporte por clase:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.701     0.733     0.717       300\n",
      "    positive      0.720     0.687     0.703       300\n",
      "\n",
      "    accuracy                          0.710       600\n",
      "   macro avg      0.710     0.710     0.710       600\n",
      "weighted avg      0.710     0.710     0.710       600\n",
      "\n",
      "\n",
      "Entrenando NB + Lexicon(SWN)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\ir-gensim\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1281: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Books] NB + Lexicon(SWN)\n",
      "Accuracy: 0.6467\n",
      "Macro  -> P: 0.6478  R: 0.6467  F1: 0.6460\n",
      "Micro  -> P: 0.6467  R: 0.6467  F1: 0.6467\n",
      "\n",
      "Reporte por clase:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.661     0.603     0.631       300\n",
      "    positive      0.635     0.690     0.661       300\n",
      "\n",
      "    accuracy                          0.647       600\n",
      "   macro avg      0.648     0.647     0.646       600\n",
      "weighted avg      0.648     0.647     0.646       600\n",
      "\n",
      "\n",
      "============================================================\n",
      "RESUMEN Books\n",
      "============================================================\n",
      "                   Accuracy  F1 Macro  F1 Micro\n",
      "NB + TF              0.7650    0.7611    0.7650\n",
      "NB + TF-IDF          0.7883    0.7836    0.7883\n",
      "LR + TF              0.8067    0.8065    0.8067\n",
      "LR + TF-IDF          0.8033    0.8022    0.8033\n",
      "LR + Lexicon(SWN)    0.7100    0.7098    0.7100\n",
      "NB + Lexicon(SWN)    0.6467    0.6460    0.6467\n",
      "\n",
      "Mejor modelo: LR + TF\n",
      "Predicciones unlabeled (primeros 10): [0, 0, 1, 0, 1, 1, 1, 0, 1, 1]\n",
      "Distribución unlabeled: Pos=2246 | Neg=2219\n",
      "\n",
      "====================================================================================================\n",
      "ANÁLISIS CATEGORÍA: DVD\n",
      "====================================================================================================\n",
      "\n",
      "Entrenando NB + TF...\n",
      "\n",
      "[DVD] NB + TF\n",
      "Accuracy: 0.8517\n",
      "Macro  -> P: 0.8550  R: 0.8517  F1: 0.8513\n",
      "Micro  -> P: 0.8517  R: 0.8517  F1: 0.8517\n",
      "\n",
      "Reporte por clase:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.821     0.900     0.859       300\n",
      "    positive      0.889     0.803     0.844       300\n",
      "\n",
      "    accuracy                          0.852       600\n",
      "   macro avg      0.855     0.852     0.851       600\n",
      "weighted avg      0.855     0.852     0.851       600\n",
      "\n",
      "\n",
      "Entrenando NB + TF-IDF...\n",
      "\n",
      "[DVD] NB + TF-IDF\n",
      "Accuracy: 0.8433\n",
      "Macro  -> P: 0.8489  R: 0.8433  F1: 0.8427\n",
      "Micro  -> P: 0.8433  R: 0.8433  F1: 0.8433\n",
      "\n",
      "Reporte por clase:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.805     0.907     0.853       300\n",
      "    positive      0.893     0.780     0.833       300\n",
      "\n",
      "    accuracy                          0.843       600\n",
      "   macro avg      0.849     0.843     0.843       600\n",
      "weighted avg      0.849     0.843     0.843       600\n",
      "\n",
      "\n",
      "Entrenando LR + TF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\ir-gensim\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1281: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[DVD] LR + TF\n",
      "Accuracy: 0.8167\n",
      "Macro  -> P: 0.8167  R: 0.8167  F1: 0.8167\n",
      "Micro  -> P: 0.8167  R: 0.8167  F1: 0.8167\n",
      "\n",
      "Reporte por clase:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.815     0.820     0.817       300\n",
      "    positive      0.819     0.813     0.816       300\n",
      "\n",
      "    accuracy                          0.817       600\n",
      "   macro avg      0.817     0.817     0.817       600\n",
      "weighted avg      0.817     0.817     0.817       600\n",
      "\n",
      "\n",
      "Entrenando LR + TF-IDF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\ir-gensim\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1281: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[DVD] LR + TF-IDF\n",
      "Accuracy: 0.8500\n",
      "Macro  -> P: 0.8506  R: 0.8500  F1: 0.8499\n",
      "Micro  -> P: 0.8500  R: 0.8500  F1: 0.8500\n",
      "\n",
      "Reporte por clase:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.837     0.870     0.853       300\n",
      "    positive      0.865     0.830     0.847       300\n",
      "\n",
      "    accuracy                          0.850       600\n",
      "   macro avg      0.851     0.850     0.850       600\n",
      "weighted avg      0.851     0.850     0.850       600\n",
      "\n",
      "\n",
      "Entrenando LR + Lexicon(SWN)...\n",
      "\n",
      "[DVD] LR + Lexicon(SWN)\n",
      "Accuracy: 0.6950\n",
      "Macro  -> P: 0.6950  R: 0.6950  F1: 0.6950\n",
      "Micro  -> P: 0.6950  R: 0.6950  F1: 0.6950\n",
      "\n",
      "Reporte por clase:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.693     0.700     0.697       300\n",
      "    positive      0.697     0.690     0.693       300\n",
      "\n",
      "    accuracy                          0.695       600\n",
      "   macro avg      0.695     0.695     0.695       600\n",
      "weighted avg      0.695     0.695     0.695       600\n",
      "\n",
      "\n",
      "Entrenando NB + Lexicon(SWN)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\ir-gensim\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1281: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[DVD] NB + Lexicon(SWN)\n",
      "Accuracy: 0.6633\n",
      "Macro  -> P: 0.6722  R: 0.6633  F1: 0.6590\n",
      "Micro  -> P: 0.6633  R: 0.6633  F1: 0.6633\n",
      "\n",
      "Reporte por clase:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.633     0.777     0.698       300\n",
      "    positive      0.711     0.550     0.620       300\n",
      "\n",
      "    accuracy                          0.663       600\n",
      "   macro avg      0.672     0.663     0.659       600\n",
      "weighted avg      0.672     0.663     0.659       600\n",
      "\n",
      "\n",
      "============================================================\n",
      "RESUMEN DVD\n",
      "============================================================\n",
      "                   Accuracy  F1 Macro  F1 Micro\n",
      "NB + TF              0.8517    0.8513    0.8517\n",
      "NB + TF-IDF          0.8433    0.8427    0.8433\n",
      "LR + TF              0.8167    0.8167    0.8167\n",
      "LR + TF-IDF          0.8500    0.8499    0.8500\n",
      "LR + Lexicon(SWN)    0.6950    0.6950    0.6950\n",
      "NB + Lexicon(SWN)    0.6633    0.6590    0.6633\n",
      "\n",
      "Mejor modelo: NB + TF\n",
      "Predicciones unlabeled (primeros 10): [1, 1, 0, 0, 0, 1, 1, 0, 1, 1]\n",
      "Distribución unlabeled: Pos=1740 | Neg=1846\n",
      "\n",
      "====================================================================================================\n",
      "ANÁLISIS CATEGORÍA: ELECTRONICS\n",
      "====================================================================================================\n",
      "\n",
      "Entrenando NB + TF...\n",
      "\n",
      "[Electronics] NB + TF\n",
      "Accuracy: 0.8283\n",
      "Macro  -> P: 0.8285  R: 0.8283  F1: 0.8283\n",
      "Micro  -> P: 0.8283  R: 0.8283  F1: 0.8283\n",
      "\n",
      "Reporte por clase:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.836     0.817     0.826       300\n",
      "    positive      0.821     0.840     0.830       300\n",
      "\n",
      "    accuracy                          0.828       600\n",
      "   macro avg      0.829     0.828     0.828       600\n",
      "weighted avg      0.829     0.828     0.828       600\n",
      "\n",
      "\n",
      "Entrenando NB + TF-IDF...\n",
      "\n",
      "[Electronics] NB + TF-IDF\n",
      "Accuracy: 0.8450\n",
      "Macro  -> P: 0.8452  R: 0.8450  F1: 0.8450\n",
      "Micro  -> P: 0.8450  R: 0.8450  F1: 0.8450\n",
      "\n",
      "Reporte por clase:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.837     0.857     0.847       300\n",
      "    positive      0.853     0.833     0.843       300\n",
      "\n",
      "    accuracy                          0.845       600\n",
      "   macro avg      0.845     0.845     0.845       600\n",
      "weighted avg      0.845     0.845     0.845       600\n",
      "\n",
      "\n",
      "Entrenando LR + TF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\ir-gensim\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1281: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Electronics] LR + TF\n",
      "Accuracy: 0.8350\n",
      "Macro  -> P: 0.8350  R: 0.8350  F1: 0.8350\n",
      "Micro  -> P: 0.8350  R: 0.8350  F1: 0.8350\n",
      "\n",
      "Reporte por clase:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.834     0.837     0.835       300\n",
      "    positive      0.836     0.833     0.835       300\n",
      "\n",
      "    accuracy                          0.835       600\n",
      "   macro avg      0.835     0.835     0.835       600\n",
      "weighted avg      0.835     0.835     0.835       600\n",
      "\n",
      "\n",
      "Entrenando LR + TF-IDF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\ir-gensim\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1281: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\anaconda3\\envs\\ir-gensim\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1281: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Electronics] LR + TF-IDF\n",
      "Accuracy: 0.8300\n",
      "Macro  -> P: 0.8302  R: 0.8300  F1: 0.8300\n",
      "Micro  -> P: 0.8300  R: 0.8300  F1: 0.8300\n",
      "\n",
      "Reporte por clase:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.839     0.817     0.828       300\n",
      "    positive      0.821     0.843     0.832       300\n",
      "\n",
      "    accuracy                          0.830       600\n",
      "   macro avg      0.830     0.830     0.830       600\n",
      "weighted avg      0.830     0.830     0.830       600\n",
      "\n",
      "\n",
      "Entrenando LR + Lexicon(SWN)...\n",
      "\n",
      "[Electronics] LR + Lexicon(SWN)\n",
      "Accuracy: 0.7450\n",
      "Macro  -> P: 0.7470  R: 0.7450  F1: 0.7445\n",
      "Micro  -> P: 0.7450  R: 0.7450  F1: 0.7450\n",
      "\n",
      "Reporte por clase:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.725     0.790     0.756       300\n",
      "    positive      0.769     0.700     0.733       300\n",
      "\n",
      "    accuracy                          0.745       600\n",
      "   macro avg      0.747     0.745     0.744       600\n",
      "weighted avg      0.747     0.745     0.744       600\n",
      "\n",
      "\n",
      "Entrenando NB + Lexicon(SWN)...\n",
      "\n",
      "[Electronics] NB + Lexicon(SWN)\n",
      "Accuracy: 0.7067\n",
      "Macro  -> P: 0.7208  R: 0.7067  F1: 0.7019\n",
      "Micro  -> P: 0.7067  R: 0.7067  F1: 0.7067\n",
      "\n",
      "Reporte por clase:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.665     0.833     0.740       300\n",
      "    positive      0.777     0.580     0.664       300\n",
      "\n",
      "    accuracy                          0.707       600\n",
      "   macro avg      0.721     0.707     0.702       600\n",
      "weighted avg      0.721     0.707     0.702       600\n",
      "\n",
      "\n",
      "============================================================\n",
      "RESUMEN Electronics\n",
      "============================================================\n",
      "                   Accuracy  F1 Macro  F1 Micro\n",
      "NB + TF              0.8283    0.8283    0.8283\n",
      "NB + TF-IDF          0.8450    0.8450    0.8450\n",
      "LR + TF              0.8350    0.8350    0.8350\n",
      "LR + TF-IDF          0.8300    0.8300    0.8300\n",
      "LR + Lexicon(SWN)    0.7450    0.7445    0.7450\n",
      "NB + Lexicon(SWN)    0.7067    0.7019    0.7067\n",
      "\n",
      "Mejor modelo: NB + TF-IDF\n",
      "Predicciones unlabeled (primeros 10): [0, 0, 0, 1, 1, 1, 0, 0, 1, 1]\n",
      "Distribución unlabeled: Pos=2804 | Neg=2877\n",
      "\n",
      "====================================================================================================\n",
      "ANÁLISIS CATEGORÍA: KITCHEN\n",
      "====================================================================================================\n",
      "\n",
      "Entrenando NB + TF...\n",
      "\n",
      "[Kitchen] NB + TF\n",
      "Accuracy: 0.8700\n",
      "Macro  -> P: 0.8703  R: 0.8700  F1: 0.8700\n",
      "Micro  -> P: 0.8700  R: 0.8700  F1: 0.8700\n",
      "\n",
      "Reporte por clase:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.880     0.857     0.868       300\n",
      "    positive      0.860     0.883     0.872       300\n",
      "\n",
      "    accuracy                          0.870       600\n",
      "   macro avg      0.870     0.870     0.870       600\n",
      "weighted avg      0.870     0.870     0.870       600\n",
      "\n",
      "\n",
      "Entrenando NB + TF-IDF...\n",
      "\n",
      "[Kitchen] NB + TF-IDF\n",
      "Accuracy: 0.8800\n",
      "Macro  -> P: 0.8806  R: 0.8800  F1: 0.8800\n",
      "Micro  -> P: 0.8800  R: 0.8800  F1: 0.8800\n",
      "\n",
      "Reporte por clase:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.896     0.860     0.878       300\n",
      "    positive      0.865     0.900     0.882       300\n",
      "\n",
      "    accuracy                          0.880       600\n",
      "   macro avg      0.881     0.880     0.880       600\n",
      "weighted avg      0.881     0.880     0.880       600\n",
      "\n",
      "\n",
      "Entrenando LR + TF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\ir-gensim\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1281: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Kitchen] LR + TF\n",
      "Accuracy: 0.8850\n",
      "Macro  -> P: 0.8850  R: 0.8850  F1: 0.8850\n",
      "Micro  -> P: 0.8850  R: 0.8850  F1: 0.8850\n",
      "\n",
      "Reporte por clase:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.884     0.887     0.885       300\n",
      "    positive      0.886     0.883     0.885       300\n",
      "\n",
      "    accuracy                          0.885       600\n",
      "   macro avg      0.885     0.885     0.885       600\n",
      "weighted avg      0.885     0.885     0.885       600\n",
      "\n",
      "\n",
      "Entrenando LR + TF-IDF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\ir-gensim\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1281: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\anaconda3\\envs\\ir-gensim\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1281: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Kitchen] LR + TF-IDF\n",
      "Accuracy: 0.8833\n",
      "Macro  -> P: 0.8834  R: 0.8833  F1: 0.8833\n",
      "Micro  -> P: 0.8833  R: 0.8833  F1: 0.8833\n",
      "\n",
      "Reporte por clase:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.889     0.877     0.883       300\n",
      "    positive      0.878     0.890     0.884       300\n",
      "\n",
      "    accuracy                          0.883       600\n",
      "   macro avg      0.883     0.883     0.883       600\n",
      "weighted avg      0.883     0.883     0.883       600\n",
      "\n",
      "\n",
      "Entrenando LR + Lexicon(SWN)...\n",
      "\n",
      "[Kitchen] LR + Lexicon(SWN)\n",
      "Accuracy: 0.7417\n",
      "Macro  -> P: 0.7417  R: 0.7417  F1: 0.7417\n",
      "Micro  -> P: 0.7417  R: 0.7417  F1: 0.7417\n",
      "\n",
      "Reporte por clase:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.739     0.747     0.743       300\n",
      "    positive      0.744     0.737     0.740       300\n",
      "\n",
      "    accuracy                          0.742       600\n",
      "   macro avg      0.742     0.742     0.742       600\n",
      "weighted avg      0.742     0.742     0.742       600\n",
      "\n",
      "\n",
      "Entrenando NB + Lexicon(SWN)...\n",
      "\n",
      "[Kitchen] NB + Lexicon(SWN)\n",
      "Accuracy: 0.7367\n",
      "Macro  -> P: 0.7414  R: 0.7367  F1: 0.7354\n",
      "Micro  -> P: 0.7367  R: 0.7367  F1: 0.7367\n",
      "\n",
      "Reporte por clase:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.708     0.807     0.754       300\n",
      "    positive      0.775     0.667     0.717       300\n",
      "\n",
      "    accuracy                          0.737       600\n",
      "   macro avg      0.741     0.737     0.735       600\n",
      "weighted avg      0.741     0.737     0.735       600\n",
      "\n",
      "\n",
      "============================================================\n",
      "RESUMEN Kitchen\n",
      "============================================================\n",
      "                   Accuracy  F1 Macro  F1 Micro\n",
      "NB + TF              0.8700    0.8700    0.8700\n",
      "NB + TF-IDF          0.8800    0.8800    0.8800\n",
      "LR + TF              0.8850    0.8850    0.8850\n",
      "LR + TF-IDF          0.8833    0.8833    0.8833\n",
      "LR + Lexicon(SWN)    0.7417    0.7417    0.7417\n",
      "NB + Lexicon(SWN)    0.7367    0.7354    0.7367\n",
      "\n",
      "Mejor modelo: LR + TF\n",
      "Predicciones unlabeled (primeros 10): [0, 0, 0, 0, 0, 1, 0, 0, 1, 0]\n",
      "Distribución unlabeled: Pos=2937 | Neg=3008\n"
     ]
    }
   ],
   "source": [
    "# Ejecutar análisis por categoría\n",
    "tablas_categoria = {}\n",
    "modelos_categoria = {}\n",
    "\n",
    "for categoria in CATEGORIES:\n",
    "    df_cat, models_cat = analizar_categoria(categoria, data_by_category[categoria], lexicon)\n",
    "    tablas_categoria[categoria] = df_cat\n",
    "    modelos_categoria[categoria] = models_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c4b9bc",
   "metadata": {},
   "source": [
    "### Evaluación de resultados TF vs TF-IDF vs Lexicons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "88ccbf93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COMPARACIÓN POR TIPO DE REPRESENTACIÓN DE CARACTERÍSTICAS\n",
      "================================================================================\n",
      "\n",
      "RENDIMIENTO PROMEDIO POR TIPO DE REPRESENTACIÓN:\n",
      "------------------------------------------------------------\n",
      "                     F1 Macro Promedio  F1 Macro Std  Accuracy Promedio  \\\n",
      "TF-IDF                          0.8396        0.0322             0.8404   \n",
      "TF (Term Frequency)             0.8317        0.0363             0.8323   \n",
      "Lexicon Features                0.7041        0.0346             0.7056   \n",
      "\n",
      "                     Accuracy Std  Mejor F1  Peor F1  \n",
      "TF-IDF                     0.0311    0.8833   0.7836  \n",
      "TF (Term Frequency)        0.0354    0.8850   0.7611  \n",
      "Lexicon Features           0.0339    0.7445   0.6460  \n",
      "\n",
      "\n",
      "ANÁLISIS DE ESTABILIDAD (Desviación Estándar):\n",
      "--------------------------------------------------\n",
      "Representación más estable (menor std):\n",
      "                     F1 Macro Std  Accuracy Std\n",
      "TF-IDF                     0.0322        0.0311\n",
      "Lexicon Features           0.0346        0.0339\n",
      "TF (Term Frequency)        0.0363        0.0354\n",
      "\n",
      "================================================================================\n",
      "MEJOR REPRESENTACIÓN POR CATEGORÍA\n",
      "================================================================================\n",
      "\n",
      "Books:\n",
      "----------------------------------------\n",
      "  TF          : F1=0.8065 (LR + TF)\n",
      "  TF-IDF      : F1=0.8022 (LR + TF-IDF)\n",
      "  Lexicon     : F1=0.7098 (LR + Lexicon(SWN))\n",
      "  → MEJOR: TF (F1=0.8065)\n",
      "\n",
      "DVD:\n",
      "----------------------------------------\n",
      "  TF          : F1=0.8513 (NB + TF)\n",
      "  TF-IDF      : F1=0.8499 (LR + TF-IDF)\n",
      "  Lexicon     : F1=0.6950 (LR + Lexicon(SWN))\n",
      "  → MEJOR: TF (F1=0.8513)\n",
      "\n",
      "Electronics:\n",
      "----------------------------------------\n",
      "  TF          : F1=0.8350 (LR + TF)\n",
      "  TF-IDF      : F1=0.8450 (NB + TF-IDF)\n",
      "  Lexicon     : F1=0.7445 (LR + Lexicon(SWN))\n",
      "  → MEJOR: TF-IDF (F1=0.8450)\n",
      "\n",
      "Kitchen:\n",
      "----------------------------------------\n",
      "  TF          : F1=0.8850 (LR + TF)\n",
      "  TF-IDF      : F1=0.8833 (LR + TF-IDF)\n",
      "  Lexicon     : F1=0.7417 (LR + Lexicon(SWN))\n",
      "  → MEJOR: TF (F1=0.8850)\n",
      "\n",
      "================================================================================\n",
      "ANÁLISIS DE COMPLEMENTARIEDAD ENTRE REPRESENTACIONES\n",
      "================================================================================\n",
      "\n",
      "RESUMEN DE OBSERVACIONES:\n",
      "--------------------------------------------------\n",
      "\n",
      "1. BRECHAS DE RENDIMIENTO:\n",
      "  Books       : TF-IDF vs TF = -0.0043, Lexicon vs TF-IDF = -0.0923\n",
      "  DVD         : TF-IDF vs TF = -0.0014, Lexicon vs TF-IDF = -0.1549\n",
      "  Electronics : TF-IDF vs TF = +0.0100, Lexicon vs TF-IDF = -0.1005\n",
      "  Kitchen     : TF-IDF vs TF = -0.0017, Lexicon vs TF-IDF = -0.1417\n",
      "\n",
      "2. INTERPRETACIÓN:\n",
      "  • TF-IDF > TF: TF-IDF normaliza y reduce ruido de palabras muy frecuentes\n",
      "  • Lexicon vs TF-IDF: Características semánticas vs estadísticas\n",
      "  • Variación entre categorías indica especificidad de dominio\n"
     ]
    }
   ],
   "source": [
    "def comparar_por_tipo_representacion(tablas_categoria, categorias):\n",
    "    \"\"\"\n",
    "    Compara el rendimiento promedio por tipo de representación\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"COMPARACIÓN POR TIPO DE REPRESENTACIÓN DE CARACTERÍSTICAS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Agrupar modelos por tipo de representación\n",
    "    tipos_representacion = {\n",
    "        'TF (Term Frequency)': ['NB + TF', 'LR + TF'],\n",
    "        'TF-IDF': ['NB + TF-IDF', 'LR + TF-IDF'], \n",
    "        'Lexicon Features': ['LR + Lexicon(SWN)', 'NB + Lexicon(SWN)']\n",
    "    }\n",
    "    \n",
    "    resultados_por_tipo = {}\n",
    "    \n",
    "    for tipo, modelos in tipos_representacion.items():\n",
    "        f1_scores = []\n",
    "        accuracy_scores = []\n",
    "        \n",
    "        for categoria in categorias:\n",
    "            if categoria in tablas_categoria:\n",
    "                tabla = tablas_categoria[categoria]\n",
    "                for modelo in modelos:\n",
    "                    if modelo in tabla.index:\n",
    "                        f1_scores.append(tabla.loc[modelo, 'F1 Macro'])\n",
    "                        accuracy_scores.append(tabla.loc[modelo, 'Accuracy'])\n",
    "        \n",
    "        if f1_scores:\n",
    "            resultados_por_tipo[tipo] = {\n",
    "                'F1 Macro Promedio': np.mean(f1_scores),\n",
    "                'F1 Macro Std': np.std(f1_scores),\n",
    "                'Accuracy Promedio': np.mean(accuracy_scores),\n",
    "                'Accuracy Std': np.std(accuracy_scores),\n",
    "                'Mejor F1': np.max(f1_scores),\n",
    "                'Peor F1': np.min(f1_scores)\n",
    "            }\n",
    "    \n",
    "    # Crear DataFrame para comparación\n",
    "    df_tipos = pd.DataFrame(resultados_por_tipo).T\n",
    "    df_tipos = df_tipos.sort_values('F1 Macro Promedio', ascending=False)\n",
    "    \n",
    "    print(\"\\nRENDIMIENTO PROMEDIO POR TIPO DE REPRESENTACIÓN:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(df_tipos.round(4))\n",
    "    \n",
    "    # Análisis de estabilidad\n",
    "    print(\"\\n\\nANÁLISIS DE ESTABILIDAD (Desviación Estándar):\")\n",
    "    print(\"-\" * 50)\n",
    "    estabilidad = df_tipos[['F1 Macro Std', 'Accuracy Std']].sort_values('F1 Macro Std')\n",
    "    print(\"Representación más estable (menor std):\")\n",
    "    print(estabilidad.round(4))\n",
    "    \n",
    "    return df_tipos\n",
    "\n",
    "def comparar_por_categoria_representacion(tablas_categoria, categorias):\n",
    "    \"\"\"\n",
    "    Analiza qué representación funciona mejor en cada categoría\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"MEJOR REPRESENTACIÓN POR CATEGORÍA\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    mejor_por_categoria = {}\n",
    "    \n",
    "    for categoria in categorias:\n",
    "        if categoria in tablas_categoria:\n",
    "            tabla = tablas_categoria[categoria]\n",
    "            \n",
    "            # Agrupar por tipo de representación\n",
    "            tf_models = [modelo for modelo in tabla.index if 'TF' in modelo and 'TF-IDF' not in modelo]\n",
    "            tfidf_models = [modelo for modelo in tabla.index if 'TF-IDF' in modelo]\n",
    "            lexicon_models = [modelo for modelo in tabla.index if 'Lexicon' in modelo]\n",
    "            \n",
    "            resultados_categoria = {}\n",
    "            \n",
    "            # Mejor modelo por tipo\n",
    "            if tf_models:\n",
    "                best_tf = tabla.loc[tf_models, 'F1 Macro'].max()\n",
    "                best_tf_model = tabla.loc[tf_models, 'F1 Macro'].idxmax()\n",
    "                resultados_categoria['TF'] = {'F1': best_tf, 'Modelo': best_tf_model}\n",
    "            \n",
    "            if tfidf_models:\n",
    "                best_tfidf = tabla.loc[tfidf_models, 'F1 Macro'].max()\n",
    "                best_tfidf_model = tabla.loc[tfidf_models, 'F1 Macro'].idxmax()\n",
    "                resultados_categoria['TF-IDF'] = {'F1': best_tfidf, 'Modelo': best_tfidf_model}\n",
    "            \n",
    "            if lexicon_models:\n",
    "                best_lexicon = tabla.loc[lexicon_models, 'F1 Macro'].max()\n",
    "                best_lexicon_model = tabla.loc[lexicon_models, 'F1 Macro'].idxmax()\n",
    "                resultados_categoria['Lexicon'] = {'F1': best_lexicon, 'Modelo': best_lexicon_model}\n",
    "            \n",
    "            mejor_por_categoria[categoria] = resultados_categoria\n",
    "            \n",
    "            print(f\"\\n{categoria}:\")\n",
    "            print(\"-\" * 40)\n",
    "            for repr_type, data in resultados_categoria.items():\n",
    "                print(f\"  {repr_type:12s}: F1={data['F1']:.4f} ({data['Modelo']})\")\n",
    "            \n",
    "            # Identificar la mejor representación para esta categoría\n",
    "            if resultados_categoria:\n",
    "                mejor_repr = max(resultados_categoria.keys(), \n",
    "                               key=lambda x: resultados_categoria[x]['F1'])\n",
    "                print(f\"  → MEJOR: {mejor_repr} (F1={resultados_categoria[mejor_repr]['F1']:.4f})\")\n",
    "    \n",
    "    return mejor_por_categoria\n",
    "\n",
    "def analizar_complementariedad_representaciones(tablas_categoria, categorias):\n",
    "    \"\"\"\n",
    "    Analiza si diferentes representaciones capturan aspectos complementarios\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ANÁLISIS DE COMPLEMENTARIEDAD ENTRE REPRESENTACIONES\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Calcular correlaciones entre tipos de representación\n",
    "    correlaciones = {}\n",
    "    \n",
    "    for categoria in categorias:\n",
    "        if categoria in tablas_categoria:\n",
    "            tabla = tablas_categoria[categoria]\n",
    "            \n",
    "            # Extraer F1 scores por tipo\n",
    "            tf_scores = []\n",
    "            tfidf_scores = []\n",
    "            lexicon_scores = []\n",
    "            \n",
    "            modelos_tf = [m for m in tabla.index if 'TF' in m and 'TF-IDF' not in m]\n",
    "            modelos_tfidf = [m for m in tabla.index if 'TF-IDF' in m]\n",
    "            modelos_lexicon = [m for m in tabla.index if 'Lexicon' in m]\n",
    "            \n",
    "            if modelos_tf:\n",
    "                tf_scores.extend(tabla.loc[modelos_tf, 'F1 Macro'].tolist())\n",
    "            if modelos_tfidf:\n",
    "                tfidf_scores.extend(tabla.loc[modelos_tfidf, 'F1 Macro'].tolist())\n",
    "            if modelos_lexicon:\n",
    "                lexicon_scores.extend(tabla.loc[modelos_lexicon, 'F1 Macro'].tolist())\n",
    "    \n",
    "    print(\"\\nRESUMEN DE OBSERVACIONES:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Análisis de brechas de rendimiento\n",
    "    print(\"\\n1. BRECHAS DE RENDIMIENTO:\")\n",
    "    for categoria in categorias:\n",
    "        if categoria in tablas_categoria:\n",
    "            tabla = tablas_categoria[categoria]\n",
    "            \n",
    "            tf_best = tabla.loc[[m for m in tabla.index if 'TF' in m and 'TF-IDF' not in m], 'F1 Macro'].max() if any('TF' in m and 'TF-IDF' not in m for m in tabla.index) else 0\n",
    "            tfidf_best = tabla.loc[[m for m in tabla.index if 'TF-IDF' in m], 'F1 Macro'].max() if any('TF-IDF' in m for m in tabla.index) else 0\n",
    "            lexicon_best = tabla.loc[[m for m in tabla.index if 'Lexicon' in m], 'F1 Macro'].max() if any('Lexicon' in m for m in tabla.index) else 0\n",
    "            \n",
    "            tfidf_vs_tf = tfidf_best - tf_best\n",
    "            lexicon_vs_tfidf = lexicon_best - tfidf_best\n",
    "            \n",
    "            print(f\"  {categoria:12s}: TF-IDF vs TF = {tfidf_vs_tf:+.4f}, Lexicon vs TF-IDF = {lexicon_vs_tfidf:+.4f}\")\n",
    "    \n",
    "    print(\"\\n2. INTERPRETACIÓN:\")\n",
    "    print(\"  • TF-IDF > TF: TF-IDF normaliza y reduce ruido de palabras muy frecuentes\")\n",
    "    print(\"  • Lexicon vs TF-IDF: Características semánticas vs estadísticas\")\n",
    "    print(\"  • Variación entre categorías indica especificidad de dominio\")\n",
    "\n",
    "# Función principal para ejecutar todas las comparaciones\n",
    "def ejecutar_comparacion_completa(tablas_categoria, categorias):\n",
    "    \"\"\"\n",
    "    Ejecuta todas las comparaciones de representación de características\n",
    "    \"\"\"\n",
    "    # 1. Comparación general por tipo\n",
    "    df_tipos = comparar_por_tipo_representacion(tablas_categoria, categorias)\n",
    "    \n",
    "    # 2. Mejor representación por categoría  \n",
    "    mejor_por_cat = comparar_por_categoria_representacion(tablas_categoria, categorias)\n",
    "    \n",
    "    # 3. Análisis de complementariedad\n",
    "    analizar_complementariedad_representaciones(tablas_categoria, categorias)\n",
    "    \n",
    "    return df_tipos, mejor_por_cat\n",
    "\n",
    "# uso:\n",
    "df_tipos, mejor_por_cat = ejecutar_comparacion_completa(tablas_categoria, CATEGORIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "df3bcfa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COMPARACIÓN DE ALGORITMOS: NAIVE BAYES vs LOGISTIC REGRESSION\n",
      "================================================================================\n",
      "\n",
      "RENDIMIENTO PROMEDIO POR ALGORITMO:\n",
      "------------------------------------------------------------\n",
      "                     F1 Macro Promedio  F1 Macro Std  Accuracy Promedio  \\\n",
      "Logistic Regression             0.8000        0.0611             0.8001   \n",
      "Naive Bayes                     0.7837        0.0788             0.7854   \n",
      "\n",
      "                     Accuracy Std  \n",
      "Logistic Regression        0.0610  \n",
      "Naive Bayes                0.0777  \n",
      "\n",
      "DIFERENCIA PROMEDIO (LR - NB): +0.0163\n",
      "→ Logistic Regression supera a Naive Bayes en promedio\n",
      "\n",
      "================================================================================\n",
      "ANÁLISIS NB vs LR POR CATEGORÍA\n",
      "================================================================================\n",
      "\n",
      "Books:\n",
      "----------------------------------------\n",
      "  Naive Bayes:\n",
      "    Mejor:    F1=0.7836 (NB + TF-IDF)\n",
      "    Promedio: F1=0.7302\n",
      "  Logistic Regression:\n",
      "    Mejor:    F1=0.8065 (LR + TF)\n",
      "    Promedio: F1=0.7728\n",
      "  → GANADOR: LR (ventaja: +0.0229)\n",
      "\n",
      "DVD:\n",
      "----------------------------------------\n",
      "  Naive Bayes:\n",
      "    Mejor:    F1=0.8513 (NB + TF)\n",
      "    Promedio: F1=0.7843\n",
      "  Logistic Regression:\n",
      "    Mejor:    F1=0.8499 (LR + TF-IDF)\n",
      "    Promedio: F1=0.7872\n",
      "  → GANADOR: NB (ventaja: +0.0014)\n",
      "\n",
      "Electronics:\n",
      "----------------------------------------\n",
      "  Naive Bayes:\n",
      "    Mejor:    F1=0.8450 (NB + TF-IDF)\n",
      "    Promedio: F1=0.7917\n",
      "  Logistic Regression:\n",
      "    Mejor:    F1=0.8350 (LR + TF)\n",
      "    Promedio: F1=0.8032\n",
      "  → GANADOR: NB (ventaja: +0.0100)\n",
      "\n",
      "Kitchen:\n",
      "----------------------------------------\n",
      "  Naive Bayes:\n",
      "    Mejor:    F1=0.8800 (NB + TF-IDF)\n",
      "    Promedio: F1=0.8284\n",
      "  Logistic Regression:\n",
      "    Mejor:    F1=0.8850 (LR + TF)\n",
      "    Promedio: F1=0.8367\n",
      "  → GANADOR: LR (ventaja: +0.0050)\n",
      "\n",
      "================================================================================\n",
      "NB vs LR POR TIPO DE REPRESENTACIÓN\n",
      "================================================================================\n",
      "\n",
      "TF:\n",
      "------------------------------\n",
      "  NB promedio:  0.8277\n",
      "  LR promedio:  0.8358\n",
      "  Diferencia:   +0.0081\n",
      "  → Rendimiento similar con TF\n",
      "\n",
      "TF-IDF:\n",
      "------------------------------\n",
      "  NB promedio:  0.8378\n",
      "  LR promedio:  0.8414\n",
      "  Diferencia:   +0.0036\n",
      "  → Rendimiento similar con TF-IDF\n",
      "\n",
      "Lexicon:\n",
      "------------------------------\n",
      "  NB promedio:  0.6856\n",
      "  LR promedio:  0.7227\n",
      "  Diferencia:   +0.0372\n",
      "  → LR claramente superior con Lexicon\n",
      "\n",
      "================================================================================\n",
      "ESTADÍSTICAS DETALLADAS: NB vs LR\n",
      "================================================================================\n",
      "\n",
      "COMPARACIÓN ESTADÍSTICA:\n",
      "----------------------------------------\n",
      "\n",
      "F1:\n",
      "  NB: μ=0.7837, σ=0.0788, min=0.6460, max=0.8800\n",
      "  LR: μ=0.8000, σ=0.0611, min=0.6950, max=0.8850\n",
      "  Diferencia (LR-NB): +0.0163\n",
      "\n",
      "Accuracy:\n",
      "  NB: μ=0.7854, σ=0.0777, min=0.6467, max=0.8800\n",
      "  LR: μ=0.8001, σ=0.0610, min=0.6950, max=0.8850\n",
      "  Diferencia (LR-NB): +0.0147\n",
      "\n",
      "Precision:\n",
      "  NB: μ=0.7925, σ=0.0757, min=0.6478, max=0.8806\n",
      "  LR: μ=0.8011, σ=0.0609, min=0.6950, max=0.8850\n",
      "  Diferencia (LR-NB): +0.0086\n",
      "\n",
      "Recall:\n",
      "  NB: μ=0.7854, σ=0.0777, min=0.6467, max=0.8800\n",
      "  LR: μ=0.8001, σ=0.0610, min=0.6950, max=0.8850\n",
      "  Diferencia (LR-NB): +0.0147\n",
      "\n",
      "CONTEO DE VICTORIAS:\n",
      "-------------------------\n",
      "  NB: 2/4 (50.0%)\n",
      "  LR: 2/4 (50.0%)\n",
      "  Empates: 0/4 (0.0%)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# COMPARACIÓN DE ALGORITMOS: NAIVE BAYES vs LOGISTIC REGRESSION\n",
    "# =============================================================================\n",
    "\n",
    "def comparar_nb_vs_lr(tablas_categoria, categorias):\n",
    "    \"\"\"\n",
    "    Compara el rendimiento promedio entre Naive Bayes y Logistic Regression\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"COMPARACIÓN DE ALGORITMOS: NAIVE BAYES vs LOGISTIC REGRESSION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Agrupar modelos por algoritmo\n",
    "    algoritmos = {\n",
    "        'Naive Bayes': [modelo for modelo in ['NB + TF', 'NB + TF-IDF', 'NB + Lexicon(SWN)']],\n",
    "        'Logistic Regression': [modelo for modelo in ['LR + TF', 'LR + TF-IDF', 'LR + Lexicon(SWN)']]\n",
    "    }\n",
    "    \n",
    "    resultados_por_algoritmo = {}\n",
    "    \n",
    "    for algoritmo, modelos in algoritmos.items():\n",
    "        f1_scores = []\n",
    "        accuracy_scores = []\n",
    "        precision_scores = []\n",
    "        recall_scores = []\n",
    "        \n",
    "        for categoria in categorias:\n",
    "            if categoria in tablas_categoria:\n",
    "                tabla = tablas_categoria[categoria]\n",
    "                for modelo in modelos:\n",
    "                    if modelo in tabla.index:\n",
    "                        f1_scores.append(tabla.loc[modelo, 'F1 Macro'])\n",
    "                        accuracy_scores.append(tabla.loc[modelo, 'Accuracy'])\n",
    "                        precision_scores.append(tabla.loc[modelo, 'Precision Macro'])\n",
    "                        recall_scores.append(tabla.loc[modelo, 'Recall Macro'])\n",
    "        \n",
    "        if f1_scores:\n",
    "            resultados_por_algoritmo[algoritmo] = {\n",
    "                'F1 Macro Promedio': np.mean(f1_scores),\n",
    "                'F1 Macro Std': np.std(f1_scores),\n",
    "                'Accuracy Promedio': np.mean(accuracy_scores),\n",
    "                'Accuracy Std': np.std(accuracy_scores),\n",
    "                'Precision Promedio': np.mean(precision_scores),\n",
    "                'Recall Promedio': np.mean(recall_scores),\n",
    "                'Mejor F1': np.max(f1_scores),\n",
    "                'Peor F1': np.min(f1_scores),\n",
    "                'Num Modelos': len(f1_scores)\n",
    "            }\n",
    "    \n",
    "    # Crear DataFrame para comparación\n",
    "    df_algoritmos = pd.DataFrame(resultados_por_algoritmo).T\n",
    "    df_algoritmos = df_algoritmos.sort_values('F1 Macro Promedio', ascending=False)\n",
    "    \n",
    "    print(\"\\nRENDIMIENTO PROMEDIO POR ALGORITMO:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(df_algoritmos[['F1 Macro Promedio', 'F1 Macro Std', 'Accuracy Promedio', 'Accuracy Std']].round(4))\n",
    "    \n",
    "    # Análisis de ventaja estadística\n",
    "    nb_scores = []\n",
    "    lr_scores = []\n",
    "    \n",
    "    for categoria in categorias:\n",
    "        if categoria in tablas_categoria:\n",
    "            tabla = tablas_categoria[categoria]\n",
    "            # Recopilar todos los scores de NB y LR\n",
    "            for modelo in tabla.index:\n",
    "                if 'NB' in modelo:\n",
    "                    nb_scores.append(tabla.loc[modelo, 'F1 Macro'])\n",
    "                elif 'LR' in modelo:\n",
    "                    lr_scores.append(tabla.loc[modelo, 'F1 Macro'])\n",
    "    \n",
    "    diferencia_promedio = np.mean(lr_scores) - np.mean(nb_scores)\n",
    "    print(f\"\\nDIFERENCIA PROMEDIO (LR - NB): {diferencia_promedio:+.4f}\")\n",
    "    \n",
    "    if diferencia_promedio > 0:\n",
    "        print(\"→ Logistic Regression supera a Naive Bayes en promedio\")\n",
    "    else:\n",
    "        print(\"→ Naive Bayes supera a Logistic Regression en promedio\")\n",
    "    \n",
    "    return df_algoritmos\n",
    "\n",
    "def analizar_nb_vs_lr_por_categoria(tablas_categoria, categorias):\n",
    "    \"\"\"\n",
    "    Analiza qué algoritmo funciona mejor en cada categoría\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ANÁLISIS NB vs LR POR CATEGORÍA\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    mejor_por_categoria = {}\n",
    "    ventajas_detalladas = {}\n",
    "    \n",
    "    for categoria in categorias:\n",
    "        if categoria in tablas_categoria:\n",
    "            tabla = tablas_categoria[categoria]\n",
    "            \n",
    "            # Separar modelos por algoritmo\n",
    "            nb_models = [modelo for modelo in tabla.index if 'NB' in modelo]\n",
    "            lr_models = [modelo for modelo in tabla.index if 'LR' in modelo]\n",
    "            \n",
    "            resultados_categoria = {}\n",
    "            \n",
    "            # Estadísticas por algoritmo en esta categoría\n",
    "            if nb_models:\n",
    "                nb_f1_scores = tabla.loc[nb_models, 'F1 Macro']\n",
    "                best_nb = nb_f1_scores.max()\n",
    "                best_nb_model = nb_f1_scores.idxmax()\n",
    "                avg_nb = nb_f1_scores.mean()\n",
    "                resultados_categoria['NB'] = {\n",
    "                    'Mejor F1': best_nb, \n",
    "                    'Mejor Modelo': best_nb_model,\n",
    "                    'F1 Promedio': avg_nb,\n",
    "                    'Todos los F1': nb_f1_scores.tolist()\n",
    "                }\n",
    "            \n",
    "            if lr_models:\n",
    "                lr_f1_scores = tabla.loc[lr_models, 'F1 Macro']\n",
    "                best_lr = lr_f1_scores.max()\n",
    "                best_lr_model = lr_f1_scores.idxmax()\n",
    "                avg_lr = lr_f1_scores.mean()\n",
    "                resultados_categoria['LR'] = {\n",
    "                    'Mejor F1': best_lr,\n",
    "                    'Mejor Modelo': best_lr_model, \n",
    "                    'F1 Promedio': avg_lr,\n",
    "                    'Todos los F1': lr_f1_scores.tolist()\n",
    "                }\n",
    "            \n",
    "            mejor_por_categoria[categoria] = resultados_categoria\n",
    "            \n",
    "            print(f\"\\n{categoria}:\")\n",
    "            print(\"-\" * 40)\n",
    "            \n",
    "            if 'NB' in resultados_categoria and 'LR' in resultados_categoria:\n",
    "                nb_data = resultados_categoria['NB']\n",
    "                lr_data = resultados_categoria['LR']\n",
    "                \n",
    "                print(f\"  Naive Bayes:\")\n",
    "                print(f\"    Mejor:    F1={nb_data['Mejor F1']:.4f} ({nb_data['Mejor Modelo']})\")\n",
    "                print(f\"    Promedio: F1={nb_data['F1 Promedio']:.4f}\")\n",
    "                \n",
    "                print(f\"  Logistic Regression:\")\n",
    "                print(f\"    Mejor:    F1={lr_data['Mejor F1']:.4f} ({lr_data['Mejor Modelo']})\")\n",
    "                print(f\"    Promedio: F1={lr_data['F1 Promedio']:.4f}\")\n",
    "                \n",
    "                # Determinar ganador\n",
    "                if lr_data['Mejor F1'] > nb_data['Mejor F1']:\n",
    "                    ventaja = lr_data['Mejor F1'] - nb_data['Mejor F1']\n",
    "                    ganador = \"LR\"\n",
    "                    print(f\"  → GANADOR: LR (ventaja: +{ventaja:.4f})\")\n",
    "                elif nb_data['Mejor F1'] > lr_data['Mejor F1']:\n",
    "                    ventaja = nb_data['Mejor F1'] - lr_data['Mejor F1']\n",
    "                    ganador = \"NB\"\n",
    "                    print(f\"  → GANADOR: NB (ventaja: +{ventaja:.4f})\")\n",
    "                else:\n",
    "                    ventaja = 0\n",
    "                    ganador = \"Empate\"\n",
    "                    print(f\"  → EMPATE\")\n",
    "                \n",
    "                ventajas_detalladas[categoria] = {\n",
    "                    'ganador': ganador,\n",
    "                    'ventaja': ventaja,\n",
    "                    'nb_mejor': nb_data['Mejor F1'],\n",
    "                    'lr_mejor': lr_data['Mejor F1'],\n",
    "                    'nb_promedio': nb_data['F1 Promedio'],\n",
    "                    'lr_promedio': lr_data['F1 Promedio']\n",
    "                }\n",
    "    \n",
    "    return mejor_por_categoria, ventajas_detalladas\n",
    "\n",
    "def analizar_nb_vs_lr_por_representacion(tablas_categoria, categorias):\n",
    "    \"\"\"\n",
    "    Analiza cómo se comportan NB vs LR con diferentes representaciones\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"NB vs LR POR TIPO DE REPRESENTACIÓN\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    representaciones = ['TF', 'TF-IDF', 'Lexicon']\n",
    "    \n",
    "    for repr_type in representaciones:\n",
    "        print(f\"\\n{repr_type}:\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        nb_scores = []\n",
    "        lr_scores = []\n",
    "        \n",
    "        for categoria in categorias:\n",
    "            if categoria in tablas_categoria:\n",
    "                tabla = tablas_categoria[categoria]\n",
    "                \n",
    "                # Buscar modelos correspondientes\n",
    "                if repr_type == 'TF':\n",
    "                    nb_model = 'NB + TF'\n",
    "                    lr_model = 'LR + TF'\n",
    "                elif repr_type == 'TF-IDF':\n",
    "                    nb_model = 'NB + TF-IDF'\n",
    "                    lr_model = 'LR + TF-IDF'\n",
    "                else:  # Lexicon\n",
    "                    nb_model = 'NB + Lexicon(SWN)'\n",
    "                    lr_model = 'LR + Lexicon(SWN)'\n",
    "                \n",
    "                if nb_model in tabla.index:\n",
    "                    nb_scores.append(tabla.loc[nb_model, 'F1 Macro'])\n",
    "                if lr_model in tabla.index:\n",
    "                    lr_scores.append(tabla.loc[lr_model, 'F1 Macro'])\n",
    "        \n",
    "        if nb_scores and lr_scores:\n",
    "            nb_promedio = np.mean(nb_scores)\n",
    "            lr_promedio = np.mean(lr_scores)\n",
    "            diferencia = lr_promedio - nb_promedio\n",
    "            \n",
    "            print(f\"  NB promedio:  {nb_promedio:.4f}\")\n",
    "            print(f\"  LR promedio:  {lr_promedio:.4f}\")\n",
    "            print(f\"  Diferencia:   {diferencia:+.4f}\")\n",
    "            \n",
    "            if diferencia > 0.01:\n",
    "                print(f\"  → LR claramente superior con {repr_type}\")\n",
    "            elif diferencia < -0.01:\n",
    "                print(f\"  → NB claramente superior con {repr_type}\")\n",
    "            else:\n",
    "                print(f\"  → Rendimiento similar con {repr_type}\")\n",
    "\n",
    "def crear_estadisticas_detalladas_nb_lr(tablas_categoria, categorias):\n",
    "    \"\"\"\n",
    "    Crea estadísticas detalladas para el análisis NB vs LR\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ESTADÍSTICAS DETALLADAS: NB vs LR\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Recopilar todas las métricas\n",
    "    todas_metricas = {\n",
    "        'NB': {'F1': [], 'Accuracy': [], 'Precision': [], 'Recall': []},\n",
    "        'LR': {'F1': [], 'Accuracy': [], 'Precision': [], 'Recall': []}\n",
    "    }\n",
    "    \n",
    "    for categoria in categorias:\n",
    "        if categoria in tablas_categoria:\n",
    "            tabla = tablas_categoria[categoria]\n",
    "            \n",
    "            for modelo in tabla.index:\n",
    "                if 'NB' in modelo:\n",
    "                    todas_metricas['NB']['F1'].append(tabla.loc[modelo, 'F1 Macro'])\n",
    "                    todas_metricas['NB']['Accuracy'].append(tabla.loc[modelo, 'Accuracy'])\n",
    "                    todas_metricas['NB']['Precision'].append(tabla.loc[modelo, 'Precision Macro'])\n",
    "                    todas_metricas['NB']['Recall'].append(tabla.loc[modelo, 'Recall Macro'])\n",
    "                elif 'LR' in modelo:\n",
    "                    todas_metricas['LR']['F1'].append(tabla.loc[modelo, 'F1 Macro'])\n",
    "                    todas_metricas['LR']['Accuracy'].append(tabla.loc[modelo, 'Accuracy'])\n",
    "                    todas_metricas['LR']['Precision'].append(tabla.loc[modelo, 'Precision Macro'])\n",
    "                    todas_metricas['LR']['Recall'].append(tabla.loc[modelo, 'Recall Macro'])\n",
    "    \n",
    "    # Análisis estadístico\n",
    "    print(\"\\nCOMPARACIÓN ESTADÍSTICA:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    metricas_nombres = ['F1', 'Accuracy', 'Precision', 'Recall']\n",
    "    \n",
    "    for metrica in metricas_nombres:\n",
    "        nb_vals = todas_metricas['NB'][metrica]\n",
    "        lr_vals = todas_metricas['LR'][metrica]\n",
    "        \n",
    "        if nb_vals and lr_vals:\n",
    "            print(f\"\\n{metrica}:\")\n",
    "            print(f\"  NB: μ={np.mean(nb_vals):.4f}, σ={np.std(nb_vals):.4f}, min={np.min(nb_vals):.4f}, max={np.max(nb_vals):.4f}\")\n",
    "            print(f\"  LR: μ={np.mean(lr_vals):.4f}, σ={np.std(lr_vals):.4f}, min={np.min(lr_vals):.4f}, max={np.max(lr_vals):.4f}\")\n",
    "            \n",
    "            diferencia = np.mean(lr_vals) - np.mean(nb_vals)\n",
    "            print(f\"  Diferencia (LR-NB): {diferencia:+.4f}\")\n",
    "    \n",
    "    # Contar victorias\n",
    "    print(f\"\\nCONTEO DE VICTORIAS:\")\n",
    "    print(\"-\" * 25)\n",
    "    \n",
    "    victorias = {'NB': 0, 'LR': 0, 'Empates': 0}\n",
    "    \n",
    "    for categoria in categorias:\n",
    "        if categoria in tablas_categoria:\n",
    "            tabla = tablas_categoria[categoria]\n",
    "            \n",
    "            nb_models = [modelo for modelo in tabla.index if 'NB' in modelo]\n",
    "            lr_models = [modelo for modelo in tabla.index if 'LR' in modelo]\n",
    "            \n",
    "            if nb_models and lr_models:\n",
    "                best_nb = tabla.loc[nb_models, 'F1 Macro'].max()\n",
    "                best_lr = tabla.loc[lr_models, 'F1 Macro'].max()\n",
    "                \n",
    "                if best_lr > best_nb:\n",
    "                    victorias['LR'] += 1\n",
    "                elif best_nb > best_lr:\n",
    "                    victorias['NB'] += 1\n",
    "                else:\n",
    "                    victorias['Empates'] += 1\n",
    "    \n",
    "    total_comparaciones = sum(victorias.values())\n",
    "    for algoritmo, wins in victorias.items():\n",
    "        porcentaje = (wins / total_comparaciones) * 100 if total_comparaciones > 0 else 0\n",
    "        print(f\"  {algoritmo}: {wins}/{total_comparaciones} ({porcentaje:.1f}%)\")\n",
    "    \n",
    "    return todas_metricas, victorias\n",
    "\n",
    "# Función principal para ejecutar todas las comparaciones NB vs LR\n",
    "def ejecutar_comparacion_nb_lr_completa(tablas_categoria, categorias):\n",
    "    \"\"\"\n",
    "    Ejecuta todas las comparaciones entre Naive Bayes y Logistic Regression\n",
    "    \"\"\"\n",
    "    # 1. Comparación general\n",
    "    df_algoritmos = comparar_nb_vs_lr(tablas_categoria, categorias)\n",
    "    \n",
    "    # 2. Análisis por categoría\n",
    "    mejor_por_cat, ventajas = analizar_nb_vs_lr_por_categoria(tablas_categoria, categorias)\n",
    "    \n",
    "    # 3. Análisis por representación\n",
    "    analizar_nb_vs_lr_por_representacion(tablas_categoria, categorias)\n",
    "    \n",
    "    # 4. Estadísticas detalladas\n",
    "    metricas, victorias = crear_estadisticas_detalladas_nb_lr(tablas_categoria, categorias)\n",
    "    \n",
    "    return df_algoritmos, mejor_por_cat, ventajas, metricas, victorias\n",
    "\n",
    "# Ejemplo de uso:\n",
    "df_alg, mejor_cat, ventajas, metricas, victorias = ejecutar_comparacion_nb_lr_completa(tablas_categoria, CATEGORIES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65f0f07",
   "metadata": {},
   "source": [
    "### Identificación de categorías difíciles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "39e5b47d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ANÁLISIS DE DIFICULTAD POR CATEGORÍA\n",
      "================================================================================\n",
      "\n",
      "RANKING DE DIFICULTAD (de más difícil a más fácil):\n",
      "            Best F1 Macro Best F1 Micro Best Accuracy   Best Model\n",
      "Books            0.806493      0.806667      0.806667      LR + TF\n",
      "Electronics      0.844979         0.845         0.845  NB + TF-IDF\n",
      "DVD              0.851319      0.851667      0.851667      NB + TF\n",
      "Kitchen             0.885         0.885         0.885      LR + TF\n",
      "\n",
      "CATEGORÍA MÁS DIFÍCIL: Books\n",
      "   F1-Macro: 0.8065\n",
      "   Mejor modelo: LR + TF\n",
      "\n",
      "CATEGORÍA MÁS FÁCIL: Kitchen\n",
      "   F1-Macro: 0.8850\n",
      "   Mejor modelo: LR + TF\n",
      "\n",
      "DIFERENCIA DE RENDIMIENTO: 0.0785\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"ANÁLISIS DE DIFICULTAD POR CATEGORÍA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Recopilar métricas por categoría\n",
    "categoria_metrics = {}\n",
    "\n",
    "for categoria, tabla in tablas_categoria.items():\n",
    "    if len(tabla) > 0:\n",
    "        # Obtener la mejor métrica F1 Macro de cada categoría\n",
    "        best_f1_macro = tabla['F1 Macro'].max()\n",
    "        best_f1_micro = tabla['F1 Micro'].max()\n",
    "        best_accuracy = tabla['Accuracy'].max()\n",
    "        \n",
    "        # Obtener métricas del mejor modelo\n",
    "        best_model_idx = tabla['F1 Macro'].idxmax()\n",
    "        best_model_name = best_model_idx\n",
    "        \n",
    "        categoria_metrics[categoria] = {\n",
    "            'Best F1 Macro': best_f1_macro,\n",
    "            'Best F1 Micro': best_f1_micro,\n",
    "            'Best Accuracy': best_accuracy,\n",
    "            'Best Model': best_model_name\n",
    "        }\n",
    "\n",
    "# Crear DataFrame para análisis\n",
    "df_dificultad = pd.DataFrame(categoria_metrics).T\n",
    "df_dificultad = df_dificultad.sort_values('Best F1 Macro')\n",
    "\n",
    "print(\"\\nRANKING DE DIFICULTAD (de más difícil a más fácil):\")\n",
    "print(df_dificultad.round(4))\n",
    "\n",
    "# Identificar la más difícil y la más fácil\n",
    "mas_dificil = df_dificultad.index[0]\n",
    "mas_facil = df_dificultad.index[-1]\n",
    "\n",
    "print(f\"\\nCATEGORÍA MÁS DIFÍCIL: {mas_dificil}\")\n",
    "print(f\"   F1-Macro: {df_dificultad.loc[mas_dificil, 'Best F1 Macro']:.4f}\")\n",
    "print(f\"   Mejor modelo: {df_dificultad.loc[mas_dificil, 'Best Model']}\")\n",
    "\n",
    "print(f\"\\nCATEGORÍA MÁS FÁCIL: {mas_facil}\")\n",
    "print(f\"   F1-Macro: {df_dificultad.loc[mas_facil, 'Best F1 Macro']:.4f}\")\n",
    "print(f\"   Mejor modelo: {df_dificultad.loc[mas_facil, 'Best Model']}\")\n",
    "\n",
    "# Diferencias de rendimiento\n",
    "diferencia = df_dificultad.loc[mas_facil, 'Best F1 Macro'] - df_dificultad.loc[mas_dificil, 'Best F1 Macro']\n",
    "print(f\"\\nDIFERENCIA DE RENDIMIENTO: {diferencia:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f69de37",
   "metadata": {},
   "source": [
    "### Evaluación de características más importantes por categorías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5666e3cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CARACTERÍSTICAS MÁS IMPORTANTES POR CATEGORÍA (LR Parameters)\n",
      "================================================================================\n",
      "\n",
      "CATEGORÍA: Books\n",
      "Modelo: LR + TF-IDF\n",
      "------------------------------------------------------------\n",
      "\n",
      "CARACTERÍSTICAS PRO-POSITIVAS:\n",
      "   great                     :  +1.3768\n",
      "   excellent                 :  +1.2891\n",
      "   recommend                 :  +0.8717\n",
      "   love                      :  +0.8584\n",
      "   wonderful                 :  +0.8498\n",
      "   best                      :  +0.8440\n",
      "   the_best                  :  +0.8184\n",
      "   easy                      :  +0.7792\n",
      "   an_excellent              :  +0.7354\n",
      "   loved                     :  +0.6922\n",
      "   a_great                   :  +0.6780\n",
      "   favorite                  :  +0.6761\n",
      "   highly                    :  +0.6751\n",
      "   my                        :  +0.6293\n",
      "   life                      :  +0.6230\n",
      "\n",
      "CARACTERÍSTICAS PRO-NEGATIVAS:\n",
      "   not                       :  -1.6153\n",
      "   i                         :  -1.6071\n",
      "   no                        :  -1.5530\n",
      "   bad                       :  -1.2817\n",
      "   was                       :  -1.1729\n",
      "   don't                     :  -1.1125\n",
      "   waste                     :  -1.0887\n",
      "   boring                    :  -1.0251\n",
      "   too                       :  -1.0134\n",
      "   if                        :  -1.0069\n",
      "   just                      :  -0.9152\n",
      "   much                      :  -0.8847\n",
      "   money                     :  -0.8483\n",
      "   disappointing             :  -0.8125\n",
      "   nothing                   :  -0.7967\n",
      "\n",
      "CATEGORÍA: DVD\n",
      "Modelo: LR + TF-IDF\n",
      "------------------------------------------------------------\n",
      "\n",
      "CARACTERÍSTICAS PRO-POSITIVAS:\n",
      "   great                     :  +1.9432\n",
      "   his                       :  +1.5644\n",
      "   best                      :  +1.3489\n",
      "   excellent                 :  +1.1827\n",
      "   love                      :  +1.1357\n",
      "   a_great                   :  +1.0481\n",
      "   the_best                  :  +1.0448\n",
      "   wonderful                 :  +0.9519\n",
      "   family                    :  +0.8697\n",
      "   season                    :  +0.8653\n",
      "   must                      :  +0.8392\n",
      "   who                       :  +0.8116\n",
      "   well                      :  +0.7928\n",
      "   a_must                    :  +0.7437\n",
      "   loved                     :  +0.7340\n",
      "\n",
      "CARACTERÍSTICAS PRO-NEGATIVAS:\n",
      "   not                       :  -1.6728\n",
      "   bad                       :  -1.4894\n",
      "   worst                     :  -1.2422\n",
      "   no                        :  -1.2296\n",
      "   was                       :  -1.1003\n",
      "   waste                     :  -1.0759\n",
      "   boring                    :  -1.0184\n",
      "   the_worst                 :  -0.9928\n",
      "   horrible                  :  -0.9049\n",
      "   money                     :  -0.8815\n",
      "   terrible                  :  -0.7883\n",
      "   only                      :  -0.7850\n",
      "   waste_of                  :  -0.7679\n",
      "   book                      :  -0.7346\n",
      "   poor                      :  -0.7064\n",
      "\n",
      "CATEGORÍA: Electronics\n",
      "Modelo: LR + TF-IDF\n",
      "------------------------------------------------------------\n",
      "\n",
      "CARACTERÍSTICAS PRO-POSITIVAS:\n",
      "   great                     :  +3.2001\n",
      "   price                     :  +1.8650\n",
      "   good                      :  +1.5739\n",
      "   excellent                 :  +1.5145\n",
      "   works                     :  +1.3274\n",
      "   perfect                   :  +1.2594\n",
      "   the_price                 :  +1.0727\n",
      "   easy                      :  +1.0658\n",
      "   speakers                  :  +0.9912\n",
      "   a_great                   :  +0.9424\n",
      "   use                       :  +0.9279\n",
      "   used                      :  +0.8847\n",
      "   very_good                 :  +0.8831\n",
      "   easy_to                   :  +0.8785\n",
      "   memory                    :  +0.8756\n",
      "\n",
      "CARACTERÍSTICAS PRO-NEGATIVAS:\n",
      "   not                       :  -2.5911\n",
      "   work                      :  -1.5107\n",
      "   after                     :  -1.1985\n",
      "   poor                      :  -1.1161\n",
      "   not_work                  :  -1.1140\n",
      "   back                      :  -1.1023\n",
      "   waste                     :  -1.0952\n",
      "   bad                       :  -1.0541\n",
      "   return                    :  -1.0269\n",
      "   does_not                  :  -0.9925\n",
      "   support                   :  -0.9716\n",
      "   don't                     :  -0.9433\n",
      "   junk                      :  -0.9056\n",
      "   they                      :  -0.8812\n",
      "   worked                    :  -0.8293\n",
      "\n",
      "CATEGORÍA: Kitchen\n",
      "Modelo: LR + TF-IDF\n",
      "------------------------------------------------------------\n",
      "\n",
      "CARACTERÍSTICAS PRO-POSITIVAS:\n",
      "   great                     :  +3.0815\n",
      "   easy                      :  +2.2241\n",
      "   love                      :  +2.0047\n",
      "   easy_to                   :  +1.8693\n",
      "   best                      :  +1.5584\n",
      "   my                        :  +1.2357\n",
      "   excellent                 :  +1.2112\n",
      "   perfect                   :  +1.2029\n",
      "   i_love                    :  +1.1487\n",
      "   the_best                  :  +1.0464\n",
      "   you                       :  +1.0368\n",
      "   works                     :  +1.0077\n",
      "   clean                     :  +1.0050\n",
      "   love_it                   :  +0.9684\n",
      "   little                    :  +0.9520\n",
      "\n",
      "CARACTERÍSTICAS PRO-NEGATIVAS:\n",
      "   not                       :  -2.5143\n",
      "   was                       :  -1.8405\n",
      "   after                     :  -1.2994\n",
      "   disappointed              :  -1.2801\n",
      "   <num>                     :  -1.2448\n",
      "   waste                     :  -1.0857\n",
      "   poor                      :  -1.0450\n",
      "   money                     :  -0.9515\n",
      "   broken                    :  -0.9223\n",
      "   broke                     :  -0.9183\n",
      "   bad                       :  -0.9120\n",
      "   your_money                :  -0.8808\n",
      "   return                    :  -0.8613\n",
      "   i_was                     :  -0.8328\n",
      "   returned                  :  -0.8269\n",
      "\n",
      "============================================================\n",
      "RESUMEN DE CARACTERÍSTICAS POR CATEGORÍA\n",
      "============================================================\n",
      "\n",
      "Books:\n",
      "  Top Positivas: ['great', 'excellent', 'recommend', 'love', 'wonderful']\n",
      "  Top Negativas: ['not', 'i', 'no', 'bad', 'was']\n",
      "\n",
      "DVD:\n",
      "  Top Positivas: ['great', 'his', 'best', 'excellent', 'love']\n",
      "  Top Negativas: ['not', 'bad', 'worst', 'no', 'was']\n",
      "\n",
      "Electronics:\n",
      "  Top Positivas: ['great', 'price', 'good', 'excellent', 'works']\n",
      "  Top Negativas: ['not', 'work', 'after', 'poor', 'not_work']\n",
      "\n",
      "Kitchen:\n",
      "  Top Positivas: ['great', 'easy', 'love', 'easy_to', 'best']\n",
      "  Top Negativas: ['not', 'was', 'after', 'disappointed', '<num>']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CARACTERÍSTICAS MÁS IMPORTANTES POR CATEGORÍA (LR Parameters)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def extraer_top_features_categoria(modelos_categoria, categoria, top_k=15):\n",
    "    \"\"\"Extrae top features de LR para una categoría específica\"\"\"\n",
    "    \n",
    "    if categoria in modelos_categoria:\n",
    "        models = modelos_categoria[categoria]\n",
    "        \n",
    "        # Buscar modelo LR + TF-IDF (suele ser el mejor)\n",
    "        lr_model = None\n",
    "        modelo_name = None\n",
    "        \n",
    "        for name, model in models.items():\n",
    "            if 'LR' in name and 'TF-IDF' in name:\n",
    "                lr_model = model\n",
    "                modelo_name = name\n",
    "                break\n",
    "        \n",
    "        if lr_model is None:\n",
    "            # Si no hay TF-IDF, buscar cualquier LR\n",
    "            for name, model in models.items():\n",
    "                if 'LR' in name:\n",
    "                    lr_model = model\n",
    "                    modelo_name = name\n",
    "                    break\n",
    "        \n",
    "        if lr_model is not None:\n",
    "            print(f\"\\nCATEGORÍA: {categoria}\")\n",
    "            print(f\"Modelo: {modelo_name}\")\n",
    "            print(\"-\" * 60)\n",
    "            \n",
    "            # Extraer vectorizador y coeficientes\n",
    "            if hasattr(lr_model.named_steps, 'dict'):\n",
    "                vectorizer = lr_model.named_steps['dict']\n",
    "                feature_names = vectorizer.get_feature_names_out()\n",
    "                \n",
    "                if hasattr(lr_model.named_steps, 'lr'):\n",
    "                    lr_classifier = lr_model.named_steps['lr']\n",
    "                    coefs = lr_classifier.coef_.ravel()\n",
    "                    \n",
    "                    # Top features positivas (pro-positivas)\n",
    "                    top_pos_idx = np.argsort(coefs)[-top_k:]\n",
    "                    # Top features negativas (pro-negativas)  \n",
    "                    top_neg_idx = np.argsort(coefs)[:top_k]\n",
    "                    \n",
    "                    print(\"\\nCARACTERÍSTICAS PRO-POSITIVAS:\")\n",
    "                    for i in reversed(top_pos_idx):\n",
    "                        print(f\"   {feature_names[i]:25s} : {coefs[i]:+8.4f}\")\n",
    "                    \n",
    "                    print(\"\\nCARACTERÍSTICAS PRO-NEGATIVAS:\")\n",
    "                    for i in top_neg_idx:\n",
    "                        print(f\"   {feature_names[i]:25s} : {coefs[i]:+8.4f}\")\n",
    "                    \n",
    "                    return {\n",
    "                        'top_positive': [(feature_names[i], coefs[i]) for i in reversed(top_pos_idx)],\n",
    "                        'top_negative': [(feature_names[i], coefs[i]) for i in top_neg_idx]\n",
    "                    }\n",
    "            else:\n",
    "                print(f\"   No se pudo extraer características - no hay DictVectorizer\")\n",
    "        else:\n",
    "            print(f\"   No se encontró modelo LR para {categoria}\")\n",
    "    else:\n",
    "        print(f\"   Categoría {categoria} no encontrada en modelos\")\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Extraer características para cada categoría\n",
    "CATEGORIAS = [\"Books\", \"DVD\", \"Electronics\", \"Kitchen\"]\n",
    "features_por_categoria = {}\n",
    "\n",
    "for categoria in CATEGORIAS:\n",
    "    features = extraer_top_features_categoria(modelos_categoria, categoria)\n",
    "    if features:\n",
    "        features_por_categoria[categoria] = features\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"RESUMEN DE CARACTERÍSTICAS POR CATEGORÍA\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "for categoria, features in features_por_categoria.items():\n",
    "    print(f\"\\n{categoria}:\")\n",
    "    print(\"  Top Positivas:\", [word for word, coef in features['top_positive'][:5]])\n",
    "    print(\"  Top Negativas:\", [word for word, coef in features['top_negative'][:5]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61bf67b",
   "metadata": {},
   "source": [
    "## Evaluación de los modelos globales\n",
    "\n",
    "A continuación se encuentra la implementación del modelo global que clasifica todas las categorías."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0c4a2428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "ANÁLISIS GLOBAL: TODAS LAS CATEGORÍAS COMBINADAS\n",
      "====================================================================================================\n",
      "Cargando datos combinados...\n",
      "Datos etiquetados: 8000\n",
      "Datos no etiquetados: 19677\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELDA 6: MODELOS GLOBALES (TODAS LAS CATEGORÍAS COMBINADAS)\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\n{'='*100}\")\n",
    "print(\"ANÁLISIS GLOBAL: TODAS LAS CATEGORÍAS COMBINADAS\")\n",
    "print(f\"{'='*100}\")\n",
    "\n",
    "# Cargar datos combinados\n",
    "print(\"Cargando datos combinados...\")\n",
    "all_data_combined = load_all_domains_combined(ROOT, CATEGORIES)\n",
    "X_labeled_global = all_data_combined[\"X_labeled\"]\n",
    "y_labeled_global = all_data_combined[\"y_labeled\"]\n",
    "X_unlabeled_global = all_data_combined[\"X_unlabeled\"]\n",
    "\n",
    "print(f\"Datos etiquetados: {len(X_labeled_global)}\")\n",
    "print(f\"Datos no etiquetados: {len(X_unlabeled_global)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da36f405",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_global, X_te_global, y_tr_global, y_te_global = train_test_split(\n",
    "    X_labeled_global, y_labeled_global,\n",
    "    test_size=0.30, random_state=42, stratify=y_labeled_global\n",
    ")\n",
    "\n",
    "# Definir modelos globales (mismos 6 tipos)\n",
    "modelos_globales = {\n",
    "    \"NB + TF (Global)\": make_nb_tf(),\n",
    "    \"NB + TF-IDF (Global)\": make_nb_tfidf(),\n",
    "    \"LR + TF (Global)\": make_lr_tf(),\n",
    "    \"LR + TF-IDF (Global)\": make_lr_tfidf(),\n",
    "    \"LR + Lexicon(SWN) (Global)\": make_lr_lexicon(lexicon),\n",
    "    \"NB + Lexicon(SWN) (Global)\": make_nb_lexicon(lexicon)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c95f679f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Entrenando modelo global: NB + TF (Global)...\n",
      "\n",
      "NB + TF (Global)\n",
      "Accuracy: 0.8313\n",
      "Macro  -> P: 0.8342  R: 0.8313  F1: 0.8309\n",
      "Micro  -> P: 0.8313  R: 0.8313  F1: 0.8313\n",
      "\n",
      "Reporte por clase:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.803     0.878     0.839      1200\n",
      "    positive      0.866     0.784     0.823      1200\n",
      "\n",
      "    accuracy                          0.831      2400\n",
      "   macro avg      0.834     0.831     0.831      2400\n",
      "weighted avg      0.834     0.831     0.831      2400\n",
      "\n",
      "\n",
      "Entrenando modelo global: NB + TF-IDF (Global)...\n",
      "\n",
      "NB + TF-IDF (Global)\n",
      "Accuracy: 0.8433\n",
      "Macro  -> P: 0.8509  R: 0.8433  F1: 0.8425\n",
      "Micro  -> P: 0.8433  R: 0.8433  F1: 0.8433\n",
      "\n",
      "Reporte por clase:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.799     0.917     0.854      1200\n",
      "    positive      0.902     0.770     0.831      1200\n",
      "\n",
      "    accuracy                          0.843      2400\n",
      "   macro avg      0.851     0.843     0.842      2400\n",
      "weighted avg      0.851     0.843     0.842      2400\n",
      "\n",
      "\n",
      "Entrenando modelo global: LR + TF (Global)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\ir-gensim\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1281: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LR + TF (Global)\n",
      "Accuracy: 0.8529\n",
      "Macro  -> P: 0.8530  R: 0.8529  F1: 0.8529\n",
      "Micro  -> P: 0.8529  R: 0.8529  F1: 0.8529\n",
      "\n",
      "Reporte por clase:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.847     0.862     0.854      1200\n",
      "    positive      0.859     0.844     0.852      1200\n",
      "\n",
      "    accuracy                          0.853      2400\n",
      "   macro avg      0.853     0.853     0.853      2400\n",
      "weighted avg      0.853     0.853     0.853      2400\n",
      "\n",
      "\n",
      "Entrenando modelo global: LR + TF-IDF (Global)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\ir-gensim\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1281: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LR + TF-IDF (Global)\n",
      "Accuracy: 0.8467\n",
      "Macro  -> P: 0.8476  R: 0.8467  F1: 0.8466\n",
      "Micro  -> P: 0.8467  R: 0.8467  F1: 0.8467\n",
      "\n",
      "Reporte por clase:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.830     0.873     0.851      1200\n",
      "    positive      0.866     0.821     0.843      1200\n",
      "\n",
      "    accuracy                          0.847      2400\n",
      "   macro avg      0.848     0.847     0.847      2400\n",
      "weighted avg      0.848     0.847     0.847      2400\n",
      "\n",
      "\n",
      "Entrenando modelo global: LR + Lexicon(SWN) (Global)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\ir-gensim\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1281: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LR + Lexicon(SWN) (Global)\n",
      "Accuracy: 0.7121\n",
      "Macro  -> P: 0.7127  R: 0.7121  F1: 0.7119\n",
      "Micro  -> P: 0.7121  R: 0.7121  F1: 0.7121\n",
      "\n",
      "Reporte por clase:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.701     0.739     0.720      1200\n",
      "    positive      0.724     0.685     0.704      1200\n",
      "\n",
      "    accuracy                          0.712      2400\n",
      "   macro avg      0.713     0.712     0.712      2400\n",
      "weighted avg      0.713     0.712     0.712      2400\n",
      "\n",
      "\n",
      "Entrenando modelo global: NB + Lexicon(SWN) (Global)...\n",
      "\n",
      "NB + Lexicon(SWN) (Global)\n",
      "Accuracy: 0.6875\n",
      "Macro  -> P: 0.6962  R: 0.6875  F1: 0.6840\n",
      "Micro  -> P: 0.6875  R: 0.6875  F1: 0.6875\n",
      "\n",
      "Reporte por clase:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.655     0.792     0.717      1200\n",
      "    positive      0.737     0.583     0.651      1200\n",
      "\n",
      "    accuracy                          0.688      2400\n",
      "   macro avg      0.696     0.688     0.684      2400\n",
      "weighted avg      0.696     0.688     0.684      2400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluar modelos globales\n",
    "resultados_globales = {}\n",
    "modelos_globales_entrenados = {}\n",
    "\n",
    "for nombre, pipe in modelos_globales.items():\n",
    "    print(f\"\\nEntrenando modelo global: {nombre}...\")\n",
    "    try:\n",
    "        resultado = evaluar_modelo(nombre, pipe, X_tr_global, y_tr_global, X_te_global, y_te_global, mostrar_reporte=True)\n",
    "        resultados_globales[nombre] = resultado\n",
    "        modelos_globales_entrenados[nombre] = pipe\n",
    "    except Exception as e:\n",
    "        print(f\"Error con {nombre}: {e}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "daf7062a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "RESUMEN MODELOS GLOBALES\n",
      "================================================================================\n",
      "                            Accuracy  F1 Macro  F1 Micro\n",
      "NB + TF (Global)              0.8312    0.8309    0.8312\n",
      "NB + TF-IDF (Global)          0.8433    0.8425    0.8433\n",
      "LR + TF (Global)              0.8529    0.8529    0.8529\n",
      "LR + TF-IDF (Global)          0.8467    0.8466    0.8467\n",
      "LR + Lexicon(SWN) (Global)    0.7121    0.7119    0.7121\n",
      "NB + Lexicon(SWN) (Global)    0.6875    0.6840    0.6875\n"
     ]
    }
   ],
   "source": [
    "df_globales = pd.DataFrame(resultados_globales).T\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"RESUMEN MODELOS GLOBALES\")\n",
    "print(f\"{'='*80}\")\n",
    "print(df_globales[[\"Accuracy\", \"F1 Macro\", \"F1 Micro\"]].round(4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a6c86e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PREDICCIONES EN DATOS UNLABELED GLOBALES\n",
      "================================================================================\n",
      "\n",
      "NB + TF (Global)\n",
      "Predicciones (primeros 20): [0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1]\n",
      "Distribución: Pos=8973 | Neg=10704\n",
      "\n",
      "NB + TF-IDF (Global)\n",
      "Predicciones (primeros 20): [0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1]\n",
      "Distribución: Pos=8671 | Neg=11006\n",
      "\n",
      "LR + TF (Global)\n",
      "Predicciones (primeros 20): [0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1]\n",
      "Distribución: Pos=9872 | Neg=9805\n",
      "\n",
      "LR + TF-IDF (Global)\n",
      "Predicciones (primeros 20): [0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1]\n",
      "Distribución: Pos=9544 | Neg=10133\n",
      "\n",
      "LR + Lexicon(SWN) (Global)\n",
      "Predicciones (primeros 20): [0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1]\n",
      "Distribución: Pos=9689 | Neg=9988\n",
      "\n",
      "NB + Lexicon(SWN) (Global)\n",
      "Predicciones (primeros 20): [0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1]\n",
      "Distribución: Pos=7974 | Neg=11703\n"
     ]
    }
   ],
   "source": [
    "# Predicciones en unlabeled global\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PREDICCIONES EN DATOS UNLABELED GLOBALES\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "for nombre, pipe in modelos_globales_entrenados.items():\n",
    "    y_pred_unl_global = pipe.predict(X_unlabeled_global)\n",
    "    print(f\"\\n{nombre}\")\n",
    "    print(f\"Predicciones (primeros 20): {list(y_pred_unl_global[:20])}\")\n",
    "    print(f\"Distribución: Pos={np.sum(y_pred_unl_global)} | Neg={len(y_pred_unl_global)-np.sum(y_pred_unl_global)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5b734a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COMPARACIÓN GLOBAL POR TIPO DE REPRESENTACIÓN DE CARACTERÍSTICAS\n",
      "================================================================================\n",
      "\n",
      "RENDIMIENTO PROMEDIO POR TIPO DE REPRESENTACIÓN (MODELOS GLOBALES):\n",
      "----------------------------------------------------------------------\n",
      "                    F1 Macro Promedio F1 Macro Std Accuracy Promedio  \\\n",
      "TF-IDF                       0.844525     0.002039             0.845   \n",
      "TF (Term Frequency)           0.84189     0.011015          0.842083   \n",
      "Lexicon Features             0.697944     0.013928          0.699792   \n",
      "\n",
      "                    Precision Promedio Recall Promedio  \n",
      "TF-IDF                        0.849238           0.845  \n",
      "TF (Term Frequency)           0.843619        0.842083  \n",
      "Lexicon Features              0.704429        0.699792  \n",
      "\n",
      "\n",
      "ANÁLISIS DE VARIANZA ENTRE REPRESENTACIONES:\n",
      "--------------------------------------------------\n",
      "Representación más consistente (menor std):\n",
      "TF-IDF                 0.002039\n",
      "TF (Term Frequency)    0.011015\n",
      "Lexicon Features       0.013928\n",
      "Name: F1 Macro Std, dtype: object\n",
      "\n",
      "\n",
      "MEJORES MODELOS POR TIPO DE REPRESENTACIÓN:\n",
      "--------------------------------------------------\n",
      "TF (Term Frequency)      : LR + TF (Global)               (F1=0.8529)\n",
      "TF-IDF                   : LR + TF-IDF (Global)           (F1=0.8466)\n",
      "Lexicon Features         : LR + Lexicon(SWN) (Global)     (F1=0.7119)\n",
      "\n",
      "================================================================================\n",
      "MEJOR REPRESENTACIÓN PARA MODELO GLOBAL\n",
      "================================================================================\n",
      "\n",
      "RENDIMIENTO POR TIPO DE REPRESENTACIÓN:\n",
      "--------------------------------------------------\n",
      "  TF          : F1=0.8529 (LR + TF (Global))\n",
      "  TF-IDF      : F1=0.8466 (LR + TF-IDF (Global))\n",
      "  Lexicon     : F1=0.7119 (LR + Lexicon(SWN) (Global))\n",
      "\n",
      "→ MEJOR REPRESENTACIÓN GLOBAL: TF\n",
      "  Modelo: LR + TF (Global)\n",
      "  F1 Score: 0.8529\n",
      "\n",
      "VENTAJAS SOBRE OTRAS REPRESENTACIONES:\n",
      "  vs TF-IDF: +0.0063\n",
      "  vs Lexicon: +0.1410\n",
      "\n",
      "================================================================================\n",
      "ANÁLISIS DE COMPLEMENTARIEDAD ENTRE REPRESENTACIONES (GLOBAL)\n",
      "================================================================================\n",
      "\n",
      "COMPARACIÓN DIRECTA DE RENDIMIENTOS:\n",
      "----------------------------------------\n",
      "TF      → Mejor: 0.8529, Promedio: 0.8419\n",
      "TF-IDF  → Mejor: 0.8466, Promedio: 0.8445\n",
      "Mejora TF-IDF vs TF: -0.0063 (mejor), +0.0026 (promedio)\n",
      "Lexicon → Mejor: 0.7119, Promedio: 0.6979\n",
      "Diferencia Lexicon vs TF-IDF: -0.1347 (mejor), -0.1466 (promedio)\n",
      "\n",
      "INTERPRETACIÓN PARA MODELOS GLOBALES:\n",
      "----------------------------------------\n",
      "• TF-IDF vs TF: Normalización crucial para datos multi-dominio\n",
      "• Lexicon: Características semánticas vs estadísticas textuales\n",
      "• Modelo global: Mayor diversidad de datos puede favorecer representaciones robustas\n",
      "\n",
      "================================================================================\n",
      "IMPACTO DE COMBINAR DOMINIOS EN REPRESENTACIONES\n",
      "================================================================================\n",
      "\n",
      "COMPARACIÓN: MODELOS INDIVIDUALES vs GLOBALES\n",
      "------------------------------------------------------------\n",
      "\n",
      "TF:\n",
      "  Individual → Promedio: 0.8317, Mejor: 0.8850\n",
      "  Global     → Promedio: 0.8419, Mejor: 0.8529\n",
      "  Diferencia → Promedio: +0.0102, Mejor: -0.0321\n",
      "  → Modelo global MEJOR con TF\n",
      "\n",
      "TF-IDF:\n",
      "  Individual → Promedio: 0.8396, Mejor: 0.8833\n",
      "  Global     → Promedio: 0.8445, Mejor: 0.8466\n",
      "  Diferencia → Promedio: +0.0049, Mejor: -0.0368\n",
      "  → Modelo global MEJOR con TF-IDF\n",
      "\n",
      "Lexicon:\n",
      "  Individual → Promedio: 0.7041, Mejor: 0.7445\n",
      "  Global     → Promedio: 0.6979, Mejor: 0.7119\n",
      "  Diferencia → Promedio: -0.0062, Mejor: -0.0326\n",
      "  → Modelos individuales MEJORES con Lexicon\n",
      "\n",
      "CONCLUSIONES SOBRE COMBINACIÓN DE DOMINIOS:\n",
      "--------------------------------------------------\n",
      "• Modelos globales pueden generalizar mejor entre dominios\n",
      "• Representaciones robustas (TF-IDF) se benefician más de más datos\n",
      "• Características lexicon pueden ser más estables entre dominios\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# COMPARACIÓN DE MODELOS GLOBALES POR TIPO DE REPRESENTACIÓN DE CARACTERÍSTICAS\n",
    "# =============================================================================\n",
    "\n",
    "def comparar_por_tipo_representacion_global(df_globales):\n",
    "    \"\"\"\n",
    "    Compara el rendimiento de modelos globales por tipo de representación\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"COMPARACIÓN GLOBAL POR TIPO DE REPRESENTACIÓN DE CARACTERÍSTICAS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Agrupar modelos por tipo de representación\n",
    "    tipos_representacion = {\n",
    "        'TF (Term Frequency)': ['NB + TF (Global)', 'LR + TF (Global)'],\n",
    "        'TF-IDF': ['NB + TF-IDF (Global)', 'LR + TF-IDF (Global)'], \n",
    "        'Lexicon Features': ['LR + Lexicon(SWN) (Global)', 'NB + Lexicon(SWN) (Global)']\n",
    "    }\n",
    "    \n",
    "    resultados_por_tipo = {}\n",
    "    \n",
    "    for tipo, modelos in tipos_representacion.items():\n",
    "        f1_scores = []\n",
    "        accuracy_scores = []\n",
    "        precision_scores = []\n",
    "        recall_scores = []\n",
    "        \n",
    "        for modelo in modelos:\n",
    "            if modelo in df_globales.index:\n",
    "                f1_scores.append(df_globales.loc[modelo, 'F1 Macro'])\n",
    "                accuracy_scores.append(df_globales.loc[modelo, 'Accuracy'])\n",
    "                precision_scores.append(df_globales.loc[modelo, 'Precision Macro'])\n",
    "                recall_scores.append(df_globales.loc[modelo, 'Recall Macro'])\n",
    "        \n",
    "        if f1_scores:\n",
    "            resultados_por_tipo[tipo] = {\n",
    "                'F1 Macro Promedio': np.mean(f1_scores),\n",
    "                'F1 Macro Std': np.std(f1_scores),\n",
    "                'Accuracy Promedio': np.mean(accuracy_scores),\n",
    "                'Accuracy Std': np.std(accuracy_scores),\n",
    "                'Precision Promedio': np.mean(precision_scores),\n",
    "                'Recall Promedio': np.mean(recall_scores),\n",
    "                'Mejor F1': np.max(f1_scores),\n",
    "                'Peor F1': np.min(f1_scores),\n",
    "                'Modelos': modelos,\n",
    "                'Scores': f1_scores\n",
    "            }\n",
    "    \n",
    "    # Crear DataFrame para comparación\n",
    "    df_tipos = pd.DataFrame(resultados_por_tipo).T\n",
    "    df_tipos = df_tipos.sort_values('F1 Macro Promedio', ascending=False)\n",
    "    \n",
    "    print(\"\\nRENDIMIENTO PROMEDIO POR TIPO DE REPRESENTACIÓN (MODELOS GLOBALES):\")\n",
    "    print(\"-\" * 70)\n",
    "    print(df_tipos[['F1 Macro Promedio', 'F1 Macro Std', 'Accuracy Promedio', 'Precision Promedio', 'Recall Promedio']].round(4))\n",
    "    \n",
    "    # Análisis de varianza entre tipos\n",
    "    print(\"\\n\\nANÁLISIS DE VARIANZA ENTRE REPRESENTACIONES:\")\n",
    "    print(\"-\" * 50)\n",
    "    varianzas = df_tipos['F1 Macro Std'].sort_values()\n",
    "    print(\"Representación más consistente (menor std):\")\n",
    "    print(varianzas)\n",
    "    \n",
    "    # Mejores modelos por tipo\n",
    "    print(\"\\n\\nMEJORES MODELOS POR TIPO DE REPRESENTACIÓN:\")\n",
    "    print(\"-\" * 50)\n",
    "    for tipo, data in resultados_por_tipo.items():\n",
    "        mejor_idx = np.argmax(data['Scores'])\n",
    "        mejor_modelo = data['Modelos'][mejor_idx]\n",
    "        mejor_score = data['Scores'][mejor_idx]\n",
    "        print(f\"{tipo:25s}: {mejor_modelo:30s} (F1={mejor_score:.4f})\")\n",
    "    \n",
    "    return df_tipos, resultados_por_tipo\n",
    "\n",
    "def analizar_complementariedad_global(df_globales):\n",
    "    \"\"\"\n",
    "    Analiza complementariedad entre representaciones en modelos globales\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ANÁLISIS DE COMPLEMENTARIEDAD ENTRE REPRESENTACIONES (GLOBAL)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Extraer scores por tipo de representación\n",
    "    tf_scores = []\n",
    "    tfidf_scores = []\n",
    "    lexicon_scores = []\n",
    "    \n",
    "    modelos_tf = [m for m in df_globales.index if 'TF (Global)' in m]\n",
    "    modelos_tfidf = [m for m in df_globales.index if 'TF-IDF (Global)' in m]\n",
    "    modelos_lexicon = [m for m in df_globales.index if 'Lexicon(SWN) (Global)' in m]\n",
    "    \n",
    "    if modelos_tf:\n",
    "        tf_scores = df_globales.loc[modelos_tf, 'F1 Macro'].tolist()\n",
    "    if modelos_tfidf:\n",
    "        tfidf_scores = df_globales.loc[modelos_tfidf, 'F1 Macro'].tolist()\n",
    "    if modelos_lexicon:\n",
    "        lexicon_scores = df_globales.loc[modelos_lexicon, 'F1 Macro'].tolist()\n",
    "    \n",
    "    print(\"\\nCOMPARACIÓN DIRECTA DE RENDIMIENTOS:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    if tf_scores and tfidf_scores:\n",
    "        tf_best = max(tf_scores)\n",
    "        tfidf_best = max(tfidf_scores)\n",
    "        tf_avg = np.mean(tf_scores)\n",
    "        tfidf_avg = np.mean(tfidf_scores)\n",
    "        \n",
    "        print(f\"TF      → Mejor: {tf_best:.4f}, Promedio: {tf_avg:.4f}\")\n",
    "        print(f\"TF-IDF  → Mejor: {tfidf_best:.4f}, Promedio: {tfidf_avg:.4f}\")\n",
    "        print(f\"Mejora TF-IDF vs TF: {tfidf_best - tf_best:+.4f} (mejor), {tfidf_avg - tf_avg:+.4f} (promedio)\")\n",
    "    \n",
    "    if lexicon_scores:\n",
    "        lexicon_best = max(lexicon_scores)\n",
    "        lexicon_avg = np.mean(lexicon_scores)\n",
    "        print(f\"Lexicon → Mejor: {lexicon_best:.4f}, Promedio: {lexicon_avg:.4f}\")\n",
    "        \n",
    "        if tfidf_scores:\n",
    "            print(f\"Diferencia Lexicon vs TF-IDF: {lexicon_best - tfidf_best:+.4f} (mejor), {lexicon_avg - tfidf_avg:+.4f} (promedio)\")\n",
    "    \n",
    "    print(\"\\nINTERPRETACIÓN PARA MODELOS GLOBALES:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(\"• TF-IDF vs TF: Normalización crucial para datos multi-dominio\")\n",
    "    print(\"• Lexicon: Características semánticas vs estadísticas textuales\")\n",
    "    print(\"• Modelo global: Mayor diversidad de datos puede favorecer representaciones robustas\")\n",
    "    \n",
    "    return {\n",
    "        'tf_scores': tf_scores,\n",
    "        'tfidf_scores': tfidf_scores, \n",
    "        'lexicon_scores': lexicon_scores\n",
    "    }\n",
    "\n",
    "def comparar_mejor_representacion_global(df_globales):\n",
    "    \"\"\"\n",
    "    Identifica la mejor representación para el modelo global\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"MEJOR REPRESENTACIÓN PARA MODELO GLOBAL\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Agrupar por tipo de representación\n",
    "    tf_models = [modelo for modelo in df_globales.index if 'TF (Global)' in modelo]\n",
    "    tfidf_models = [modelo for modelo in df_globales.index if 'TF-IDF (Global)' in modelo]\n",
    "    lexicon_models = [modelo for modelo in df_globales.index if 'Lexicon(SWN) (Global)' in modelo]\n",
    "    \n",
    "    resultados_globales = {}\n",
    "    \n",
    "    # Mejor modelo por tipo\n",
    "    if tf_models:\n",
    "        best_tf = df_globales.loc[tf_models, 'F1 Macro'].max()\n",
    "        best_tf_model = df_globales.loc[tf_models, 'F1 Macro'].idxmax()\n",
    "        resultados_globales['TF'] = {'F1': best_tf, 'Modelo': best_tf_model}\n",
    "    \n",
    "    if tfidf_models:\n",
    "        best_tfidf = df_globales.loc[tfidf_models, 'F1 Macro'].max()\n",
    "        best_tfidf_model = df_globales.loc[tfidf_models, 'F1 Macro'].idxmax()\n",
    "        resultados_globales['TF-IDF'] = {'F1': best_tfidf, 'Modelo': best_tfidf_model}\n",
    "    \n",
    "    if lexicon_models:\n",
    "        best_lexicon = df_globales.loc[lexicon_models, 'F1 Macro'].max()\n",
    "        best_lexicon_model = df_globales.loc[lexicon_models, 'F1 Macro'].idxmax()\n",
    "        resultados_globales['Lexicon'] = {'F1': best_lexicon, 'Modelo': best_lexicon_model}\n",
    "    \n",
    "    print(\"\\nRENDIMIENTO POR TIPO DE REPRESENTACIÓN:\")\n",
    "    print(\"-\" * 50)\n",
    "    for repr_type, data in resultados_globales.items():\n",
    "        print(f\"  {repr_type:12s}: F1={data['F1']:.4f} ({data['Modelo']})\")\n",
    "    \n",
    "    # Identificar la mejor representación global\n",
    "    if resultados_globales:\n",
    "        mejor_repr = max(resultados_globales.keys(), \n",
    "                       key=lambda x: resultados_globales[x]['F1'])\n",
    "        mejor_score = resultados_globales[mejor_repr]['F1']\n",
    "        mejor_modelo = resultados_globales[mejor_repr]['Modelo']\n",
    "        \n",
    "        print(f\"\\n→ MEJOR REPRESENTACIÓN GLOBAL: {mejor_repr}\")\n",
    "        print(f\"  Modelo: {mejor_modelo}\")\n",
    "        print(f\"  F1 Score: {mejor_score:.4f}\")\n",
    "        \n",
    "        # Calcular ventajas\n",
    "        print(f\"\\nVENTAJAS SOBRE OTRAS REPRESENTACIONES:\")\n",
    "        for repr_type, data in resultados_globales.items():\n",
    "            if repr_type != mejor_repr:\n",
    "                ventaja = mejor_score - data['F1']\n",
    "                print(f\"  vs {repr_type}: +{ventaja:.4f}\")\n",
    "    \n",
    "    return resultados_globales\n",
    "\n",
    "def analizar_impacto_combinacion_dominios(df_globales, tablas_categoria, categorias):\n",
    "    \"\"\"\n",
    "    Analiza cómo la combinación de dominios afecta diferentes representaciones\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"IMPACTO DE COMBINAR DOMINIOS EN REPRESENTACIONES\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Calcular promedio de modelos individuales por tipo de representación\n",
    "    tipos_individuales = {\n",
    "        'TF': [],\n",
    "        'TF-IDF': [], \n",
    "        'Lexicon': []\n",
    "    }\n",
    "    \n",
    "    for categoria in categorias:\n",
    "        if categoria in tablas_categoria:\n",
    "            tabla = tablas_categoria[categoria]\n",
    "            \n",
    "            # TF scores\n",
    "            tf_models = [m for m in tabla.index if 'TF' in m and 'TF-IDF' not in m]\n",
    "            if tf_models:\n",
    "                tipos_individuales['TF'].extend(tabla.loc[tf_models, 'F1 Macro'].tolist())\n",
    "            \n",
    "            # TF-IDF scores\n",
    "            tfidf_models = [m for m in tabla.index if 'TF-IDF' in m]\n",
    "            if tfidf_models:\n",
    "                tipos_individuales['TF-IDF'].extend(tabla.loc[tfidf_models, 'F1 Macro'].tolist())\n",
    "            \n",
    "            # Lexicon scores\n",
    "            lexicon_models = [m for m in tabla.index if 'Lexicon' in m]\n",
    "            if lexicon_models:\n",
    "                tipos_individuales['Lexicon'].extend(tabla.loc[lexicon_models, 'F1 Macro'].tolist())\n",
    "    \n",
    "    # Obtener scores globales\n",
    "    tipos_globales = {}\n",
    "    \n",
    "    tf_global_models = [m for m in df_globales.index if 'TF (Global)' in m]\n",
    "    if tf_global_models:\n",
    "        tipos_globales['TF'] = df_globales.loc[tf_global_models, 'F1 Macro'].tolist()\n",
    "    \n",
    "    tfidf_global_models = [m for m in df_globales.index if 'TF-IDF (Global)' in m]\n",
    "    if tfidf_global_models:\n",
    "        tipos_globales['TF-IDF'] = df_globales.loc[tfidf_global_models, 'F1 Macro'].tolist()\n",
    "    \n",
    "    lexicon_global_models = [m for m in df_globales.index if 'Lexicon(SWN) (Global)' in m]\n",
    "    if lexicon_global_models:\n",
    "        tipos_globales['Lexicon'] = df_globales.loc[lexicon_global_models, 'F1 Macro'].tolist()\n",
    "    \n",
    "    print(\"\\nCOMPARACIÓN: MODELOS INDIVIDUALES vs GLOBALES\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for tipo in ['TF', 'TF-IDF', 'Lexicon']:\n",
    "        if tipo in tipos_individuales and tipo in tipos_globales:\n",
    "            ind_promedio = np.mean(tipos_individuales[tipo])\n",
    "            ind_mejor = np.max(tipos_individuales[tipo])\n",
    "            \n",
    "            glob_promedio = np.mean(tipos_globales[tipo])\n",
    "            glob_mejor = np.max(tipos_globales[tipo])\n",
    "            \n",
    "            print(f\"\\n{tipo}:\")\n",
    "            print(f\"  Individual → Promedio: {ind_promedio:.4f}, Mejor: {ind_mejor:.4f}\")\n",
    "            print(f\"  Global     → Promedio: {glob_promedio:.4f}, Mejor: {glob_mejor:.4f}\")\n",
    "            print(f\"  Diferencia → Promedio: {glob_promedio - ind_promedio:+.4f}, Mejor: {glob_mejor - ind_mejor:+.4f}\")\n",
    "            \n",
    "            if glob_promedio > ind_promedio:\n",
    "                print(f\"  → Modelo global MEJOR con {tipo}\")\n",
    "            elif glob_promedio < ind_promedio:\n",
    "                print(f\"  → Modelos individuales MEJORES con {tipo}\")\n",
    "            else:\n",
    "                print(f\"  → Rendimiento similar con {tipo}\")\n",
    "    \n",
    "    print(f\"\\nCONCLUSIONES SOBRE COMBINACIÓN DE DOMINIOS:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(\"• Modelos globales pueden generalizar mejor entre dominios\")\n",
    "    print(\"• Representaciones robustas (TF-IDF) se benefician más de más datos\")\n",
    "    print(\"• Características lexicon pueden ser más estables entre dominios\")\n",
    "\n",
    "# Función principal para ejecutar comparación global de representaciones\n",
    "def ejecutar_comparacion_representaciones_global(df_globales, tablas_categoria=None, categorias=None):\n",
    "    \"\"\"\n",
    "    Ejecuta todas las comparaciones de representación para modelos globales\n",
    "    \"\"\"\n",
    "    # 1. Comparación general por tipo\n",
    "    df_tipos, resultados_tipo = comparar_por_tipo_representacion_global(df_globales)\n",
    "    \n",
    "    # 2. Mejor representación global\n",
    "    resultados_globales = comparar_mejor_representacion_global(df_globales)\n",
    "    \n",
    "    # 3. Análisis de complementariedad\n",
    "    scores_repr = analizar_complementariedad_global(df_globales)\n",
    "    \n",
    "    # 4. Impacto de combinación (si se proporcionan datos individuales)\n",
    "    if tablas_categoria is not None and categorias is not None:\n",
    "        analizar_impacto_combinacion_dominios(df_globales, tablas_categoria, categorias)\n",
    "    \n",
    "    return df_tipos, resultados_tipo, resultados_globales, scores_repr\n",
    "\n",
    "# Ejemplo de uso:\n",
    "df_tipos, resultados_tipo, resultados_globales, scores_repr = ejecutar_comparacion_representaciones_global(df_globales, tablas_categoria, CATEGORIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "565e78d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COMPARACIÓN ALGORITMOS GLOBALES: NAIVE BAYES vs LOGISTIC REGRESSION\n",
      "================================================================================\n",
      "\n",
      "RENDIMIENTO PROMEDIO POR ALGORITMO (MODELOS GLOBALES):\n",
      "----------------------------------------------------------------------\n",
      "                    F1 Macro Promedio F1 Macro Std Accuracy Promedio  \\\n",
      "Logistic Regression          0.803781     0.065041          0.803889   \n",
      "Naive Bayes                  0.785793     0.072123          0.787361   \n",
      "\n",
      "                    Precision Promedio Recall Promedio  Mejor F1  \n",
      "Logistic Regression           0.804442        0.803889  0.852905  \n",
      "Naive Bayes                   0.793748        0.787361  0.842486  \n",
      "\n",
      "ANÁLISIS DE DIFERENCIAS:\n",
      "------------------------------\n",
      "Diferencia promedio (LR - NB): +0.0180\n",
      "Diferencia mejor modelo:       +0.0104\n",
      "→ Logistic Regression supera a Naive Bayes en modelos globales\n",
      "\n",
      "================================================================================\n",
      "NB vs LR POR TIPO DE REPRESENTACIÓN (MODELOS GLOBALES)\n",
      "================================================================================\n",
      "\n",
      "TF:\n",
      "----------------------------------------\n",
      "  NB:  F1=0.8309, Acc=0.8313\n",
      "  LR:  F1=0.8529, Acc=0.8529\n",
      "  Diferencia F1:  +0.0220\n",
      "  Diferencia Acc: +0.0217\n",
      "  → LR claramente superior con TF\n",
      "\n",
      "TF-IDF:\n",
      "----------------------------------------\n",
      "  NB:  F1=0.8425, Acc=0.8433\n",
      "  LR:  F1=0.8466, Acc=0.8467\n",
      "  Diferencia F1:  +0.0041\n",
      "  Diferencia Acc: +0.0033\n",
      "  → Rendimiento similar con TF-IDF\n",
      "\n",
      "Lexicon:\n",
      "----------------------------------------\n",
      "  NB:  F1=0.6840, Acc=0.6875\n",
      "  LR:  F1=0.7119, Acc=0.7121\n",
      "  Diferencia F1:  +0.0279\n",
      "  Diferencia Acc: +0.0246\n",
      "  → LR claramente superior con Lexicon\n",
      "\n",
      "================================================================================\n",
      "RESUMEN DE VICTORIAS: NB vs LR (MODELOS GLOBALES)\n",
      "================================================================================\n",
      "\n",
      "RESULTADOS POR REPRESENTACIÓN:\n",
      "--------------------------------------------------\n",
      "  TF          : LR gana (diferencia: +0.0220)\n",
      "  TF-IDF      : LR gana (diferencia: +0.0041)\n",
      "  Lexicon     : LR gana (diferencia: +0.0279)\n",
      "\n",
      "CONTEO TOTAL DE VICTORIAS:\n",
      "------------------------------\n",
      "  NB                  : 0/3 (0.0%)\n",
      "  LR                  : 3/3 (100.0%)\n",
      "  Empates             : 0/3 (0.0%)\n",
      "\n",
      "================================================================================\n",
      "ANÁLISIS DE ESTABILIDAD: ALGORITMOS GLOBALES\n",
      "================================================================================\n",
      "\n",
      "ESTADÍSTICAS DE ESTABILIDAD:\n",
      "----------------------------------------\n",
      "Naive Bayes:\n",
      "  Promedio: 0.7858\n",
      "  Std Dev:  0.0721\n",
      "  Rango:    0.1585\n",
      "  Scores:   ['0.831', '0.842', '0.684']\n",
      "\n",
      "Logistic Regression:\n",
      "  Promedio: 0.8038\n",
      "  Std Dev:  0.0650\n",
      "  Rango:    0.1410\n",
      "  Scores:   ['0.853', '0.847', '0.712']\n",
      "\n",
      "COMPARACIÓN DE ESTABILIDAD:\n",
      "------------------------------\n",
      "→ LR es más estable (menor varianza)\n",
      "→ Diferencia en std: 0.0071\n",
      "\n",
      "================================================================================\n",
      "COMPARACIÓN: MODELOS GLOBALES vs INDIVIDUALES\n",
      "================================================================================\n",
      "\n",
      "COMPARACIÓN DE RENDIMIENTOS:\n",
      "----------------------------------------\n",
      "Naive Bayes:\n",
      "  Individual → Promedio: 0.7837, Mejor: 0.8800\n",
      "  Global     → Promedio: 0.7858, Mejor: 0.8425\n",
      "  Diferencia → Promedio: +0.0021, Mejor: -0.0375\n",
      "\n",
      "Logistic Regression:\n",
      "  Individual → Promedio: 0.8000, Mejor: 0.8850\n",
      "  Global     → Promedio: 0.8038, Mejor: 0.8529\n",
      "  Diferencia → Promedio: +0.0038, Mejor: -0.0321\n",
      "\n",
      "CONCLUSIONES:\n",
      "--------------------\n",
      "• Modelos globales: Entrenados en datos combinados de todos los dominios\n",
      "• Modelos individuales: Especializados en cada categoría\n",
      "• Comparación muestra trade-off entre especialización vs generalización\n",
      "\n",
      "================================================================================\n",
      "ESTADÍSTICAS DETALLADAS: NB vs LR (MODELOS GLOBALES)\n",
      "================================================================================\n",
      "\n",
      "COMPARACIÓN ESTADÍSTICA DETALLADA:\n",
      "---------------------------------------------\n",
      "\n",
      "F1:\n",
      "  NB: μ=0.7858, σ=0.0721, min=0.6840, max=0.8425\n",
      "  LR: μ=0.8038, σ=0.0650, min=0.7119, max=0.8529\n",
      "  Diferencia (LR-NB): +0.0180\n",
      "\n",
      "Accuracy:\n",
      "  NB: μ=0.7874, σ=0.0708, min=0.6875, max=0.8433\n",
      "  LR: μ=0.8039, σ=0.0650, min=0.7121, max=0.8529\n",
      "  Diferencia (LR-NB): +0.0165\n",
      "\n",
      "Precision:\n",
      "  NB: μ=0.7937, σ=0.0693, min=0.6962, max=0.8509\n",
      "  LR: μ=0.8044, σ=0.0649, min=0.7127, max=0.8530\n",
      "  Diferencia (LR-NB): +0.0107\n",
      "\n",
      "Recall:\n",
      "  NB: μ=0.7874, σ=0.0708, min=0.6875, max=0.8433\n",
      "  LR: μ=0.8039, σ=0.0650, min=0.7121, max=0.8529\n",
      "  Diferencia (LR-NB): +0.0165\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# COMPARACIÓN DE ALGORITMOS GLOBALES: NAIVE BAYES vs LOGISTIC REGRESSION\n",
    "# =============================================================================\n",
    "\n",
    "def comparar_nb_vs_lr_global(df_globales):\n",
    "    \"\"\"\n",
    "    Compara el rendimiento entre Naive Bayes y Logistic Regression en modelos globales\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"COMPARACIÓN ALGORITMOS GLOBALES: NAIVE BAYES vs LOGISTIC REGRESSION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Agrupar modelos por algoritmo\n",
    "    algoritmos = {\n",
    "        'Naive Bayes': [modelo for modelo in df_globales.index if 'NB' in modelo],\n",
    "        'Logistic Regression': [modelo for modelo in df_globales.index if 'LR' in modelo]\n",
    "    }\n",
    "    \n",
    "    resultados_por_algoritmo = {}\n",
    "    \n",
    "    for algoritmo, modelos in algoritmos.items():\n",
    "        if modelos:\n",
    "            f1_scores = df_globales.loc[modelos, 'F1 Macro'].tolist()\n",
    "            accuracy_scores = df_globales.loc[modelos, 'Accuracy'].tolist()\n",
    "            precision_scores = df_globales.loc[modelos, 'Precision Macro'].tolist()\n",
    "            recall_scores = df_globales.loc[modelos, 'Recall Macro'].tolist()\n",
    "            \n",
    "            resultados_por_algoritmo[algoritmo] = {\n",
    "                'F1 Macro Promedio': np.mean(f1_scores),\n",
    "                'F1 Macro Std': np.std(f1_scores),\n",
    "                'Accuracy Promedio': np.mean(accuracy_scores),\n",
    "                'Accuracy Std': np.std(accuracy_scores),\n",
    "                'Precision Promedio': np.mean(precision_scores),\n",
    "                'Recall Promedio': np.mean(recall_scores),\n",
    "                'Mejor F1': np.max(f1_scores),\n",
    "                'Peor F1': np.min(f1_scores),\n",
    "                'Mejor Modelo': df_globales.loc[modelos, 'F1 Macro'].idxmax(),\n",
    "                'Num Modelos': len(f1_scores),\n",
    "                'Todos los F1': f1_scores\n",
    "            }\n",
    "    \n",
    "    # Crear DataFrame para comparación\n",
    "    df_algoritmos = pd.DataFrame(resultados_por_algoritmo).T\n",
    "    df_algoritmos = df_algoritmos.sort_values('F1 Macro Promedio', ascending=False)\n",
    "    \n",
    "    print(\"\\nRENDIMIENTO PROMEDIO POR ALGORITMO (MODELOS GLOBALES):\")\n",
    "    print(\"-\" * 70)\n",
    "    cols_mostrar = ['F1 Macro Promedio', 'F1 Macro Std', 'Accuracy Promedio', \n",
    "                   'Precision Promedio', 'Recall Promedio', 'Mejor F1']\n",
    "    print(df_algoritmos[cols_mostrar].round(4))\n",
    "    \n",
    "    # Análisis de ventaja estadística\n",
    "    if 'Naive Bayes' in resultados_por_algoritmo and 'Logistic Regression' in resultados_por_algoritmo:\n",
    "        nb_scores = resultados_por_algoritmo['Naive Bayes']['Todos los F1']\n",
    "        lr_scores = resultados_por_algoritmo['Logistic Regression']['Todos los F1']\n",
    "        \n",
    "        diferencia_promedio = np.mean(lr_scores) - np.mean(nb_scores)\n",
    "        diferencia_mejor = resultados_por_algoritmo['Logistic Regression']['Mejor F1'] - resultados_por_algoritmo['Naive Bayes']['Mejor F1']\n",
    "        \n",
    "        print(f\"\\nANÁLISIS DE DIFERENCIAS:\")\n",
    "        print(\"-\" * 30)\n",
    "        print(f\"Diferencia promedio (LR - NB): {diferencia_promedio:+.4f}\")\n",
    "        print(f\"Diferencia mejor modelo:       {diferencia_mejor:+.4f}\")\n",
    "        \n",
    "        if diferencia_promedio > 0:\n",
    "            print(\"→ Logistic Regression supera a Naive Bayes en modelos globales\")\n",
    "        else:\n",
    "            print(\"→ Naive Bayes supera a Logistic Regression en modelos globales\")\n",
    "    \n",
    "    return df_algoritmos, resultados_por_algoritmo\n",
    "\n",
    "def analizar_nb_vs_lr_por_representacion_global(df_globales):\n",
    "    \"\"\"\n",
    "    Analiza cómo se comportan NB vs LR con diferentes representaciones en modelos globales\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"NB vs LR POR TIPO DE REPRESENTACIÓN (MODELOS GLOBALES)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    representaciones = [\n",
    "        ('TF', 'TF (Global)'),\n",
    "        ('TF-IDF', 'TF-IDF (Global)'),\n",
    "        ('Lexicon', 'Lexicon(SWN) (Global)')\n",
    "    ]\n",
    "    \n",
    "    comparaciones_repr = {}\n",
    "    \n",
    "    for repr_name, repr_suffix in representaciones:\n",
    "        print(f\"\\n{repr_name}:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Buscar modelos correspondientes\n",
    "        nb_model = f'NB + {repr_suffix}'\n",
    "        lr_model = f'LR + {repr_suffix}'\n",
    "        \n",
    "        nb_score = None\n",
    "        lr_score = None\n",
    "        \n",
    "        if nb_model in df_globales.index:\n",
    "            nb_score = df_globales.loc[nb_model, 'F1 Macro']\n",
    "            nb_acc = df_globales.loc[nb_model, 'Accuracy']\n",
    "            \n",
    "        if lr_model in df_globales.index:\n",
    "            lr_score = df_globales.loc[lr_model, 'F1 Macro']\n",
    "            lr_acc = df_globales.loc[lr_model, 'Accuracy']\n",
    "        \n",
    "        if nb_score is not None and lr_score is not None:\n",
    "            diferencia_f1 = lr_score - nb_score\n",
    "            diferencia_acc = lr_acc - nb_acc\n",
    "            \n",
    "            print(f\"  NB:  F1={nb_score:.4f}, Acc={nb_acc:.4f}\")\n",
    "            print(f\"  LR:  F1={lr_score:.4f}, Acc={lr_acc:.4f}\")\n",
    "            print(f\"  Diferencia F1:  {diferencia_f1:+.4f}\")\n",
    "            print(f\"  Diferencia Acc: {diferencia_acc:+.4f}\")\n",
    "            \n",
    "            if diferencia_f1 > 0.01:\n",
    "                print(f\"  → LR claramente superior con {repr_name}\")\n",
    "                ganador = \"LR\"\n",
    "            elif diferencia_f1 < -0.01:\n",
    "                print(f\"  → NB claramente superior con {repr_name}\")\n",
    "                ganador = \"NB\"\n",
    "            else:\n",
    "                print(f\"  → Rendimiento similar con {repr_name}\")\n",
    "                ganador = \"Similar\"\n",
    "            \n",
    "            comparaciones_repr[repr_name] = {\n",
    "                'nb_f1': nb_score,\n",
    "                'lr_f1': lr_score,\n",
    "                'nb_acc': nb_acc,\n",
    "                'lr_acc': lr_acc,\n",
    "                'diferencia_f1': diferencia_f1,\n",
    "                'diferencia_acc': diferencia_acc,\n",
    "                'ganador': ganador\n",
    "            }\n",
    "        else:\n",
    "            print(f\"  → Datos incompletos para {repr_name}\")\n",
    "            comparaciones_repr[repr_name] = None\n",
    "    \n",
    "    return comparaciones_repr\n",
    "\n",
    "def crear_resumen_victorias_global(df_globales):\n",
    "    \"\"\"\n",
    "    Crea resumen de victorias para modelos globales\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"RESUMEN DE VICTORIAS: NB vs LR (MODELOS GLOBALES)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    representaciones = [\n",
    "        ('TF', 'TF (Global)'),\n",
    "        ('TF-IDF', 'TF-IDF (Global)'),\n",
    "        ('Lexicon', 'Lexicon(SWN) (Global)')\n",
    "    ]\n",
    "    \n",
    "    victorias = {'NB': 0, 'LR': 0, 'Empates': 0}\n",
    "    detalles_victorias = []\n",
    "    \n",
    "    print(\"\\nRESULTADOS POR REPRESENTACIÓN:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for repr_name, repr_suffix in representaciones:\n",
    "        nb_model = f'NB + {repr_suffix}'\n",
    "        lr_model = f'LR + {repr_suffix}'\n",
    "        \n",
    "        if nb_model in df_globales.index and lr_model in df_globales.index:\n",
    "            nb_score = df_globales.loc[nb_model, 'F1 Macro']\n",
    "            lr_score = df_globales.loc[lr_model, 'F1 Macro']\n",
    "            \n",
    "            if lr_score > nb_score:\n",
    "                ganador = 'LR'\n",
    "                victorias['LR'] += 1\n",
    "                diferencia = lr_score - nb_score\n",
    "            elif nb_score > lr_score:\n",
    "                ganador = 'NB'\n",
    "                victorias['NB'] += 1\n",
    "                diferencia = nb_score - lr_score\n",
    "            else:\n",
    "                ganador = 'Empate'\n",
    "                victorias['Empates'] += 1\n",
    "                diferencia = 0\n",
    "            \n",
    "            print(f\"  {repr_name:12s}: {ganador} gana (diferencia: {diferencia:+.4f})\")\n",
    "            detalles_victorias.append({\n",
    "                'representacion': repr_name,\n",
    "                'ganador': ganador,\n",
    "                'nb_score': nb_score,\n",
    "                'lr_score': lr_score,\n",
    "                'diferencia': diferencia\n",
    "            })\n",
    "    \n",
    "    total_comparaciones = sum(victorias.values())\n",
    "    \n",
    "    print(f\"\\nCONTEO TOTAL DE VICTORIAS:\")\n",
    "    print(\"-\" * 30)\n",
    "    for algoritmo, wins in victorias.items():\n",
    "        if total_comparaciones > 0:\n",
    "            porcentaje = (wins / total_comparaciones) * 100\n",
    "            print(f\"  {algoritmo:20s}: {wins}/{total_comparaciones} ({porcentaje:.1f}%)\")\n",
    "    \n",
    "    return victorias, detalles_victorias\n",
    "\n",
    "def analizar_estabilidad_algoritmos_global(df_globales):\n",
    "    \"\"\"\n",
    "    Analiza la estabilidad de cada algoritmo en modelos globales\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ANÁLISIS DE ESTABILIDAD: ALGORITMOS GLOBALES\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    nb_models = [modelo for modelo in df_globales.index if 'NB' in modelo]\n",
    "    lr_models = [modelo for modelo in df_globales.index if 'LR' in modelo]\n",
    "    \n",
    "    nb_f1_scores = df_globales.loc[nb_models, 'F1 Macro'].tolist() if nb_models else []\n",
    "    lr_f1_scores = df_globales.loc[lr_models, 'F1 Macro'].tolist() if lr_models else []\n",
    "    \n",
    "    print(\"\\nESTADÍSTICAS DE ESTABILIDAD:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    if nb_f1_scores:\n",
    "        nb_mean = np.mean(nb_f1_scores)\n",
    "        nb_std = np.std(nb_f1_scores)\n",
    "        nb_range = max(nb_f1_scores) - min(nb_f1_scores)\n",
    "        \n",
    "        print(f\"Naive Bayes:\")\n",
    "        print(f\"  Promedio: {nb_mean:.4f}\")\n",
    "        print(f\"  Std Dev:  {nb_std:.4f}\")\n",
    "        print(f\"  Rango:    {nb_range:.4f}\")\n",
    "        print(f\"  Scores:   {[f'{s:.3f}' for s in nb_f1_scores]}\")\n",
    "    \n",
    "    if lr_f1_scores:\n",
    "        lr_mean = np.mean(lr_f1_scores)\n",
    "        lr_std = np.std(lr_f1_scores)\n",
    "        lr_range = max(lr_f1_scores) - min(lr_f1_scores)\n",
    "        \n",
    "        print(f\"\\nLogistic Regression:\")\n",
    "        print(f\"  Promedio: {lr_mean:.4f}\")\n",
    "        print(f\"  Std Dev:  {lr_std:.4f}\")\n",
    "        print(f\"  Rango:    {lr_range:.4f}\")\n",
    "        print(f\"  Scores:   {[f'{s:.3f}' for s in lr_f1_scores]}\")\n",
    "    \n",
    "    # Análisis de estabilidad\n",
    "    if nb_f1_scores and lr_f1_scores:\n",
    "        print(f\"\\nCOMPARACIÓN DE ESTABILIDAD:\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        if nb_std < lr_std:\n",
    "            print(f\"→ NB es más estable (menor varianza)\")\n",
    "            mas_estable = \"NB\"\n",
    "        elif lr_std < nb_std:\n",
    "            print(f\"→ LR es más estable (menor varianza)\")\n",
    "            mas_estable = \"LR\"\n",
    "        else:\n",
    "            print(f\"→ Estabilidad similar\")\n",
    "            mas_estable = \"Similar\"\n",
    "        \n",
    "        diferencia_estabilidad = abs(nb_std - lr_std)\n",
    "        print(f\"→ Diferencia en std: {diferencia_estabilidad:.4f}\")\n",
    "        \n",
    "        return {\n",
    "            'nb_stats': {'mean': nb_mean, 'std': nb_std, 'range': nb_range, 'scores': nb_f1_scores},\n",
    "            'lr_stats': {'mean': lr_mean, 'std': lr_std, 'range': lr_range, 'scores': lr_f1_scores},\n",
    "            'mas_estable': mas_estable,\n",
    "            'diferencia_estabilidad': diferencia_estabilidad\n",
    "        }\n",
    "    \n",
    "    return None\n",
    "\n",
    "def comparar_global_vs_individual(df_globales, tablas_categoria, categorias):\n",
    "    \"\"\"\n",
    "    Compara rendimiento de modelos globales vs promedio de modelos individuales\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COMPARACIÓN: MODELOS GLOBALES vs INDIVIDUALES\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Calcular promedios de modelos individuales\n",
    "    nb_individual = []\n",
    "    lr_individual = []\n",
    "    \n",
    "    for categoria in categorias:\n",
    "        if categoria in tablas_categoria:\n",
    "            tabla = tablas_categoria[categoria]\n",
    "            \n",
    "            for modelo in tabla.index:\n",
    "                if 'NB' in modelo:\n",
    "                    nb_individual.append(tabla.loc[modelo, 'F1 Macro'])\n",
    "                elif 'LR' in modelo:\n",
    "                    lr_individual.append(tabla.loc[modelo, 'F1 Macro'])\n",
    "    \n",
    "    # Obtener scores globales\n",
    "    nb_global = [df_globales.loc[m, 'F1 Macro'] for m in df_globales.index if 'NB' in m]\n",
    "    lr_global = [df_globales.loc[m, 'F1 Macro'] for m in df_globales.index if 'LR' in m]\n",
    "    \n",
    "    print(\"\\nCOMPARACIÓN DE RENDIMIENTOS:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    if nb_individual and nb_global:\n",
    "        nb_ind_promedio = np.mean(nb_individual)\n",
    "        nb_ind_mejor = np.max(nb_individual)\n",
    "        nb_glob_promedio = np.mean(nb_global)\n",
    "        nb_glob_mejor = np.max(nb_global)\n",
    "        \n",
    "        print(f\"Naive Bayes:\")\n",
    "        print(f\"  Individual → Promedio: {nb_ind_promedio:.4f}, Mejor: {nb_ind_mejor:.4f}\")\n",
    "        print(f\"  Global     → Promedio: {nb_glob_promedio:.4f}, Mejor: {nb_glob_mejor:.4f}\")\n",
    "        print(f\"  Diferencia → Promedio: {nb_glob_promedio - nb_ind_promedio:+.4f}, Mejor: {nb_glob_mejor - nb_ind_mejor:+.4f}\")\n",
    "    \n",
    "    if lr_individual and lr_global:\n",
    "        lr_ind_promedio = np.mean(lr_individual)\n",
    "        lr_ind_mejor = np.max(lr_individual)\n",
    "        lr_glob_promedio = np.mean(lr_global)\n",
    "        lr_glob_mejor = np.max(lr_global)\n",
    "        \n",
    "        print(f\"\\nLogistic Regression:\")\n",
    "        print(f\"  Individual → Promedio: {lr_ind_promedio:.4f}, Mejor: {lr_ind_mejor:.4f}\")\n",
    "        print(f\"  Global     → Promedio: {lr_glob_promedio:.4f}, Mejor: {lr_glob_mejor:.4f}\")\n",
    "        print(f\"  Diferencia → Promedio: {lr_glob_promedio - lr_ind_promedio:+.4f}, Mejor: {lr_glob_mejor - lr_ind_mejor:+.4f}\")\n",
    "    \n",
    "    print(f\"\\nCONCLUSIONES:\")\n",
    "    print(\"-\" * 20)\n",
    "    print(\"• Modelos globales: Entrenados en datos combinados de todos los dominios\")\n",
    "    print(\"• Modelos individuales: Especializados en cada categoría\")\n",
    "    print(\"• Comparación muestra trade-off entre especialización vs generalización\")\n",
    "\n",
    "def crear_estadisticas_detalladas_nb_lr_global(df_globales):\n",
    "    \"\"\"\n",
    "    Crea estadísticas detalladas para el análisis NB vs LR global\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ESTADÍSTICAS DETALLADAS: NB vs LR (MODELOS GLOBALES)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Recopilar todas las métricas\n",
    "    nb_models = [modelo for modelo in df_globales.index if 'NB' in modelo]\n",
    "    lr_models = [modelo for modelo in df_globales.index if 'LR' in modelo]\n",
    "    \n",
    "    metricas_nb = {\n",
    "        'F1': df_globales.loc[nb_models, 'F1 Macro'].tolist() if nb_models else [],\n",
    "        'Accuracy': df_globales.loc[nb_models, 'Accuracy'].tolist() if nb_models else [],\n",
    "        'Precision': df_globales.loc[nb_models, 'Precision Macro'].tolist() if nb_models else [],\n",
    "        'Recall': df_globales.loc[nb_models, 'Recall Macro'].tolist() if nb_models else []\n",
    "    }\n",
    "    \n",
    "    metricas_lr = {\n",
    "        'F1': df_globales.loc[lr_models, 'F1 Macro'].tolist() if lr_models else [],\n",
    "        'Accuracy': df_globales.loc[lr_models, 'Accuracy'].tolist() if lr_models else [],\n",
    "        'Precision': df_globales.loc[lr_models, 'Precision Macro'].tolist() if lr_models else [],\n",
    "        'Recall': df_globales.loc[lr_models, 'Recall Macro'].tolist() if lr_models else []\n",
    "    }\n",
    "    \n",
    "    print(\"\\nCOMPARACIÓN ESTADÍSTICA DETALLADA:\")\n",
    "    print(\"-\" * 45)\n",
    "    \n",
    "    for metrica in ['F1', 'Accuracy', 'Precision', 'Recall']:\n",
    "        nb_vals = metricas_nb[metrica]\n",
    "        lr_vals = metricas_lr[metrica]\n",
    "        \n",
    "        if nb_vals and lr_vals:\n",
    "            print(f\"\\n{metrica}:\")\n",
    "            print(f\"  NB: μ={np.mean(nb_vals):.4f}, σ={np.std(nb_vals):.4f}, min={np.min(nb_vals):.4f}, max={np.max(nb_vals):.4f}\")\n",
    "            print(f\"  LR: μ={np.mean(lr_vals):.4f}, σ={np.std(lr_vals):.4f}, min={np.min(lr_vals):.4f}, max={np.max(lr_vals):.4f}\")\n",
    "            \n",
    "            diferencia = np.mean(lr_vals) - np.mean(nb_vals)\n",
    "            print(f\"  Diferencia (LR-NB): {diferencia:+.4f}\")\n",
    "    \n",
    "    return {'NB': metricas_nb, 'LR': metricas_lr}\n",
    "\n",
    "# Función principal para ejecutar todas las comparaciones NB vs LR globales\n",
    "def ejecutar_comparacion_nb_lr_global_completa(df_globales, tablas_categoria=None, categorias=None):\n",
    "    \"\"\"\n",
    "    Ejecuta todas las comparaciones entre Naive Bayes y Logistic Regression para modelos globales\n",
    "    \"\"\"\n",
    "    # 1. Comparación general\n",
    "    df_algoritmos, resultados_alg = comparar_nb_vs_lr_global(df_globales)\n",
    "    \n",
    "    # 2. Análisis por representación\n",
    "    comparaciones_repr = analizar_nb_vs_lr_por_representacion_global(df_globales)\n",
    "    \n",
    "    # 3. Resumen de victorias\n",
    "    victorias, detalles = crear_resumen_victorias_global(df_globales)\n",
    "    \n",
    "    # 4. Análisis de estabilidad\n",
    "    estabilidad = analizar_estabilidad_algoritmos_global(df_globales)\n",
    "    \n",
    "    # 5. Comparación global vs individual (si se proporcionan datos)\n",
    "    if tablas_categoria is not None and categorias is not None:\n",
    "        comparar_global_vs_individual(df_globales, tablas_categoria, categorias)\n",
    "    \n",
    "    # 6. Estadísticas detalladas\n",
    "    metricas_detalladas = crear_estadisticas_detalladas_nb_lr_global(df_globales)\n",
    "    \n",
    "    return df_algoritmos, resultados_alg, comparaciones_repr, victorias, detalles, estabilidad, metricas_detalladas\n",
    "\n",
    "# Ejemplo de uso:\n",
    "df_alg, res_alg, comp_repr, victorias, detalles, estabilidad, metricas = ejecutar_comparacion_nb_lr_global_completa(df_globales, tablas_categoria, CATEGORIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4372cb2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CARACTERÍSTICAS MÁS IMPORTANTES - MODELO GLOBAL\n",
      "================================================================================\n",
      "Analizando modelo global: LR + TF-IDF (Global)\n",
      "------------------------------------------------------------\n",
      "\n",
      "CARACTERÍSTICAS PRO-POSITIVAS (Modelo Global):\n",
      "   great                     :  +6.0325\n",
      "   excellent                 :  +3.5396\n",
      "   love                      :  +3.2534\n",
      "   easy                      :  +3.1149\n",
      "   best                      :  +3.0337\n",
      "   easy_to                   :  +2.4416\n",
      "   the_best                  :  +2.3529\n",
      "   perfect                   :  +2.2856\n",
      "   price                     :  +2.2393\n",
      "   well                      :  +2.1104\n",
      "   a_great                   :  +2.0907\n",
      "   my                        :  +1.8678\n",
      "   good                      :  +1.8520\n",
      "   wonderful                 :  +1.7742\n",
      "   i_love                    :  +1.7147\n",
      "\n",
      "CARACTERÍSTICAS PRO-NEGATIVAS (Modelo Global):\n",
      "   not                       :  -5.4339\n",
      "   bad                       :  -2.9638\n",
      "   waste                     :  -2.7388\n",
      "   was                       :  -2.6585\n",
      "   disappointed              :  -2.3405\n",
      "   poor                      :  -2.2941\n",
      "   worst                     :  -2.2875\n",
      "   after                     :  -1.9476\n",
      "   boring                    :  -1.9126\n",
      "   money                     :  -1.8837\n",
      "   your_money                :  -1.8258\n",
      "   terrible                  :  -1.7584\n",
      "   too                       :  -1.7337\n",
      "   get                       :  -1.7187\n",
      "   disappointing             :  -1.6921\n",
      "\n",
      "============================================================\n",
      "COMPARACIÓN: INDIVIDUAL vs GLOBAL\n",
      "============================================================\n",
      "\n",
      "Características globales más importantes:\n",
      "  Positivas: ['great', 'excellent', 'love', 'easy', 'best', 'easy_to', 'the_best', 'perfect']\n",
      "  Negativas: ['not', 'bad', 'waste', 'was', 'disappointed', 'poor', 'worst', 'after']\n",
      "\n",
      "Comparación con modelos individuales:\n",
      "  Palabras positivas comunes: ['price', 'best', 'easy_to', 'love', 'easy', 'great', 'the_best', 'perfect']\n",
      "  Palabras negativas comunes: ['worst', 'waste', 'disappointed', 'was', 'poor', 'not', 'bad', 'after']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CARACTERÍSTICAS MÁS IMPORTANTES - MODELO GLOBAL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def extraer_features_modelo_global(modelos_globales_entrenados, top_k=15):\n",
    "    \"\"\"Extrae características del mejor modelo global\"\"\"\n",
    "    \n",
    "    # Buscar el mejor modelo LR global\n",
    "    lr_global_model = None\n",
    "    modelo_name = None\n",
    "    \n",
    "    # Priorizar LR + TF-IDF\n",
    "    for name, model in modelos_globales_entrenados.items():\n",
    "        if 'LR' in name and 'TF-IDF' in name:\n",
    "            lr_global_model = model\n",
    "            modelo_name = name\n",
    "            break\n",
    "    \n",
    "    # Si no hay TF-IDF, buscar cualquier LR\n",
    "    if lr_global_model is None:\n",
    "        for name, model in modelos_globales_entrenados.items():\n",
    "            if 'LR' in name:\n",
    "                lr_global_model = model\n",
    "                modelo_name = name\n",
    "                break\n",
    "    \n",
    "    if lr_global_model is not None:\n",
    "        print(f\"Analizando modelo global: {modelo_name}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        # Extraer características\n",
    "        if hasattr(lr_global_model.named_steps, 'dict'):\n",
    "            vectorizer = lr_global_model.named_steps['dict']\n",
    "            feature_names = vectorizer.get_feature_names_out()\n",
    "            \n",
    "            if hasattr(lr_global_model.named_steps, 'lr'):\n",
    "                lr_classifier = lr_global_model.named_steps['lr']\n",
    "                coefs = lr_classifier.coef_.ravel()\n",
    "                \n",
    "                # Top features\n",
    "                top_pos_idx = np.argsort(coefs)[-top_k:]\n",
    "                top_neg_idx = np.argsort(coefs)[:top_k]\n",
    "                \n",
    "                print(\"\\nCARACTERÍSTICAS PRO-POSITIVAS (Modelo Global):\")\n",
    "                for i in reversed(top_pos_idx):\n",
    "                    print(f\"   {feature_names[i]:25s} : {coefs[i]:+8.4f}\")\n",
    "                \n",
    "                print(\"\\nCARACTERÍSTICAS PRO-NEGATIVAS (Modelo Global):\")\n",
    "                for i in top_neg_idx:\n",
    "                    print(f\"   {feature_names[i]:25s} : {coefs[i]:+8.4f}\")\n",
    "                \n",
    "                global_features = {\n",
    "                    'top_positive': [(feature_names[i], coefs[i]) for i in reversed(top_pos_idx)],\n",
    "                    'top_negative': [(feature_names[i], coefs[i]) for i in top_neg_idx],\n",
    "                    'model_name': modelo_name\n",
    "                }\n",
    "                \n",
    "                return global_features\n",
    "        else:\n",
    "            print(\"No se pudo extraer características - no hay DictVectorizer\")\n",
    "    else:\n",
    "        print(\"No se encontró modelo LR global\")\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Extraer características del modelo global\n",
    "features_global = extraer_features_modelo_global(modelos_globales_entrenados)\n",
    "\n",
    "if features_global:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"COMPARACIÓN: INDIVIDUAL vs GLOBAL\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    print(\"\\nCaracterísticas globales más importantes:\")\n",
    "    print(\"  Positivas:\", [word for word, coef in features_global['top_positive'][:8]])\n",
    "    print(\"  Negativas:\", [word for word, coef in features_global['top_negative'][:8]])\n",
    "    \n",
    "    # Comparar con características por categoría\n",
    "    if features_por_categoria:\n",
    "        print(\"\\nComparación con modelos individuales:\")\n",
    "        \n",
    "        # Palabras más comunes entre categorías\n",
    "        all_positive_words = []\n",
    "        all_negative_words = []\n",
    "        \n",
    "        for categoria, features in features_por_categoria.items():\n",
    "            positive_words = [word for word, coef in features['top_positive'][:10]]\n",
    "            negative_words = [word for word, coef in features['top_negative'][:10]]\n",
    "            all_positive_words.extend(positive_words)\n",
    "            all_negative_words.extend(negative_words)\n",
    "        \n",
    "        # Encontrar intersecciones\n",
    "        global_pos_words = [word for word, coef in features_global['top_positive'][:10]]\n",
    "        global_neg_words = [word for word, coef in features_global['top_negative'][:10]]\n",
    "        \n",
    "        common_positive = set(global_pos_words) & set(all_positive_words)\n",
    "        common_negative = set(global_neg_words) & set(all_negative_words)\n",
    "        \n",
    "        print(f\"  Palabras positivas comunes: {list(common_positive)[:8]}\")\n",
    "        print(f\"  Palabras negativas comunes: {list(common_negative)[:8]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
